#!/usr/bin/env python3
"""
æ­·å²è³‡æ–™å›å¡«å·¥å…·

å°‡éå» N å¤©çš„åŸå§‹ NetFlow è³‡æ–™èšåˆä¸¦å¯«å…¥ netflow_stats_5m ç´¢å¼•
æ¨¡æ“¬ Transform çš„èšåˆé‚è¼¯ï¼Œä½†è™•ç†æ­·å²è³‡æ–™
"""

import requests
import json
from datetime import datetime, timedelta
import time
import sys

# ES é…ç½®
ES_HOST = "http://localhost:9200"
SOURCE_INDEX = "radar_flow_collector-*"
DEST_INDEX = "netflow_stats_5m"

class HistoricalDataBackfill:
    """æ­·å²è³‡æ–™å›å¡«è™•ç†å™¨"""

    def __init__(self):
        self.es_url = ES_HOST
        self.processed_buckets = 0
        self.processed_docs = 0
        self.errors = []

    def backfill(self, days=3, batch_hours=1, dry_run=False, auto_confirm=False):
        """
        å›å¡«éå» N å¤©çš„æ­·å²è³‡æ–™

        Args:
            days: å›å¡«å¤©æ•¸
            batch_hours: æ¯æ‰¹è™•ç†çš„å°æ™‚æ•¸ (å»ºè­°1-6å°æ™‚ï¼Œé¿å…å–®æ¬¡æŸ¥è©¢éå¤§)
            dry_run: æ˜¯å¦åƒ…æ¸¬è©¦ä¸å¯¦éš›å¯«å…¥
            auto_confirm: è‡ªå‹•ç¢ºèªåŸ·è¡Œï¼ˆç”¨æ–¼èƒŒæ™¯åŸ·è¡Œï¼‰
        """
        print("=" * 80)
        print(f"NetFlow æ­·å²è³‡æ–™å›å¡«å·¥å…·")
        print("=" * 80)
        print(f"å›å¡«ç¯„åœ: éå» {days} å¤©")
        print(f"æ‰¹æ¬¡å¤§å°: {batch_hours} å°æ™‚/æ‰¹")
        print(f"æ¨¡å¼: {'æ¸¬è©¦æ¨¡å¼ (ä¸å¯«å…¥)' if dry_run else 'æ­£å¼æ¨¡å¼ (å¯«å…¥æ•¸æ“š)'}")
        print("=" * 80)
        print()

        # è¨ˆç®—æ™‚é–“ç¯„åœ
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(days=days)

        print(f"é–‹å§‹æ™‚é–“: {start_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
        print(f"çµæŸæ™‚é–“: {end_time.strftime('%Y-%m-%d %H:%M:%S')} UTC")
        print()

        # ç¢ºèªæ˜¯å¦ç¹¼çºŒ
        if not dry_run and not auto_confirm:
            try:
                response = input("âš ï¸  é€™å°‡å¯«å…¥å¯¦éš›æ•¸æ“šï¼Œæ˜¯å¦ç¹¼çºŒ? (yes/no): ")
                if response.lower() != 'yes':
                    print("å·²å–æ¶ˆ")
                    return
            except EOFError:
                print("âš ï¸  ç„¡æ³•è®€å–è¼¸å…¥ï¼ˆå¯èƒ½åœ¨èƒŒæ™¯åŸ·è¡Œï¼‰ï¼Œè«‹ä½¿ç”¨ --auto-confirm åƒæ•¸")
                print("å·²å–æ¶ˆ")
                return
        elif not dry_run and auto_confirm:
            print("âœ… è‡ªå‹•ç¢ºèªæ¨¡å¼ï¼Œé–‹å§‹åŸ·è¡Œ...")
            print()

        # åˆ†æ‰¹è™•ç†
        current_time = start_time
        batch_num = 0

        while current_time < end_time:
            batch_end = min(current_time + timedelta(hours=batch_hours), end_time)
            batch_num += 1

            print(f"\n{'='*80}")
            print(f"æ‰¹æ¬¡ #{batch_num}: {current_time.strftime('%Y-%m-%d %H:%M')} - {batch_end.strftime('%Y-%m-%d %H:%M')}")
            print(f"{'='*80}")

            try:
                self._process_time_range(current_time, batch_end, dry_run)
            except Exception as e:
                error_msg = f"æ‰¹æ¬¡ #{batch_num} è™•ç†å¤±æ•—: {e}"
                print(f"âŒ {error_msg}")
                self.errors.append(error_msg)

            current_time = batch_end

            # é¿å…éåº¦è² è¼‰ ES
            if not dry_run and batch_num % 5 == 0:
                print("\nâ¸ï¸  æš«åœ 5 ç§’ä»¥é¿å… ES éè¼‰...")
                time.sleep(5)

        # è¼¸å‡ºç¸½çµ
        self._print_summary(dry_run)

    def _process_time_range(self, start_time, end_time, dry_run=False):
        """è™•ç†æŒ‡å®šæ™‚é–“ç¯„åœçš„è³‡æ–™"""

        # è¨ˆç®—æ™‚é–“ç¯„åœå…§æœ‰å¤šå°‘å€‹5åˆ†é˜æ¡¶
        num_5m_buckets = int((end_time - start_time).total_seconds() / 300)

        print(f"ğŸ” æŸ¥è©¢åŸå§‹è³‡æ–™...")
        print(f"   é è¨ˆåŒ…å« {num_5m_buckets} å€‹5åˆ†é˜æ™‚é–“æ¡¶")

        # æ§‹å»ºèšåˆæŸ¥è©¢ (å®Œå…¨æ¨¡æ“¬ Transform é…ç½®)
        query = {
            "size": 0,
            "query": {
                "range": {
                    "FLOW_START_MILLISECONDS": {
                        "gte": start_time.isoformat(),
                        "lt": end_time.isoformat(),
                        "format": "strict_date_optional_time"
                    }
                }
            },
            "aggs": {
                "time_buckets": {
                    "date_histogram": {
                        "field": "FLOW_START_MILLISECONDS",
                        "fixed_interval": "5m",
                        "min_doc_count": 1
                    },
                    "aggs": {
                        "by_src_ip": {
                            "terms": {
                                "field": "IPV4_SRC_ADDR",
                                "size": 10000  # è™•ç†å¤§é‡ IP
                            },
                            "aggs": {
                                "total_bytes": {
                                    "sum": {"field": "IN_BYTES"}
                                },
                                "total_packets": {
                                    "sum": {"field": "IN_PKTS"}
                                },
                                "flow_count": {
                                    "value_count": {"field": "IPV4_SRC_ADDR"}
                                },
                                "unique_dsts": {
                                    "cardinality": {
                                        "field": "IPV4_DST_ADDR",
                                        "precision_threshold": 3000
                                    }
                                },
                                "unique_src_ports": {
                                    "cardinality": {
                                        "field": "L4_SRC_PORT",
                                        "precision_threshold": 1000
                                    }
                                },
                                "unique_dst_ports": {
                                    "cardinality": {
                                        "field": "L4_DST_PORT",
                                        "precision_threshold": 1000
                                    }
                                },
                                "avg_bytes": {
                                    "avg": {"field": "IN_BYTES"}
                                },
                                "max_bytes": {
                                    "max": {"field": "IN_BYTES"}
                                }
                            }
                        }
                    }
                }
            }
        }

        # åŸ·è¡ŒæŸ¥è©¢
        response = requests.post(
            f"{self.es_url}/{SOURCE_INDEX}/_search",
            json=query,
            headers={'Content-Type': 'application/json'},
            timeout=300  # 5åˆ†é˜è¶…æ™‚
        )
        response.raise_for_status()
        data = response.json()

        # æª¢æŸ¥æ˜¯å¦æœ‰æ•¸æ“š
        time_buckets = data['aggregations']['time_buckets']['buckets']

        if not time_buckets:
            print(f"   âš ï¸  æ­¤æ™‚é–“ç¯„åœå…§ç„¡æ•¸æ“š")
            return

        print(f"   âœ“ æ‰¾åˆ° {len(time_buckets)} å€‹æ™‚é–“æ¡¶")

        # æº–å‚™æ‰¹æ¬¡å¯«å…¥çš„æ–‡æª”
        docs_to_index = []
        total_ips = 0

        for time_bucket in time_buckets:
            bucket_time = time_bucket['key_as_string']
            ip_buckets = time_bucket['by_src_ip']['buckets']
            total_ips += len(ip_buckets)

            for ip_bucket in ip_buckets:
                doc = {
                    "time_bucket": bucket_time,
                    "src_ip": ip_bucket['key'],
                    "total_bytes": ip_bucket['total_bytes']['value'],
                    "total_packets": ip_bucket['total_packets']['value'],
                    "flow_count": ip_bucket['flow_count']['value'],
                    "unique_dsts": ip_bucket['unique_dsts']['value'],
                    "unique_src_ports": ip_bucket['unique_src_ports']['value'],
                    "unique_dst_ports": ip_bucket['unique_dst_ports']['value'],
                    "avg_bytes": ip_bucket['avg_bytes']['value'],
                    "max_bytes": ip_bucket['max_bytes']['value']
                }
                docs_to_index.append(doc)

        print(f"   ğŸ“Š èšåˆçµæœ: {total_ips} å€‹å”¯ä¸€ IP")

        if dry_run:
            print(f"   ğŸ” [æ¸¬è©¦æ¨¡å¼] è·³éå¯«å…¥ï¼Œå…± {len(docs_to_index)} ç­†æ–‡æª”")
            # é¡¯ç¤ºç¯„ä¾‹æ–‡æª”
            if docs_to_index:
                print(f"\n   ç¯„ä¾‹æ–‡æª”:")
                print(f"   {json.dumps(docs_to_index[0], indent=4)}")
        else:
            # æ‰¹æ¬¡å¯«å…¥åˆ°ç›®æ¨™ç´¢å¼•
            self._bulk_index(docs_to_index)

        self.processed_buckets += len(time_buckets)
        self.processed_docs += len(docs_to_index)

    def _bulk_index(self, docs):
        """æ‰¹æ¬¡å¯«å…¥æ–‡æª”åˆ° ES"""
        if not docs:
            return

        print(f"   ğŸ’¾ å¯«å…¥ {len(docs)} ç­†æ–‡æª”åˆ° {DEST_INDEX}...")

        # æ§‹å»º bulk è«‹æ±‚
        bulk_body = []
        for doc in docs:
            # ä½¿ç”¨ time_bucket + src_ip ä½œç‚ºæ–‡æª” IDï¼Œé¿å…é‡è¤‡
            doc_id = f"{doc['time_bucket']}_{doc['src_ip']}"

            # Index action
            bulk_body.append(json.dumps({
                "index": {
                    "_index": DEST_INDEX,
                    "_id": doc_id
                }
            }))
            # Document
            bulk_body.append(json.dumps(doc))

        bulk_data = "\n".join(bulk_body) + "\n"

        # åŸ·è¡Œ bulk å¯«å…¥
        response = requests.post(
            f"{self.es_url}/_bulk",
            data=bulk_data,
            headers={'Content-Type': 'application/x-ndjson'},
            timeout=120
        )
        response.raise_for_status()
        result = response.json()

        # æª¢æŸ¥éŒ¯èª¤
        if result.get('errors'):
            error_count = sum(1 for item in result['items'] if 'error' in item.get('index', {}))
            print(f"   âš ï¸  å¯«å…¥æ™‚ç™¼ç”Ÿ {error_count} å€‹éŒ¯èª¤")

            # é¡¯ç¤ºç¬¬ä¸€å€‹éŒ¯èª¤ç¯„ä¾‹
            for item in result['items']:
                if 'error' in item.get('index', {}):
                    print(f"   éŒ¯èª¤ç¯„ä¾‹: {item['index']['error']}")
                    break
        else:
            print(f"   âœ“ æˆåŠŸå¯«å…¥ {len(docs)} ç­†æ–‡æª”")

    def _print_summary(self, dry_run):
        """è¼¸å‡ºåŸ·è¡Œç¸½çµ"""
        print("\n" + "=" * 80)
        print("åŸ·è¡Œç¸½çµ")
        print("=" * 80)
        print(f"è™•ç†çš„æ™‚é–“æ¡¶æ•¸: {self.processed_buckets}")
        print(f"ç”Ÿæˆçš„æ–‡æª”æ•¸: {self.processed_docs:,}")

        if self.errors:
            print(f"\nâŒ éŒ¯èª¤æ•¸é‡: {len(self.errors)}")
            for error in self.errors[:5]:  # åªé¡¯ç¤ºå‰5å€‹éŒ¯èª¤
                print(f"   - {error}")
        else:
            print(f"\nâœ… ç„¡éŒ¯èª¤")

        if dry_run:
            print(f"\nâš ï¸  é€™æ˜¯æ¸¬è©¦é‹è¡Œï¼Œæœªå¯¦éš›å¯«å…¥æ•¸æ“š")
            print(f"   è‹¥è¦å¯¦éš›å¯«å…¥ï¼Œè«‹åŸ·è¡Œ: python3 {sys.argv[0]} --execute")
        else:
            print(f"\nâœ… å›å¡«å®Œæˆï¼")
            print(f"\nå»ºè­°åŸ·è¡Œä»¥ä¸‹å‘½ä»¤é©—è­‰:")
            print(f"   python3 verify_coverage.py")

        print("=" * 80)

    def check_existing_data(self, days=3):
        """æª¢æŸ¥ç›®æ¨™ç´¢å¼•ä¸­å·²æœ‰çš„æ­·å²è³‡æ–™ç¯„åœ"""
        print("\n" + "=" * 80)
        print("æª¢æŸ¥ç¾æœ‰è³‡æ–™")
        print("=" * 80)

        query = {
            "size": 0,
            "query": {
                "range": {
                    "time_bucket": {
                        "gte": f"now-{days}d"
                    }
                }
            },
            "aggs": {
                "time_range": {
                    "stats": {
                        "field": "time_bucket"
                    }
                },
                "doc_count": {
                    "value_count": {
                        "field": "time_bucket"
                    }
                }
            }
        }

        try:
            response = requests.post(
                f"{self.es_url}/{DEST_INDEX}/_search",
                json=query,
                headers={'Content-Type': 'application/json'}
            )
            response.raise_for_status()
            data = response.json()

            total_docs = data['hits']['total']['value']

            if total_docs > 0:
                stats = data['aggregations']['time_range']
                min_time = datetime.fromtimestamp(stats['min'] / 1000)
                max_time = datetime.fromtimestamp(stats['max'] / 1000)

                print(f"ç¾æœ‰æ–‡æª”æ•¸: {total_docs:,}")
                print(f"æ™‚é–“ç¯„åœ: {min_time.strftime('%Y-%m-%d %H:%M')} - {max_time.strftime('%Y-%m-%d %H:%M')}")
            else:
                print("ç´¢å¼•ä¸­ç›®å‰ç„¡è³‡æ–™")

        except Exception as e:
            print(f"âŒ æŸ¥è©¢å¤±æ•—: {e}")

        print("=" * 80)


def main():
    backfill = HistoricalDataBackfill()

    # è§£æå‘½ä»¤è¡Œåƒæ•¸
    if '--check' in sys.argv:
        # åªæª¢æŸ¥ç¾æœ‰è³‡æ–™
        days = int(sys.argv[sys.argv.index('--check') + 1]) if len(sys.argv) > sys.argv.index('--check') + 1 else 7
        backfill.check_existing_data(days)
        return

    # å›å¡«æ¨¡å¼
    dry_run = '--execute' not in sys.argv
    auto_confirm = '--auto-confirm' in sys.argv

    # ç²å–å¤©æ•¸åƒæ•¸
    days = 3
    if '--days' in sys.argv:
        days = int(sys.argv[sys.argv.index('--days') + 1])

    # ç²å–æ‰¹æ¬¡å¤§å°
    batch_hours = 1
    if '--batch-hours' in sys.argv:
        batch_hours = int(sys.argv[sys.argv.index('--batch-hours') + 1])

    # å…ˆæª¢æŸ¥ç¾æœ‰è³‡æ–™
    backfill.check_existing_data(days)

    # åŸ·è¡Œå›å¡«
    backfill.backfill(days=days, batch_hours=batch_hours, dry_run=dry_run, auto_confirm=auto_confirm)


if __name__ == "__main__":
    if len(sys.argv) == 1 or '--help' in sys.argv or '-h' in sys.argv:
        print("""
NetFlow æ­·å²è³‡æ–™å›å¡«å·¥å…·

ç”¨æ³•:
    # æ¸¬è©¦æ¨¡å¼ (ä¸å¯¦éš›å¯«å…¥ï¼Œåƒ…é¡¯ç¤ºæœƒè™•ç†çš„è³‡æ–™)
    python3 backfill_historical_data.py
    python3 backfill_historical_data.py --days 3

    # æ­£å¼åŸ·è¡Œ (å¯¦éš›å¯«å…¥è³‡æ–™)
    python3 backfill_historical_data.py --execute
    python3 backfill_historical_data.py --execute --days 3

    # èƒŒæ™¯åŸ·è¡Œï¼ˆè‡ªå‹•ç¢ºèªï¼Œä¸éœ€è¦è¼¸å…¥ yes/noï¼‰
    python3 backfill_historical_data.py --execute --auto-confirm --days 3

    # è‡ªè¨‚æ‰¹æ¬¡å¤§å° (æ¯æ‰¹è™•ç†å¹¾å°æ™‚)
    python3 backfill_historical_data.py --execute --days 7 --batch-hours 2

    # æª¢æŸ¥ç¾æœ‰è³‡æ–™
    python3 backfill_historical_data.py --check
    python3 backfill_historical_data.py --check 7

åƒæ•¸èªªæ˜:
    --execute        æ­£å¼åŸ·è¡Œæ¨¡å¼ (æœƒå¯¦éš›å¯«å…¥è³‡æ–™)
    --auto-confirm   è‡ªå‹•ç¢ºèªåŸ·è¡Œï¼ˆç”¨æ–¼ nohup èƒŒæ™¯åŸ·è¡Œï¼‰
    --days N         å›å¡«éå» N å¤©çš„è³‡æ–™ (é è¨­: 3)
    --batch-hours N  æ¯æ‰¹è™•ç† N å°æ™‚ (é è¨­: 1ï¼Œå»ºè­° 1-6)
    --check [N]      æª¢æŸ¥ç´¢å¼•ä¸­ç¾æœ‰è³‡æ–™ (å¯é¸æ“‡å¤©æ•¸)
    --help, -h       é¡¯ç¤ºæ­¤èªªæ˜

æ³¨æ„äº‹é …:
    1. é¦–æ¬¡åŸ·è¡Œå»ºè­°ä½¿ç”¨æ¸¬è©¦æ¨¡å¼ï¼Œç¢ºèªè³‡æ–™ç¯„åœç„¡èª¤
    2. æ‰¹æ¬¡å¤§å°å»ºè­° 1-6 å°æ™‚ï¼Œé¿å…å–®æ¬¡æŸ¥è©¢éå¤§
    3. å›å¡«æœƒè‡ªå‹•è·³éå·²å­˜åœ¨çš„æ–‡æª” (ä½¿ç”¨ time_bucket + src_ip ä½œç‚º ID)
    4. å¤§é‡å›å¡«å¯èƒ½éœ€è¦è¼ƒé•·æ™‚é–“ï¼Œè«‹è€å¿ƒç­‰å¾…

ç¯„ä¾‹:
    # å›å¡«éå»3å¤©ï¼Œæ¸¬è©¦æ¨¡å¼
    python3 backfill_historical_data.py --days 3

    # ç¢ºèªç„¡èª¤å¾Œï¼Œæ­£å¼åŸ·è¡Œ
    python3 backfill_historical_data.py --execute --days 3

    # å›å¡«éå»7å¤©ï¼Œæ¯æ‰¹è™•ç†2å°æ™‚
    python3 backfill_historical_data.py --execute --days 7 --batch-hours 2
        """)
        sys.exit(0)

    main()
