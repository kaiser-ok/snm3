#!/usr/bin/env python3
"""
å„ªåŒ–çš„ Isolation Forest ç•°å¸¸æª¢æ¸¬å™¨

åŸºæ–¼ netflow_stats_5m èšåˆæ•¸æ“šçš„ç„¡ç›£ç£ç•°å¸¸æª¢æ¸¬
"""

import numpy as np
import pickle
import os
from datetime import datetime, timedelta
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from elasticsearch import Elasticsearch
from typing import Dict, List, Tuple

from .feature_engineer import FeatureEngineer


class OptimizedIsolationForest:
    """
    å„ªåŒ–çš„ Isolation Forest æª¢æ¸¬å™¨

    ç‰¹é»ï¼š
    - ç›´æ¥ä½¿ç”¨èšåˆæ•¸æ“šè¨“ç·´
    - 99.57% æ•¸æ“šè¦†è“‹ç‡
    - è¨“ç·´é€Ÿåº¦å¿«ï¼ˆ5-10åˆ†é˜ï¼‰
    - æ¨è«–å»¶é²ä½ï¼ˆ< 1ç§’ï¼‰
    """

    def __init__(self, config=None):
        self.config = config
        self.model = None
        self.scaler = StandardScaler()
        self.feature_engineer = FeatureEngineer(config)

        # æ¨¡å‹é…ç½®
        if config:
            iso_config = config.isolation_forest_config
            self.model_config = iso_config
            self.model_path = os.path.join(
                config.output_config['models_dir'],
                'isolation_forest.pkl'
            )
            self.scaler_path = os.path.join(
                config.output_config['models_dir'],
                'scaler.pkl'
            )
        else:
            self.model_config = {
                'contamination': 0.05,
                'n_estimators': 150,
                'max_samples': 512,
                'max_features': 0.8,
                'random_state': 42,
                'n_jobs': -1
            }
            self.model_path = 'nad/models/isolation_forest.pkl'
            self.scaler_path = 'nad/models/scaler.pkl'

        # Elasticsearch å®¢æˆ¶ç«¯
        self.es = None

    def _init_es_client(self):
        """åˆå§‹åŒ– Elasticsearch å®¢æˆ¶ç«¯"""
        if self.es is None:
            es_host = self.config.es_host if self.config else "http://localhost:9200"
            self.es = Elasticsearch([es_host], timeout=30)

    def train_on_aggregated_data(self, days: int = 7, exclude_servers: bool = False) -> 'OptimizedIsolationForest':
        """
        ä½¿ç”¨èšåˆæ•¸æ“šè¨“ç·´æ¨¡å‹

        Args:
            days: è¨“ç·´æ•¸æ“šå¤©æ•¸ï¼ˆé»˜èª7å¤©ï¼‰
            exclude_servers: æ˜¯å¦æ’é™¤å¯èƒ½çš„æœå‹™å™¨å›æ‡‰æµé‡ï¼ˆé»˜èªFalseï¼‰

        Returns:
            self
        """
        print(f"\n{'='*70}")
        print(f"Isolation Forest è¨“ç·´ - ä½¿ç”¨éå» {days} å¤©çš„èšåˆæ•¸æ“š")
        if exclude_servers:
            print("(æ’é™¤æœå‹™å™¨å›æ‡‰æµé‡)")
        print(f"{'='*70}\n")

        self._init_es_client()

        # Step 1: æ”¶é›†è¨“ç·´æ•¸æ“š
        print(f"ğŸ“š Step 1: æ”¶é›†éå» {days} å¤©çš„èšåˆæ•¸æ“š...")
        training_records = self._fetch_training_data(days)

        if len(training_records) == 0:
            raise ValueError("æ²’æœ‰æ‰¾åˆ°è¨“ç·´æ•¸æ“šï¼è«‹æª¢æŸ¥ Elasticsearch ç´¢å¼•ã€‚")

        print(f"âœ“ æ”¶é›†åˆ° {len(training_records):,} ç­†èšåˆè¨˜éŒ„\n")

        # Step 2: ç‰¹å¾µæå–
        print("ğŸ”§ Step 2: æå–ç‰¹å¾µ...")
        X = self.feature_engineer.extract_features_batch(training_records)
        print(f"âœ“ æå–ç‰¹å¾µçŸ©é™£: {X.shape}")
        print(f"  æ¨£æœ¬æ•¸: {X.shape[0]:,}")
        print(f"  ç‰¹å¾µæ•¸: {X.shape[1]}\n")

        # Step 2.5: éæ¿¾æœå‹™å™¨å›æ‡‰ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if exclude_servers:
            print("ğŸš« Step 2.5: éæ¿¾æœå‹™å™¨å›æ‡‰æµé‡...")
            # is_likely_server_response æ˜¯å€’æ•¸ç¬¬3å€‹ç‰¹å¾µï¼ˆlog_flow_count, log_total_bytes ä¹‹å‰ï¼‰
            server_response_idx = self.feature_engineer.detection_feature_names.index('is_likely_server_response')

            # æ‰¾å‡ºä¸æ˜¯æœå‹™å™¨å›æ‡‰çš„æ¨£æœ¬
            not_server_mask = X[:, server_response_idx] == 0
            X_filtered = X[not_server_mask]
            training_records_filtered = [training_records[i] for i in range(len(training_records)) if not_server_mask[i]]

            n_excluded = len(training_records) - len(training_records_filtered)
            print(f"âœ“ æ’é™¤ {n_excluded:,} ç­†æœå‹™å™¨å›æ‡‰è¨˜éŒ„ ({n_excluded/len(training_records)*100:.2f}%)")
            print(f"  å‰©é¤˜è¨“ç·´æ¨£æœ¬: {len(training_records_filtered):,}\n")

            X = X_filtered
            training_records = training_records_filtered

        # Step 3: æ¨™æº–åŒ–
        print("ğŸ“Š Step 3: æ¨™æº–åŒ–ç‰¹å¾µ...")
        X_scaled = self.scaler.fit_transform(X)
        print(f"âœ“ ç‰¹å¾µå·²æ¨™æº–åŒ–\n")

        # Step 4: è¨“ç·´æ¨¡å‹
        print("ğŸ‹ï¸  Step 4: è¨“ç·´ Isolation Forest...")
        print(f"  é…ç½®: {self.model_config}")

        self.model = IsolationForest(**self.model_config)
        self.model.fit(X_scaled)

        print(f"âœ“ è¨“ç·´å®Œæˆ\n")

        # Step 5: è©•ä¼°è¨“ç·´çµæœ
        print("ğŸ“ˆ Step 5: è©•ä¼°è¨“ç·´çµæœ...")
        predictions = self.model.predict(X_scaled)
        n_anomalies = np.sum(predictions == -1)
        anomaly_rate = n_anomalies / len(predictions) * 100

        print(f"  è¨“ç·´é›†ç•°å¸¸æ¯”ä¾‹: {anomaly_rate:.2f}%")
        print(f"  ç•°å¸¸æ¨£æœ¬æ•¸: {n_anomalies:,} / {len(predictions):,}\n")

        # Step 6: ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ Step 6: ä¿å­˜æ¨¡å‹...")
        self._save_model()
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {self.model_path}\n")

        print(f"{'='*70}")
        print("âœ… è¨“ç·´å®Œæˆï¼")
        print(f"{'='*70}\n")

        return self

    def _fetch_training_data(self, days: int) -> List[Dict]:
        """
        å¾ ES ç²å–è¨“ç·´æ•¸æ“š

        Args:
            days: å¤©æ•¸

        Returns:
            èšåˆè¨˜éŒ„åˆ—è¡¨
        """
        index = self.config.es_aggregated_index if self.config else "netflow_stats_5m"

        query = {
            "size": 10000,
            "query": {
                "range": {
                    "time_bucket": {
                        "gte": f"now-{days}d",
                        "lt": "now"
                    }
                }
            }
        }

        # ä½¿ç”¨ scroll API ç²å–æ‰€æœ‰æ•¸æ“š
        records = []
        response = self.es.search(index=index, body=query, scroll='5m')

        scroll_id = response['_scroll_id']
        hits = response['hits']['hits']

        while hits:
            for hit in hits:
                records.append(hit['_source'])

            # ç¹¼çºŒ scroll
            response = self.es.scroll(scroll_id=scroll_id, scroll='5m')
            hits = response['hits']['hits']

        # æ¸…ç† scroll
        self.es.clear_scroll(scroll_id=scroll_id)

        return records

    def predict_realtime(self, recent_minutes: int = 10) -> List[Dict]:
        """
        å°æœ€è¿‘çš„æ•¸æ“šé€²è¡Œå¯¦æ™‚ç•°å¸¸æª¢æ¸¬

        Args:
            recent_minutes: åˆ†ææœ€è¿‘ N åˆ†é˜

        Returns:
            ç•°å¸¸åˆ—è¡¨
        """
        if self.model is None:
            self._load_model()

        self._init_es_client()

        # è¨ˆç®—æ˜ç¢ºçš„æ™‚é–“ç¯„åœ
        from datetime import datetime, timedelta, timezone

        end_time = datetime.now(timezone.utc)
        start_time = end_time - timedelta(minutes=recent_minutes)

        # è½‰æ›ç‚º ISO 8601 æ ¼å¼
        start_time_str = start_time.strftime('%Y-%m-%dT%H:%M:%S.000Z')
        end_time_str = end_time.strftime('%Y-%m-%dT%H:%M:%S.000Z')

        # æŸ¥è©¢æœ€è¿‘æ•¸æ“š
        index = self.config.es_aggregated_index if self.config else "netflow_stats_5m"

        query = {
            "size": 1000,
            "query": {
                "range": {
                    "time_bucket": {
                        "gte": start_time_str,
                        "lte": end_time_str
                    }
                }
            }
        }

        response = self.es.search(index=index, body=query)
        records = [hit['_source'] for hit in response['hits']['hits']]

        if len(records) == 0:
            return []

        # é æ¸¬
        return self._predict_batch(records)

    def predict_batch(self, records: List[Dict]) -> List[Dict]:
        """
        æ‰¹é‡é æ¸¬

        Args:
            records: èšåˆè¨˜éŒ„åˆ—è¡¨

        Returns:
            é æ¸¬çµæœåˆ—è¡¨
        """
        if self.model is None:
            self._load_model()

        return self._predict_batch(records)

    def _predict_batch(self, records: List[Dict]) -> List[Dict]:
        """
        å…§éƒ¨æ‰¹é‡é æ¸¬æ–¹æ³•

        Args:
            records: èšåˆè¨˜éŒ„

        Returns:
            ç•°å¸¸çµæœ
        """
        # æå–ç‰¹å¾µ
        X = self.feature_engineer.extract_features_batch(records)
        X_scaled = self.scaler.transform(X)

        # é æ¸¬
        predictions = self.model.predict(X_scaled)
        scores = self.model.score_samples(X_scaled)

        # çµ„è£çµæœ
        results = []
        for i, (record, pred, score) in enumerate(zip(records, predictions, scores)):
            if pred == -1:  # ç•°å¸¸
                # ä½¿ç”¨åˆ†é¡ç‰¹å¾µæå–å™¨ç²å–æ›´è±å¯Œçš„ç‰¹å¾µ
                features = self.feature_engineer.extract_classification_features(record)

                results.append({
                    'src_ip': record['src_ip'],
                    'time_bucket': record['time_bucket'],
                    'is_anomaly': True,
                    'anomaly_score': -score,  # è½‰ç‚ºæ­£å€¼ï¼Œè¶Šå¤§è¶Šç•°å¸¸
                    'confidence': self._score_to_confidence(-score),
                    'features': features,
                    'is_likely_server_response': features.get('is_likely_server_response', 0),  # æ–°å¢
                    'flow_count': record['flow_count'],
                    'unique_dsts': record['unique_dsts'],
                    'unique_src_ports': record.get('unique_src_ports', 0),
                    'unique_dst_ports': record.get('unique_dst_ports', 0),
                    'avg_bytes': record['avg_bytes'],
                    'total_bytes': record['total_bytes']
                })

        # æŒ‰ç•°å¸¸åˆ†æ•¸æ’åº
        results.sort(key=lambda x: x['anomaly_score'], reverse=True)

        return results

    def _score_to_confidence(self, score: float) -> float:
        """
        å°‡ç•°å¸¸åˆ†æ•¸è½‰æ›ç‚ºç½®ä¿¡åº¦ (0-1)

        ä½¿ç”¨åŸºæ–¼åˆ†ä½ˆçš„ Sigmoid æ˜ å°„ï¼Œé¿å…æ—©é£½å’Œå•é¡Œ

        Args:
            score: Isolation Forest åˆ†æ•¸ï¼ˆå·²å–åï¼Œæ­£å€¼ï¼‰

        Returns:
            ç½®ä¿¡åº¦ (0-1)
        """
        # åŸºæ–¼å¯¦éš›ç•°å¸¸åˆ†æ•¸åˆ†ä½ˆçš„åƒæ•¸ï¼ˆé€šéæ ¡æº–æ•¸æ“šå¾—å‡ºï¼‰
        # ä¸­ä½æ•¸ç´„ 0.57, 90åˆ†ä½ç´„ 0.66, 95åˆ†ä½ç´„ 0.68

        # ä½¿ç”¨ Sigmoid å‡½æ•¸é€²è¡Œå¹³æ»‘æ˜ å°„
        # åƒæ•¸è¨­è¨ˆ:
        #   - midpoint (ä¸­é»): 0.60 â†’ å°æ‡‰ 50% ç½®ä¿¡åº¦
        #   - steepness (é™¡åº¦): 20 â†’ æ§åˆ¶æ›²ç·šçš„å¹³æ»‘åº¦
        midpoint = 0.60
        steepness = 20

        # Sigmoid å‡½æ•¸: 1 / (1 + exp(-steepness * (score - midpoint)))
        confidence = 1 / (1 + np.exp(-steepness * (score - midpoint)))

        # ç¢ºä¿åœ¨ [0, 1] ç¯„åœå…§
        confidence = max(0.0, min(1.0, confidence))

        return confidence

    def _save_model(self):
        """ä¿å­˜æ¨¡å‹ã€scaler å’Œç‰¹å¾µå…ƒæ•¸æ“š"""
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)

        # æº–å‚™æ¨¡å‹ç‹€æ…‹ï¼ˆåŒ…å«ç‰¹å¾µå…ƒæ•¸æ“šï¼‰
        model_state = {
            'model': self.model,
            'feature_names': self.feature_engineer.detection_feature_names,
            'n_features': len(self.feature_engineer.detection_feature_names),
            'trained_at': datetime.now().isoformat(),
            'model_config': self.model_config
        }

        # ä¿å­˜æ¨¡å‹ç‹€æ…‹
        with open(self.model_path, 'wb') as f:
            pickle.dump(model_state, f)

        # ä¿å­˜ scalerï¼ˆç¨ç«‹æ–‡ä»¶ï¼‰
        with open(self.scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)

    def _load_model(self):
        """åŠ è¼‰æ¨¡å‹å’Œ scalerï¼Œä¸¦é©—è­‰ç‰¹å¾µä¸€è‡´æ€§"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(
                f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}\n"
                f"è«‹å…ˆé‹è¡Œè¨“ç·´: detector.train_on_aggregated_data()"
            )

        with open(self.model_path, 'rb') as f:
            model_state = pickle.load(f)

        # å‘å¾Œå…¼å®¹ï¼šå¦‚æœæ˜¯èˆŠæ ¼å¼ï¼ˆç›´æ¥ä¿å­˜ modelï¼‰ï¼Œè½‰æ›ç‚ºæ–°æ ¼å¼
        if isinstance(model_state, dict) and 'model' in model_state:
            # æ–°æ ¼å¼ï¼šåŒ…å«å…ƒæ•¸æ“š
            self.model = model_state['model']
            saved_feature_names = model_state.get('feature_names')
            saved_n_features = model_state.get('n_features')
            trained_at = model_state.get('trained_at', 'Unknown')

            # é©—è­‰ç‰¹å¾µä¸€è‡´æ€§
            current_feature_names = self.feature_engineer.detection_feature_names

            if saved_feature_names is not None:
                if saved_feature_names != current_feature_names:
                    # ç‰¹å¾µæ¼‚ç§»æª¢æ¸¬
                    raise ValueError(
                        f"\n{'='*70}\n"
                        f"âš ï¸  Feature Drift æª¢æ¸¬åˆ°ç‰¹å¾µä¸ä¸€è‡´ï¼\n"
                        f"{'='*70}\n\n"
                        f"æ¨¡å‹è¨“ç·´æ™‚çš„ç‰¹å¾µ ({saved_n_features} å€‹):\n  {saved_feature_names}\n\n"
                        f"ç•¶å‰é…ç½®çš„ç‰¹å¾µ ({len(current_feature_names)} å€‹):\n  {current_feature_names}\n\n"
                        f"æ¨¡å‹è¨“ç·´æ™‚é–“: {trained_at}\n\n"
                        f"å¯èƒ½åŸå› :\n"
                        f"  1. config.yaml ä¸­çš„ features é…ç½®å·²è¢«ä¿®æ”¹\n"
                        f"  2. ç‰¹å¾µé †åºç™¼ç”Ÿè®ŠåŒ–\n"
                        f"  3. æ–°å¢æˆ–åˆªé™¤äº†ç‰¹å¾µ\n\n"
                        f"è§£æ±ºæ–¹æ¡ˆ:\n"
                        f"  1. æ¢å¾© config.yaml ä¸­çš„ç‰¹å¾µé…ç½®ç‚ºè¨“ç·´æ™‚çš„è¨­å®š\n"
                        f"  2. æˆ–é‡æ–°è¨“ç·´æ¨¡å‹ä»¥ä½¿ç”¨æ–°çš„ç‰¹å¾µé…ç½®\n"
                        f"{'='*70}\n"
                    )

                print(f"âœ“ ç‰¹å¾µä¸€è‡´æ€§é©—è­‰é€šé ({saved_n_features} å€‹ç‰¹å¾µ)")
            else:
                print("âš ï¸  è­¦å‘Šï¼šæ¨¡å‹ç¼ºå°‘ç‰¹å¾µå…ƒæ•¸æ“šï¼ˆå¯èƒ½æ˜¯èˆŠç‰ˆæœ¬è¨“ç·´ï¼‰ï¼Œç„¡æ³•é€²è¡Œç‰¹å¾µä¸€è‡´æ€§æª¢æŸ¥")
        else:
            # èˆŠæ ¼å¼ï¼šç›´æ¥ä¿å­˜çš„ model å°è±¡
            self.model = model_state
            print("âš ï¸  è­¦å‘Šï¼šè¼‰å…¥èˆŠæ ¼å¼æ¨¡å‹ï¼ˆç„¡ç‰¹å¾µå…ƒæ•¸æ“šï¼‰ï¼Œå»ºè­°é‡æ–°è¨“ç·´")

        # è¼‰å…¥ scaler
        with open(self.scaler_path, 'rb') as f:
            self.scaler = pickle.load(f)

    def evaluate(self, test_records: List[Dict] = None, days: int = 1) -> Dict:
        """
        è©•ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            test_records: æ¸¬è©¦æ•¸æ“šï¼ˆå¯é¸ï¼‰
            days: å¦‚æœæ²’æœ‰æä¾›æ¸¬è©¦æ•¸æ“šï¼Œå¾æœ€è¿‘ N å¤©ç²å–

        Returns:
            è©•ä¼°çµæœ
        """
        if test_records is None:
            self._init_es_client()
            test_records = self._fetch_training_data(days)

        if len(test_records) == 0:
            raise ValueError("æ²’æœ‰æ¸¬è©¦æ•¸æ“š")

        print(f"\n{'='*70}")
        print("æ¨¡å‹è©•ä¼°")
        print(f"{'='*70}\n")

        # é æ¸¬
        results = self._predict_batch(test_records)

        # çµ±è¨ˆ
        n_total = len(test_records)
        n_anomalies = len(results)
        anomaly_rate = n_anomalies / n_total * 100

        print(f"æ¸¬è©¦æ¨£æœ¬æ•¸: {n_total:,}")
        print(f"æª¢æ¸¬åˆ°ç•°å¸¸: {n_anomalies:,}")
        print(f"ç•°å¸¸æ¯”ä¾‹: {anomaly_rate:.2f}%\n")

        # ç•°å¸¸åˆ†æ•¸åˆ†å¸ƒ
        if n_anomalies > 0:
            scores = [r['anomaly_score'] for r in results]
            print(f"ç•°å¸¸åˆ†æ•¸çµ±è¨ˆ:")
            print(f"  æœ€å°å€¼: {np.min(scores):.4f}")
            print(f"  æœ€å¤§å€¼: {np.max(scores):.4f}")
            print(f"  å¹³å‡å€¼: {np.mean(scores):.4f}")
            print(f"  ä¸­ä½æ•¸: {np.median(scores):.4f}\n")

        # Top ç•°å¸¸
        if n_anomalies > 0:
            print("Top 10 ç•°å¸¸:")
            print(f"{'-'*70}")
            for i, anomaly in enumerate(results[:10], 1):
                print(f"{i:2}. {anomaly['src_ip']:15} | "
                      f"åˆ†æ•¸: {anomaly['anomaly_score']:.4f} | "
                      f"{anomaly['flow_count']:5,} é€£ç·š | "
                      f"{anomaly['unique_dsts']:3} ç›®çš„åœ°")

        print(f"\n{'='*70}\n")

        return {
            'n_total': n_total,
            'n_anomalies': n_anomalies,
            'anomaly_rate': anomaly_rate,
            'top_anomalies': results[:20]
        }

    def get_model_info(self) -> Dict:
        """
        ç²å–æ¨¡å‹ä¿¡æ¯

        Returns:
            æ¨¡å‹ä¿¡æ¯å­—å…¸
        """
        if self.model is None:
            return {'status': 'not_trained'}

        return {
            'status': 'trained',
            'n_estimators': self.model.n_estimators,
            'contamination': self.model.contamination,
            'max_samples': self.model.max_samples,
            'max_samples': self.model.max_samples,
            'n_features': len(self.feature_engineer.detection_feature_names),
            'feature_names': self.feature_engineer.detection_feature_names,
            'model_path': self.model_path
        }
