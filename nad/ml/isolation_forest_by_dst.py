#!/usr/bin/env python3
"""
åŸºæ–¼ Dst è¦–è§’çš„ Isolation Forest ç•°å¸¸æª¢æ¸¬å™¨

ä½¿ç”¨ netflow_stats_5m_by_dst èšåˆæ•¸æ“šè¨“ç·´ï¼Œåµæ¸¬ï¼š
1. DDoS æ”»æ“Šç›®æ¨™ï¼ˆunique_srcs å¾ˆé«˜ï¼‰
2. è¢«æƒæçš„ç›®æ¨™ï¼ˆunique_src_ports å¾ˆé«˜ï¼‰
3. è³‡æ–™å¤–æ´©ç›®æ¨™ç«¯ï¼ˆå¤§é‡å…§éƒ¨ IP å‘å¤–éƒ¨ IP å‚³è¼¸æ•¸æ“šï¼‰
4. æƒ¡æ„è»Ÿé«”åˆ†ç™¼æœå‹™å™¨ï¼ˆå¤§é‡å…§éƒ¨ IP ä¸‹è¼‰ï¼‰
"""

import numpy as np
import pickle
import os
from datetime import datetime, timedelta
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import StandardScaler
from elasticsearch import Elasticsearch
from typing import Dict, List, Tuple

try:
    from .feature_engineer_dst import FeatureEngineerDst
except ImportError:
    # å¦‚æœä½œç‚ºè…³æœ¬ç›´æ¥é‹è¡Œ
    import sys
    sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))
    from nad.ml.feature_engineer_dst import FeatureEngineerDst


class IsolationForestByDst:
    """
    åŸºæ–¼ Dst è¦–è§’çš„ Isolation Forest æª¢æ¸¬å™¨

    ç‰¹é»ï¼š
    - ä½¿ç”¨ netflow_stats_5m_by_dst èšåˆæ•¸æ“š
    - åµæ¸¬ dst è¦–è§’çš„ç•°å¸¸ï¼ˆDDoS, è¢«æƒæ, è³‡æ–™å¤–æ´©ç›®æ¨™ç«¯ç­‰ï¼‰
    - èˆ‡ by_src æ¨¡å‹äº’è£œ
    """

    def __init__(self, config=None):
        self.config = config
        self.model = None
        self.scaler = StandardScaler()
        self.feature_engineer = FeatureEngineerDst(config)

        # æ¨¡å‹é…ç½®
        if config:
            iso_config = config.isolation_forest_config
            self.model_config = iso_config
            self.model_path = os.path.join(
                config.output_config['models_dir'],
                'isolation_forest_by_dst.pkl'
            )
            self.scaler_path = os.path.join(
                config.output_config['models_dir'],
                'scaler_by_dst.pkl'
            )
        else:
            self.model_config = {
                'contamination': 0.05,
                'n_estimators': 150,
                'max_samples': 512,
                'max_features': 0.8,
                'random_state': 42,
                'n_jobs': -1
            }
            self.model_path = 'nad/models/isolation_forest_by_dst.pkl'
            self.scaler_path = 'nad/models/scaler_by_dst.pkl'

        # Elasticsearch å®¢æˆ¶ç«¯
        self.es = None

    def _init_es_client(self):
        """åˆå§‹åŒ– Elasticsearch å®¢æˆ¶ç«¯"""
        if self.es is None:
            es_host = self.config.es_host if self.config else "http://localhost:9200"
            self.es = Elasticsearch([es_host], timeout=30)

    def train_on_aggregated_data(self, days: int = 7, exclude_servers: bool = False) -> 'IsolationForestByDst':
        """
        ä½¿ç”¨ by_dst èšåˆæ•¸æ“šè¨“ç·´æ¨¡å‹

        Args:
            days: è¨“ç·´æ•¸æ“šå¤©æ•¸ï¼ˆé»˜èª7å¤©ï¼‰
            exclude_servers: æ˜¯å¦æ’é™¤ä¼ºæœå™¨å›æ‡‰æµé‡ï¼ˆé ç•™åƒæ•¸ï¼Œby_dst æ¨¡å¼ä¸‹æ­¤åƒæ•¸ç„¡æ•ˆï¼‰

        Returns:
            self

        Note:
            exclude_servers åƒæ•¸åœ¨ by_dst æ¨¡å¼ä¸‹ä¸é©ç”¨ï¼Œå› ç‚ºç›®æ¨™ IP è¦–è§’
            ä¸»è¦é—œæ³¨è¢«é€£æ¥çš„ç›®æ¨™ï¼Œè€Œéç™¼èµ·é€£æ¥çš„ä¾†æº
        """
        print(f"\n{'='*70}")
        print(f"Isolation Forest (by_dst) è¨“ç·´ - ä½¿ç”¨éå» {days} å¤©çš„èšåˆæ•¸æ“š")
        print(f"{'='*70}\n")

        self._init_es_client()

        # Step 1: æ”¶é›†è¨“ç·´æ•¸æ“š
        print(f"ğŸ“š Step 1: æ”¶é›†éå» {days} å¤©çš„èšåˆæ•¸æ“š...")
        training_records = self._fetch_training_data(days)

        if len(training_records) == 0:
            raise ValueError("æ²’æœ‰æ‰¾åˆ°è¨“ç·´æ•¸æ“šï¼è«‹æª¢æŸ¥ netflow_stats_5m_by_dst ç´¢å¼•ã€‚")

        print(f"âœ“ æ”¶é›†åˆ° {len(training_records):,} ç­†èšåˆè¨˜éŒ„\n")

        # Step 2: ç‰¹å¾µæå–
        print("ğŸ”§ Step 2: æå–ç‰¹å¾µ...")
        X = self.feature_engineer.extract_features_batch(training_records)

        if len(X) == 0:
            raise ValueError("ç‰¹å¾µæå–å¤±æ•—ï¼")

        print(f"âœ“ æå–åˆ° {X.shape[1]} å€‹ç‰¹å¾µ")
        print(f"âœ“ è¨“ç·´æ¨£æœ¬æ•¸: {X.shape[0]:,}\n")

        # Step 3: æ¨™æº–åŒ–
        print("ğŸ“Š Step 3: ç‰¹å¾µæ¨™æº–åŒ–...")
        X_scaled = self.scaler.fit_transform(X)
        print(f"âœ“ æ¨™æº–åŒ–å®Œæˆ\n")

        # Step 4: è¨“ç·´ Isolation Forest
        print("ğŸ¤– Step 4: è¨“ç·´ Isolation Forest...")
        self.model = IsolationForest(**self.model_config)
        self.model.fit(X_scaled)
        print(f"âœ“ æ¨¡å‹è¨“ç·´å®Œæˆ\n")

        # Step 5: è©•ä¼°
        print("ğŸ“ˆ Step 5: è¨“ç·´é›†è©•ä¼°...")
        predictions = self.model.predict(X_scaled)
        scores = self.model.score_samples(X_scaled)

        n_anomalies = np.sum(predictions == -1)
        anomaly_rate = n_anomalies / len(predictions)

        print(f"âœ“ è¨“ç·´é›†ç•°å¸¸æ•¸: {n_anomalies:,} ({anomaly_rate*100:.2f}%)")
        print(f"âœ“ ç•°å¸¸åˆ†æ•¸ç¯„åœ: [{scores.min():.3f}, {scores.max():.3f}]")
        print(f"âœ“ ç•°å¸¸åˆ†æ•¸å¹³å‡: {scores.mean():.3f}\n")

        # Step 6: ä¿å­˜æ¨¡å‹
        print("ğŸ’¾ Step 6: ä¿å­˜æ¨¡å‹...")
        self._save_model()
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜: {self.model_path}")
        print(f"âœ“ Scaler å·²ä¿å­˜: {self.scaler_path}\n")

        print("=" * 70)
        print("è¨“ç·´å®Œæˆï¼")
        print("=" * 70)

        return self

    def _fetch_training_data(self, days: int) -> List[Dict]:
        """
        å¾ netflow_stats_5m_by_dst ç²å–è¨“ç·´æ•¸æ“š

        Args:
            days: éå» N å¤©

        Returns:
            è¨˜éŒ„åˆ—è¡¨
        """
        end_time = datetime.now()
        start_time = end_time - timedelta(days=days)

        query = {
            "size": 10000,
            "query": {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "time_bucket": {
                                    "gte": start_time.isoformat(),
                                    "lte": end_time.isoformat()
                                }
                            }
                        }
                    ]
                }
            },
            "sort": [{"time_bucket": "desc"}]
        }

        records = []
        scroll_size = 10000

        # ä½¿ç”¨ scroll API ç²å–å¤§é‡æ•¸æ“š
        result = self.es.search(
            index='netflow_stats_5m_by_dst',
            body=query,
            scroll='5m',
            size=scroll_size
        )

        scroll_id = result['_scroll_id']
        hits = result['hits']['hits']

        while hits:
            for hit in hits:
                records.append(hit['_source'])

            # ç²å–ä¸‹ä¸€æ‰¹
            result = self.es.scroll(scroll_id=scroll_id, scroll='5m')
            hits = result['hits']['hits']

        # æ¸…ç† scroll
        self.es.clear_scroll(scroll_id=scroll_id)

        return records

    def predict_realtime(self, recent_minutes: int = 10) -> List[Dict]:
        """
        å¯¦æ™‚ç•°å¸¸åµæ¸¬ï¼ˆdst è¦–è§’ï¼‰

        Args:
            recent_minutes: åˆ†ææœ€è¿‘ N åˆ†é˜çš„æ•¸æ“š

        Returns:
            ç•°å¸¸åˆ—è¡¨
        """
        if self.model is None:
            raise ValueError("æ¨¡å‹å°šæœªè¨“ç·´æˆ–åŠ è¼‰ï¼è«‹å…ˆèª¿ç”¨ train_on_aggregated_data() æˆ– _load_model()")

        self._init_es_client()

        # æŸ¥è©¢æœ€è¿‘çš„æ•¸æ“š
        records = self._fetch_recent_data(recent_minutes)

        if not records:
            return []

        # æå–ç‰¹å¾µ
        X = self.feature_engineer.extract_features_batch(records)

        if len(X) == 0:
            return []

        # æ¨™æº–åŒ–
        X_scaled = self.scaler.transform(X)

        # é æ¸¬
        predictions = self.model.predict(X_scaled)
        scores = self.model.score_samples(X_scaled)

        # æ”¶é›†ç•°å¸¸
        anomalies = []
        for i, pred in enumerate(predictions):
            if pred == -1:  # ç•°å¸¸
                record = records[i]

                # è¨ˆç®—ç½®ä¿¡åº¦ï¼ˆåŸºæ–¼ç•°å¸¸åˆ†æ•¸ï¼‰
                confidence = self._calculate_confidence(scores[i])

                # æå–ç‰¹å¾µï¼ˆç”¨æ–¼å¾ŒçºŒåˆ†é¡ï¼‰
                features = self.feature_engineer.extract_features(record)

                anomaly = {
                    'dst_ip': record['dst_ip'],
                    'time_bucket': record['time_bucket'],
                    'anomaly_score': abs(scores[i]),
                    'confidence': confidence,
                    'perspective': 'DST',  # æ¨™è¨˜è¦–è§’

                    # Dst è¦–è§’çš„é—œéµæŒ‡æ¨™
                    'unique_srcs': record.get('unique_srcs', 0),
                    'unique_src_ports': record.get('unique_src_ports', 0),
                    'unique_dst_ports': record.get('unique_dst_ports', 0),
                    'flow_count': record.get('flow_count', 0),
                    'total_bytes': record.get('total_bytes', 0),
                    'avg_bytes': record.get('avg_bytes', 0),

                    # ç‰¹å¾µå‘é‡ï¼ˆç”¨æ–¼åˆ†é¡ï¼‰
                    'features': {
                        'unique_srcs': record.get('unique_srcs', 0),
                        'unique_src_ports': record.get('unique_src_ports', 0),
                        'unique_dst_ports': record.get('unique_dst_ports', 0),
                        'flow_count': record.get('flow_count', 0),
                        'total_bytes': record.get('total_bytes', 0),
                        'avg_bytes': record.get('avg_bytes', 0),
                        'flows_per_src': features[8] if len(features) > 8 else 0,
                        'bytes_per_src': features[9] if len(features) > 9 else 0,
                    }
                }

                anomalies.append(anomaly)

        return anomalies

    def _fetch_recent_data(self, recent_minutes: int) -> List[Dict]:
        """
        æŸ¥è©¢æœ€è¿‘çš„ by_dst æ•¸æ“š

        Args:
            recent_minutes: æœ€è¿‘ N åˆ†é˜

        Returns:
            è¨˜éŒ„åˆ—è¡¨
        """
        query = {
            "size": 10000,
            "query": {
                "range": {
                    "time_bucket": {
                        "gte": f"now-{recent_minutes}m"
                    }
                }
            },
            "sort": [{"time_bucket": "desc"}]
        }

        result = self.es.search(index='netflow_stats_5m_by_dst', body=query)
        hits = result['hits']['hits']

        records = [hit['_source'] for hit in hits]
        return records

    def _calculate_confidence(self, score: float) -> float:
        """
        è¨ˆç®—ç•°å¸¸ç½®ä¿¡åº¦

        Args:
            score: Isolation Forest åˆ†æ•¸ï¼ˆè¶Šè² è¶Šç•°å¸¸ï¼‰

        Returns:
            ç½®ä¿¡åº¦ï¼ˆ0-1ï¼‰
        """
        # å°‡åˆ†æ•¸æ˜ å°„åˆ° 0-1 ç¯„åœ
        # score ç¯„åœé€šå¸¸æ˜¯ [-0.5, 0.5]
        # ç•°å¸¸åˆ†æ•¸ < 0ï¼Œè¶Šè² è¶Šç•°å¸¸
        if score >= 0:
            return 0.5

        # ä½¿ç”¨ sigmoid æ˜ å°„
        confidence = 1 / (1 + np.exp(score * 10))
        return min(max(confidence, 0.5), 1.0)

    def _save_model(self):
        """ä¿å­˜æ¨¡å‹å’Œ scaler"""
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)

        # ä¿å­˜æ¨¡å‹
        with open(self.model_path, 'wb') as f:
            pickle.dump(self.model, f)

        # ä¿å­˜ scaler
        with open(self.scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)

    def _load_model(self):
        """åŠ è¼‰æ¨¡å‹å’Œ scaler"""
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(
                f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}\n"
                f"è«‹å…ˆè¨“ç·´æ¨¡å‹: python3 train_isolation_forest_by_dst.py"
            )

        with open(self.model_path, 'rb') as f:
            self.model = pickle.load(f)

        with open(self.scaler_path, 'rb') as f:
            self.scaler = pickle.load(f)

    def get_model_info(self) -> Dict:
        """ç²å–æ¨¡å‹ä¿¡æ¯"""
        if self.model is None:
            return {'status': 'not_trained'}

        return {
            'status': 'trained',
            'n_features': self.feature_engineer.get_n_features(),
            'contamination': self.model_config['contamination'],
            'n_estimators': self.model_config['n_estimators'],
            'model_path': self.model_path,
            'perspective': 'DST'
        }


# ========== æ¸¬è©¦ ==========

def test_training():
    """æ¸¬è©¦è¨“ç·´"""
    print("æ¸¬è©¦ Isolation Forest (by_dst) è¨“ç·´\n")

    detector = IsolationForestByDst()

    try:
        detector.train_on_aggregated_data(days=7)
        print("\nâœ“ è¨“ç·´æˆåŠŸ")

        # é¡¯ç¤ºæ¨¡å‹ä¿¡æ¯
        info = detector.get_model_info()
        print(f"\næ¨¡å‹ä¿¡æ¯:")
        print(f"  - ç‰¹å¾µæ•¸: {info['n_features']}")
        print(f"  - æ±¡æŸ“ç‡: {info['contamination']}")
        print(f"  - è¦–è§’: {info['perspective']}")

    except Exception as e:
        print(f"\nâœ— è¨“ç·´å¤±æ•—: {e}")


def test_prediction():
    """æ¸¬è©¦é æ¸¬"""
    print("\n" + "=" * 70)
    print("æ¸¬è©¦ Isolation Forest (by_dst) å¯¦æ™‚åµæ¸¬")
    print("=" * 70 + "\n")

    detector = IsolationForestByDst()

    try:
        # åŠ è¼‰æ¨¡å‹
        detector._load_model()
        print("âœ“ æ¨¡å‹å·²åŠ è¼‰\n")

        # å¯¦æ™‚åµæ¸¬
        anomalies = detector.predict_realtime(recent_minutes=30)
        print(f"âœ“ åµæ¸¬åˆ° {len(anomalies)} å€‹ dst è¦–è§’ç•°å¸¸\n")

        if anomalies:
            print("å‰ 5 å€‹ç•°å¸¸:")
            for i, anomaly in enumerate(anomalies[:5], 1):
                print(f"\n{i}. {anomaly['dst_ip']}")
                print(f"   ç•°å¸¸åˆ†æ•¸: {anomaly['anomaly_score']:.4f}")
                print(f"   ç½®ä¿¡åº¦: {anomaly['confidence']:.0%}")
                print(f"   unique_srcs: {anomaly['unique_srcs']}")
                print(f"   unique_src_ports: {anomaly['unique_src_ports']}")
                print(f"   flow_count: {anomaly['flow_count']:,}")
                print(f"   avg_bytes: {anomaly['avg_bytes']:.0f}")

    except FileNotFoundError as e:
        print(f"âœ— {e}")
    except Exception as e:
        print(f"âœ— é æ¸¬å¤±æ•—: {e}")


if __name__ == "__main__":
    import sys

    if len(sys.argv) > 1 and sys.argv[1] == '--train':
        test_training()
    elif len(sys.argv) > 1 and sys.argv[1] == '--predict':
        test_prediction()
    else:
        print("ç”¨æ³•:")
        print("  è¨“ç·´: python3 nad/ml/isolation_forest_by_dst.py --train")
        print("  é æ¸¬: python3 nad/ml/isolation_forest_by_dst.py --predict")
