#!/usr/bin/env python3
"""
åˆ é™¤ Elasticsearch ä¸­æ—¶é—´å¼‚å¸¸çš„æ•°æ®

ç”¨æ³•:
  python3 delete_invalid_time_data.py --year 2026 --dry-run  # é¢„è§ˆè¦åˆ é™¤çš„æ•°æ®
  python3 delete_invalid_time_data.py --year 2026            # å®é™…åˆ é™¤
"""

import argparse
import warnings
from datetime import datetime
from nad.utils import load_config
from elasticsearch import Elasticsearch

warnings.filterwarnings('ignore')


def preview_invalid_data(es, index, cutoff_date):
    """é¢„è§ˆè¦åˆ é™¤çš„æ•°æ®"""
    query = {
        'size': 0,
        'query': {
            'range': {
                'time_bucket': {
                    'gte': cutoff_date
                }
            }
        },
        'aggs': {
            'sample_docs': {
                'top_hits': {
                    'size': 10,
                    'sort': [{'time_bucket': {'order': 'desc'}}],
                    '_source': ['src_ip', 'time_bucket', 'flow_count']
                }
            },
            'year_stats': {
                'date_histogram': {
                    'field': 'time_bucket',
                    'calendar_interval': 'year'
                }
            }
        }
    }

    try:
        response = es.search(index=index, body=query)
        total = response['hits']['total']['value']

        if total == 0:
            print(f'\nç´¢å¼• {index}: âœ… æ²¡æœ‰éœ€è¦åˆ é™¤çš„æ•°æ®')
            return 0

        print(f'\nç´¢å¼• {index}: å‘ç° {total:,} ç¬”å¼‚å¸¸æ•°æ® (æ—¶é—´ >= {cutoff_date})')

        # æ˜¾ç¤ºå¹´ä»½åˆ†å¸ƒ
        print('\nå¹´ä»½åˆ†å¸ƒ:')
        for bucket in response['aggregations']['year_stats']['buckets']:
            year = datetime.fromtimestamp(bucket['key'] / 1000).year
            print(f'  {year} å¹´: {bucket["doc_count"]:,} ç¬”')

        # æ˜¾ç¤ºæ ·æœ¬
        print('\næ ·æœ¬æ•°æ® (å‰ 10 ç¬”):')
        for hit in response['aggregations']['sample_docs']['hits']['hits']:
            src = hit['_source']
            print(f'  æ—¶é—´: {src.get("time_bucket", "N/A")} | '
                  f'IP: {src.get("src_ip", "N/A")} | '
                  f'è¿çº¿æ•°: {src.get("flow_count", "N/A")}')

        return total

    except Exception as e:
        print(f'\nç´¢å¼• {index}: âŒ æŸ¥è¯¢å¤±è´¥ - {e}')
        return 0


def delete_invalid_data(es, index, cutoff_date):
    """åˆ é™¤æ—¶é—´å¼‚å¸¸çš„æ•°æ®"""
    query = {
        'query': {
            'range': {
                'time_bucket': {
                    'gte': cutoff_date
                }
            }
        }
    }

    try:
        # ä½¿ç”¨ delete_by_query æ‰¹é‡åˆ é™¤
        response = es.delete_by_query(
            index=index,
            body=query,
            conflicts='proceed',  # å¿½ç•¥ç‰ˆæœ¬å†²çª
            refresh=True  # åˆ é™¤ååˆ·æ–°ç´¢å¼•
        )

        deleted = response.get('deleted', 0)
        print(f'\nç´¢å¼• {index}: âœ… æˆåŠŸåˆ é™¤ {deleted:,} ç¬”æ•°æ®')

        # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        if response.get('failures'):
            print(f'  âš ï¸  å¤±è´¥: {len(response["failures"])} ç¬”')
            for failure in response['failures'][:5]:  # åªæ˜¾ç¤ºå‰ 5 ä¸ªå¤±è´¥
                print(f'    - {failure}')

        return deleted

    except Exception as e:
        print(f'\nç´¢å¼• {index}: âŒ åˆ é™¤å¤±è´¥ - {e}')
        return 0


def main():
    parser = argparse.ArgumentParser(description='åˆ é™¤ Elasticsearch ä¸­æ—¶é—´å¼‚å¸¸çš„æ•°æ®')
    parser.add_argument(
        '--year',
        type=int,
        default=2026,
        help='åˆ é™¤æ­¤å¹´ä»½åŠä»¥åçš„æ•°æ® (é»˜è®¤: 2026)'
    )
    parser.add_argument(
        '--index',
        type=str,
        default='netflow_stats_5m',
        help='è¦å¤„ç†çš„ç´¢å¼•åç§° (é»˜è®¤: netflow_stats_5m)'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='nad/config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„'
    )
    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='é¢„è§ˆæ¨¡å¼ï¼Œä¸å®é™…åˆ é™¤æ•°æ®'
    )

    args = parser.parse_args()

    # æ„å»ºæˆªæ­¢æ—¥æœŸ
    cutoff_date = f'{args.year}-01-01T00:00:00.000Z'

    print('=' * 100)
    print('åˆ é™¤ Elasticsearch æ—¶é—´å¼‚å¸¸æ•°æ®')
    print('=' * 100)
    print(f'\né…ç½®:')
    print(f'  ç´¢å¼•: {args.index}')
    print(f'  åˆ é™¤æ¡ä»¶: æ—¶é—´ >= {cutoff_date}')
    print(f'  æ¨¡å¼: {"ğŸ” é¢„è§ˆæ¨¡å¼ (ä¸ä¼šå®é™…åˆ é™¤)" if args.dry_run else "âš ï¸  å®é™…åˆ é™¤æ¨¡å¼"}')
    print('=' * 100)

    # åŠ è½½é…ç½®
    try:
        config = load_config(args.config)
        es = Elasticsearch([config.es_host], request_timeout=60)
    except Exception as e:
        print(f'\nâŒ åˆå§‹åŒ–å¤±è´¥: {e}')
        return

    # é¢„è§ˆæ•°æ®
    print('\nğŸ“Š æ­¥éª¤ 1: é¢„è§ˆè¦åˆ é™¤çš„æ•°æ®')
    total = preview_invalid_data(es, args.index, cutoff_date)

    if total == 0:
        print('\nâœ… æ²¡æœ‰éœ€è¦åˆ é™¤çš„æ•°æ®ï¼Œé€€å‡º')
        return

    # ç¡®è®¤åˆ é™¤
    if not args.dry_run:
        print('\n' + '=' * 100)
        print('âš ï¸  è­¦å‘Š: å³å°†åˆ é™¤æ•°æ®ï¼Œæ­¤æ“ä½œä¸å¯æ¢å¤ï¼')
        print('=' * 100)

        confirm = input(f'\nç¡®è®¤è¦åˆ é™¤ {total:,} ç¬”æ•°æ®å—ï¼Ÿè¾“å…¥ "YES" ç¡®è®¤: ')

        if confirm != 'YES':
            print('\nâŒ å·²å–æ¶ˆåˆ é™¤æ“ä½œ')
            return

        # æ‰§è¡Œåˆ é™¤
        print('\nğŸ—‘ï¸  æ­¥éª¤ 2: æ‰§è¡Œåˆ é™¤...')
        deleted = delete_invalid_data(es, args.index, cutoff_date)

        print('\n' + '=' * 100)
        print(f'âœ… åˆ é™¤å®Œæˆï¼å…±åˆ é™¤ {deleted:,} ç¬”æ•°æ®')
        print('=' * 100)

        # éªŒè¯
        print('\nğŸ” æ­¥éª¤ 3: éªŒè¯åˆ é™¤ç»“æœ...')
        remaining = preview_invalid_data(es, args.index, cutoff_date)

        if remaining == 0:
            print('\nâœ… éªŒè¯é€šè¿‡ï¼šæ‰€æœ‰å¼‚å¸¸æ•°æ®å·²åˆ é™¤')
        else:
            print(f'\nâš ï¸  è­¦å‘Šï¼šä»æœ‰ {remaining:,} ç¬”å¼‚å¸¸æ•°æ®æœªåˆ é™¤')
    else:
        print('\nğŸ’¡ æç¤º: ç§»é™¤ --dry-run å‚æ•°å³å¯å®é™…åˆ é™¤æ•°æ®')


if __name__ == '__main__':
    main()
