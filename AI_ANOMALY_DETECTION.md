# AI è¼”åŠ©ç¶²è·¯ç•°å¸¸åµæ¸¬ç³»çµ±è¨­è¨ˆ

## æ¦‚è¿°

å°‡é¡ä¼¼æœ¬æ¬¡åˆ†æçš„ **AI æ¨è«–èƒ½åŠ›**æ•´åˆåˆ°è‡ªå‹•åŒ–ç•°å¸¸åµæ¸¬ç³»çµ±ä¸­ï¼Œå¯¦ç¾ï¼š
1. è‡ªå‹•åŒ–æ¨¡å¼è­˜åˆ¥
2. æ™ºèƒ½ç•°å¸¸åˆ†é¡
3. æ ¹å› åˆ†ææ¨è«–
4. è¡Œç‚ºé—œè¯åˆ†æ
5. é¢¨éšªè©•ä¼°èˆ‡å»ºè­°

---

## ä¸€ã€AI æ¨è«–æ©Ÿåˆ¶è¨­è¨ˆ

### 1.1 å¤šå±¤æ¬¡æ¨è«–æ¶æ§‹

```
åŸå§‹æµé‡æ•¸æ“š
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 1: è¦å‰‡å¼•æ“ (Rule-based)          â”‚
â”‚  - æ˜ç¢ºçš„é–¾å€¼æª¢æ¸¬                         â”‚
â”‚  - å·²çŸ¥æ”»æ“Šæ¨¡å¼åŒ¹é…                       â”‚
â”‚  - é€Ÿåº¦å¿«ã€æº–ç¢ºç‡é«˜                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 2: çµ±è¨ˆç•°å¸¸æª¢æ¸¬ (Statistical)     â”‚
â”‚  - Isolation Forest (å­¤ç«‹æ£®æ—)           â”‚
â”‚  - Z-Score / IQR ç•°å¸¸æª¢æ¸¬                â”‚
â”‚  - æ™‚é–“åºåˆ—ç•°å¸¸ (ARIMA, Prophet)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 3: æ©Ÿå™¨å­¸ç¿’åˆ†é¡ (ML Classification)â”‚
â”‚  - è¡Œç‚ºåˆ†é¡å™¨ (æƒæ/DDoS/æ­£å¸¸/...)       â”‚
â”‚  - éš¨æ©Ÿæ£®æ— / XGBoost                    â”‚
â”‚  - ç¥ç¶“ç¶²è·¯åˆ†é¡å™¨                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Layer 4: LLM æ™ºèƒ½åˆ†æ (AI Reasoning)    â”‚
â”‚  - æ ¹å› åˆ†ææ¨è«–                          â”‚
â”‚  - é—œè¯äº‹ä»¶åˆ†æ                          â”‚
â”‚  - è‡ªç„¶èªè¨€å ±å‘Šç”Ÿæˆ                       â”‚
â”‚  - ä¿®å¾©å»ºè­°ç”Ÿæˆ                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## äºŒã€å…·é«”å¯¦ä½œæ–¹æ¡ˆ

### æ–¹æ¡ˆ A: è¼•é‡ç´š ML æ¨¡å‹ (æ¨è–¦å„ªå…ˆå¯¦ä½œ)

**å„ªé»:**
- å¯é›¢ç·šé‹è¡Œ
- ä½å»¶é² (<100ms)
- å¯è§£é‡‹æ€§å¼·
- ä¸éœ€å¤–éƒ¨ API

#### 2.1 Isolation Forest ç•°å¸¸æª¢æ¸¬

```python
# nad/ml/isolation_forest_detector.py

from sklearn.ensemble import IsolationForest
import numpy as np
import pickle
import os

class IsolationForestDetector:
    """
    ä½¿ç”¨ Isolation Forest é€²è¡Œç„¡ç›£ç£ç•°å¸¸æª¢æ¸¬
    """

    def __init__(self, model_path='models/isolation_forest.pkl'):
        self.model = None
        self.model_path = model_path
        self.feature_names = [
            'connections_per_hour',
            'unique_destinations',
            'unique_ports',
            'avg_bytes_per_connection',
            'total_bytes',
            'tcp_ratio',
            'udp_ratio',
            'connection_rate',
            'port_diversity',
            'dst_ip_entropy'
        ]

    def train(self, training_data, contamination=0.1):
        """
        è¨“ç·´æ¨¡å‹

        Args:
            training_data: æ­£å¸¸æµé‡çš„ç‰¹å¾µæ•¸æ“š
            contamination: é æœŸç•°å¸¸æ¯”ä¾‹ (0.1 = 10%)
        """
        X = self._extract_features(training_data)

        self.model = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=100,
            max_samples='auto'
        )

        self.model.fit(X)
        self._save_model()

        return self

    def predict(self, flow_data):
        """
        é æ¸¬æ˜¯å¦ç•°å¸¸

        Returns:
            anomaly_scores: -1 ç‚ºç•°å¸¸, 1 ç‚ºæ­£å¸¸
            confidence: ç•°å¸¸ç½®ä¿¡åº¦ (0-1)
        """
        if not self.model:
            self._load_model()

        X = self._extract_features(flow_data)
        predictions = self.model.predict(X)

        # ç²å–ç•°å¸¸åˆ†æ•¸ (è¶Šè² è¶Šç•°å¸¸)
        scores = self.model.score_samples(X)

        # è½‰æ›ç‚º 0-1 çš„ç½®ä¿¡åº¦
        confidence = self._normalize_scores(scores)

        results = []
        for i, (pred, conf, data) in enumerate(zip(predictions, confidence, flow_data)):
            results.append({
                'is_anomaly': pred == -1,
                'confidence': conf,
                'anomaly_score': scores[i],
                'src_ip': data.get('src_ip'),
                'features': dict(zip(self.feature_names, X[i]))
            })

        return results

    def _extract_features(self, flow_data):
        """
        å¾æµé‡æ•¸æ“šæå–ç‰¹å¾µå‘é‡
        """
        features = []

        for record in flow_data:
            feature_vector = [
                record.get('connection_count', 0),
                record.get('unique_destinations', 0),
                record.get('unique_ports', 0),
                record.get('avg_bytes_per_connection', 0),
                record.get('total_bytes', 0),
                record.get('tcp_ratio', 0),
                record.get('udp_ratio', 0),
                record.get('connection_rate', 0),
                self._calculate_port_diversity(record),
                self._calculate_entropy(record.get('destination_ips', []))
            ]
            features.append(feature_vector)

        return np.array(features)

    def _calculate_port_diversity(self, record):
        """è¨ˆç®—ç«¯å£å¤šæ¨£æ€§"""
        unique_ports = record.get('unique_ports', 0)
        total_connections = record.get('connection_count', 1)
        return unique_ports / total_connections if total_connections > 0 else 0

    def _calculate_entropy(self, ip_list):
        """è¨ˆç®— IP åˆ†å¸ƒçš„ç†µå€¼"""
        if not ip_list:
            return 0

        from collections import Counter
        import math

        counts = Counter(ip_list)
        total = len(ip_list)
        entropy = -sum(
            (count/total) * math.log2(count/total)
            for count in counts.values()
        )
        return entropy

    def _normalize_scores(self, scores):
        """
        å°‡ç•°å¸¸åˆ†æ•¸æ­£è¦åŒ–ç‚º 0-1 çš„ç½®ä¿¡åº¦
        """
        # Isolation Forest åˆ†æ•¸é€šå¸¸åœ¨ -0.5 åˆ° 0.5 ä¹‹é–“
        # è¶Šè² è¶Šç•°å¸¸
        normalized = []
        for score in scores:
            # è½‰æ›ç‚º 0-1ï¼Œ0 ç‚ºæ­£å¸¸ï¼Œ1 ç‚ºé«˜åº¦ç•°å¸¸
            confidence = max(0, min(1, (-score + 0.25) * 2))
            normalized.append(confidence)
        return normalized

    def _save_model(self):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(os.path.dirname(self.model_path), exist_ok=True)
        with open(self.model_path, 'wb') as f:
            pickle.dump(self.model, f)

    def _load_model(self):
        """åŠ è¼‰æ¨¡å‹"""
        if os.path.exists(self.model_path):
            with open(self.model_path, 'rb') as f:
                self.model = pickle.load(f)
        else:
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
```

#### 2.2 è¡Œç‚ºåˆ†é¡å™¨

```python
# nad/ml/behavior_classifier.py

from sklearn.ensemble import RandomForestClassifier
import numpy as np

class BehaviorClassifier:
    """
    æµé‡è¡Œç‚ºåˆ†é¡å™¨
    åˆ†é¡: æ­£å¸¸ã€æƒæã€DDoSã€æ•¸æ“šå¤–æ´©ã€DNSæ¿«ç”¨
    """

    def __init__(self):
        self.model = RandomForestClassifier(
            n_estimators=200,
            max_depth=10,
            random_state=42
        )
        self.behavior_labels = {
            0: 'normal',
            1: 'port_scanning',
            2: 'network_scanning',
            3: 'ddos',
            4: 'data_exfiltration',
            5: 'dns_abuse',
            6: 'brute_force'
        }

    def train(self, labeled_data):
        """
        ä½¿ç”¨æ¨™è¨˜æ•¸æ“šè¨“ç·´

        labeled_data: [
            {
                'features': {...},
                'label': 'port_scanning'
            },
            ...
        ]
        """
        X = []
        y = []

        label_to_int = {v: k for k, v in self.behavior_labels.items()}

        for record in labeled_data:
            features = self._extract_features(record['features'])
            label = label_to_int[record['label']]
            X.append(features)
            y.append(label)

        self.model.fit(np.array(X), np.array(y))

    def predict(self, flow_data):
        """
        é æ¸¬æµé‡è¡Œç‚ºé¡å‹
        """
        results = []

        for record in flow_data:
            features = self._extract_features(record)
            prediction = self.model.predict([features])[0]
            probabilities = self.model.predict_proba([features])[0]

            behavior = self.behavior_labels[prediction]
            confidence = probabilities[prediction]

            # ç²å–ç‰¹å¾µé‡è¦æ€§
            feature_importance = self._get_top_features(features)

            results.append({
                'src_ip': record.get('src_ip'),
                'behavior': behavior,
                'confidence': confidence,
                'all_probabilities': {
                    self.behavior_labels[i]: prob
                    for i, prob in enumerate(probabilities)
                },
                'key_indicators': feature_importance
            })

        return results

    def _extract_features(self, record):
        """æå–åˆ†é¡ç‰¹å¾µ"""
        return [
            record.get('connection_count', 0),
            record.get('unique_destinations', 0),
            record.get('unique_ports', 0),
            record.get('avg_bytes_per_connection', 0),
            record.get('connection_rate', 0),
            record.get('tcp_ratio', 0),
            record.get('udp_ratio', 0),
            # è¡ç”Ÿç‰¹å¾µ
            record.get('unique_destinations', 0) / max(record.get('connection_count', 1), 1),  # ç›®æ¨™å¤šæ¨£æ€§
            record.get('unique_ports', 0) / max(record.get('connection_count', 1), 1),  # ç«¯å£å¤šæ¨£æ€§
            1 if record.get('avg_bytes_per_connection', 0) < 1000 else 0,  # å°å°åŒ…æ¨™è¨˜
            1 if record.get('dns_query_count', 0) > 1000 else 0,  # DNS å¯†é›†æ¨™è¨˜
        ]

    def _get_top_features(self, features):
        """ç²å–æœ€é‡è¦çš„ç‰¹å¾µ"""
        feature_names = [
            'connection_count',
            'unique_destinations',
            'unique_ports',
            'avg_bytes',
            'connection_rate',
            'tcp_ratio',
            'udp_ratio',
            'dest_diversity',
            'port_diversity',
            'is_small_packet',
            'is_dns_heavy'
        ]

        importances = self.model.feature_importances_
        indices = np.argsort(importances)[-3:]  # Top 3

        return [
            {
                'feature': feature_names[i],
                'value': features[i],
                'importance': importances[i]
            }
            for i in indices
        ]
```

---

### æ–¹æ¡ˆ B: LLM æ™ºèƒ½æ¨è«– (é€²éš)

**å„ªé»:**
- å¼·å¤§çš„æ¨è«–èƒ½åŠ›
- è‡ªç„¶èªè¨€å ±å‘Š
- é—œè¯åˆ†æ
- æŒçºŒå­¸ç¿’

**ä½¿ç”¨å ´æ™¯:**
- è¤‡é›œç•°å¸¸çš„æ ¹å› åˆ†æ
- ç”Ÿæˆå¯è®€çš„åˆ†æå ±å‘Š
- æä¾›ä¿®å¾©å»ºè­°

#### 2.3 LLM æ¨è«–å¼•æ“

```python
# nad/ai/llm_reasoner.py

import anthropic
import json
from typing import Dict, List

class LLMReasoner:
    """
    ä½¿ç”¨ LLM é€²è¡Œæ™ºèƒ½æ¨è«–å’Œåˆ†æ
    """

    def __init__(self, api_key=None):
        self.client = anthropic.Anthropic(api_key=api_key) if api_key else None
        self.use_local_llm = not api_key  # å¦‚æœæ²’æœ‰ API keyï¼Œä½¿ç”¨æœ¬åœ°è¦å‰‡

    def analyze_anomaly(self, anomaly_data: Dict, context: Dict) -> Dict:
        """
        æ·±åº¦åˆ†æç•°å¸¸äº‹ä»¶

        Args:
            anomaly_data: ç•°å¸¸æ•¸æ“š
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ (è¨­å‚™è³‡è¨Šã€æ­·å²æ•¸æ“šç­‰)

        Returns:
            åˆ†æçµæœåŒ…å«: æ ¹å› ã€å»ºè­°ã€é¢¨éšªè©•ä¼°
        """
        if self.use_local_llm:
            return self._rule_based_analysis(anomaly_data, context)

        # æ§‹å»ºæç¤ºè©
        prompt = self._build_analysis_prompt(anomaly_data, context)

        # èª¿ç”¨ LLM
        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=2000,
            temperature=0.3,
            system=self._get_system_prompt(),
            messages=[
                {"role": "user", "content": prompt}
            ]
        )

        # è§£æå›æ‡‰
        analysis = self._parse_llm_response(response.content[0].text)

        return analysis

    def _build_analysis_prompt(self, anomaly_data: Dict, context: Dict) -> str:
        """
        æ§‹å»º LLM åˆ†ææç¤ºè©
        """
        prompt = f"""
è«‹åˆ†æä»¥ä¸‹ç¶²è·¯ç•°å¸¸äº‹ä»¶ï¼š

## ç•°å¸¸æ•¸æ“š
IP åœ°å€: {anomaly_data.get('src_ip')}
è¨­å‚™åç¨±: {context.get('device_name', 'æœªçŸ¥')}
è¨­å‚™é¡å‹: {context.get('device_type', 'æœªçŸ¥')}

ç•°å¸¸æŒ‡æ¨™:
- é€£ç·šæ•¸: {anomaly_data.get('connection_count', 0):,}
- å”¯ä¸€ç›®çš„åœ°: {anomaly_data.get('unique_destinations', 0)}
- å”¯ä¸€ç«¯å£: {anomaly_data.get('unique_ports', 0)}
- å¹³å‡æ¯é€£ç·šæµé‡: {anomaly_data.get('avg_bytes_per_connection', 0):.2f} bytes
- ç¸½æµé‡: {anomaly_data.get('total_bytes', 0) / 1024 / 1024:.2f} MB

å”å®šåˆ†å¸ƒ:
- TCP: {anomaly_data.get('tcp_ratio', 0) * 100:.1f}%
- UDP: {anomaly_data.get('udp_ratio', 0) * 100:.1f}%

ä¸»è¦ç›®çš„ç«¯å£: {anomaly_data.get('top_ports', [])}

ç•°å¸¸åˆ†é¡: {anomaly_data.get('behavior_classification', 'æœªçŸ¥')}
ç•°å¸¸è©•åˆ†: {anomaly_data.get('anomaly_score', 0)}/100

## æ­·å²å°æ¯”
éå»7å¤©å¹³å‡é€£ç·šæ•¸: {context.get('baseline_connections', 0):,}
åå·®: {context.get('deviation_percentage', 0):.1f}%

## è«‹æä¾›ä»¥ä¸‹åˆ†æ:

1. **æ ¹å› åˆ†æ**: é€ æˆæ­¤ç•°å¸¸çš„æœ€å¯èƒ½åŸå› æ˜¯ä»€éº¼ï¼Ÿ
2. **è¡Œç‚ºåˆ¤æ–·**: é€™æ˜¯æƒ¡æ„è¡Œç‚ºã€é…ç½®éŒ¯èª¤ã€é‚„æ˜¯æ­£å¸¸æ¥­å‹™è¡Œç‚ºï¼Ÿ
3. **é¢¨éšªè©•ä¼°**: é¢¨éšªç­‰ç´š (ä½/ä¸­/é«˜/åš´é‡) åŠç†ç”±
4. **é—œè¯åˆ†æ**: æ˜¯å¦èˆ‡å…¶ä»–ç•°å¸¸äº‹ä»¶ç›¸é—œï¼Ÿ
5. **å»ºè­°æªæ–½**: å…·é«”çš„èª¿æŸ¥æ­¥é©Ÿå’Œä¿®å¾©å»ºè­°

è«‹ä»¥ JSON æ ¼å¼å›è¦†ï¼ŒåŒ…å«ä»¥ä¸‹æ¬„ä½:
{{
  "root_cause": "...",
  "behavior_type": "malicious|misconfiguration|normal",
  "risk_level": "low|medium|high|critical",
  "risk_reasoning": "...",
  "correlations": ["..."],
  "recommendations": [
    {{
      "priority": "immediate|high|medium|low",
      "action": "...",
      "reason": "..."
    }}
  ],
  "additional_investigation": ["..."]
}}
"""
        return prompt

    def _get_system_prompt(self) -> str:
        """
        ç³»çµ±æç¤ºè©
        """
        return """ä½ æ˜¯ä¸€ä½è³‡æ·±çš„ç¶²è·¯å®‰å…¨åˆ†æå°ˆå®¶ï¼Œå°ˆç²¾æ–¼ï¼š
1. ç¶²è·¯æµé‡åˆ†æ
2. ç•°å¸¸è¡Œç‚ºåµæ¸¬
3. è³‡å®‰äº‹ä»¶èª¿æŸ¥
4. æ ¹å› åˆ†æ

ä½ çš„ä»»å‹™æ˜¯åˆ†æ NetFlow æ•¸æ“šä¸­çš„ç•°å¸¸äº‹ä»¶ï¼Œæä¾›å°ˆæ¥­çš„åˆ¤æ–·å’Œå»ºè­°ã€‚

åˆ†æåŸå‰‡:
- åŸºæ–¼æ•¸æ“šå’Œäº‹å¯¦é€²è¡Œæ¨è«–
- è€ƒæ…®å¤šç¨®å¯èƒ½æ€§ï¼Œä½†æŒ‡å‡ºæœ€å¯èƒ½çš„åŸå› 
- æä¾›å¯åŸ·è¡Œçš„å…·é«”å»ºè­°
- è©•ä¼°é¢¨éšªæ™‚ä¿æŒå®¢è§€
- ä½¿ç”¨å°ˆæ¥­ä½†æ¸…æ™°çš„èªè¨€

å·²çŸ¥çš„ç•°å¸¸æ¨¡å¼:
- ç«¯å£æƒæ: é«˜é€£ç·šæ•¸ã€å¤šç›®çš„åœ°ã€å°æµé‡
- ç¶²è·¯æƒæ: æ¥µå¤šç›®çš„åœ°ã€å°æµé‡ã€å¿«é€Ÿé€£ç·š
- DDoS: æ¥µé«˜é€£ç·šæ•¸ã€å–®ä¸€æˆ–å°‘æ•¸ç›®çš„åœ°
- æ•¸æ“šå¤–æ´©: å¤§é‡æ•¸æ“šå‚³è¼¸åˆ°å¤–éƒ¨ IP
- DNS æ¿«ç”¨: å¤§é‡ DNS æŸ¥è©¢ (å¯èƒ½æ˜¯ DNS éš§é“)
- æš´åŠ›ç ´è§£: å¤šæ¬¡é€£ç·šåˆ°èªè­‰ç«¯å£ (22, 3389, ç­‰)
"""

    def _parse_llm_response(self, response_text: str) -> Dict:
        """
        è§£æ LLM å›æ‡‰
        """
        try:
            # å˜—è©¦æå– JSON
            start = response_text.find('{')
            end = response_text.rfind('}') + 1
            json_str = response_text[start:end]
            return json.loads(json_str)
        except:
            # å¦‚æœç„¡æ³•è§£æï¼Œè¿”å›åŸå§‹æ–‡æœ¬
            return {
                "root_cause": "åˆ†æå¤±æ•—",
                "raw_response": response_text
            }

    def _rule_based_analysis(self, anomaly_data: Dict, context: Dict) -> Dict:
        """
        åŸºæ–¼è¦å‰‡çš„åˆ†æ (å‚™ç”¨æ–¹æ¡ˆ)
        """
        behavior = anomaly_data.get('behavior_classification', 'unknown')
        connections = anomaly_data.get('connection_count', 0)
        unique_dsts = anomaly_data.get('unique_destinations', 0)
        avg_bytes = anomaly_data.get('avg_bytes_per_connection', 0)

        # ç°¡å–®è¦å‰‡æ¨è«–
        if behavior == 'port_scanning':
            root_cause = "è¨­å‚™æ­£åœ¨é€²è¡Œç«¯å£æƒæï¼Œå¯èƒ½æ˜¯å®‰å…¨æƒæå·¥å…·æˆ–æƒ¡æ„è»Ÿé«”"
            risk_level = "high"
            behavior_type = "malicious" if context.get('device_type') != 'security_scanner' else "normal"

        elif behavior == 'dns_abuse':
            root_cause = "DNS æŸ¥è©¢é »ç‡ç•°å¸¸ï¼Œå¯èƒ½æ˜¯ DNS é…ç½®éŒ¯èª¤æˆ– DNS éš§é“æ”»æ“Š"
            risk_level = "high"
            behavior_type = "misconfiguration"

        else:
            root_cause = "æµé‡æ¨¡å¼ç•°å¸¸ï¼Œéœ€é€²ä¸€æ­¥èª¿æŸ¥"
            risk_level = "medium"
            behavior_type = "unknown"

        return {
            "root_cause": root_cause,
            "behavior_type": behavior_type,
            "risk_level": risk_level,
            "risk_reasoning": f"åŸºæ–¼è¦å‰‡å¼•æ“åˆ†æ",
            "recommendations": [
                {
                    "priority": "high",
                    "action": f"èª¿æŸ¥ {anomaly_data.get('src_ip')} çš„ç•°å¸¸è¡Œç‚º",
                    "reason": root_cause
                }
            ]
        }

    def generate_report(self, analysis_results: List[Dict]) -> str:
        """
        ç”Ÿæˆè‡ªç„¶èªè¨€åˆ†æå ±å‘Š
        """
        if self.use_local_llm:
            return self._template_based_report(analysis_results)

        # æ§‹å»ºæç¤ºè©
        prompt = f"""
æ ¹æ“šä»¥ä¸‹ç•°å¸¸åˆ†æçµæœï¼Œç”Ÿæˆä¸€ä»½å°ˆæ¥­çš„ç¶²è·¯å®‰å…¨åˆ†æå ±å‘Š:

{json.dumps(analysis_results, indent=2, ensure_ascii=False)}

å ±å‘Šè¦æ±‚:
1. åŸ·è¡Œæ‘˜è¦ (100å­—å…§)
2. é—œéµç™¼ç¾ (3-5é»)
3. é¢¨éšªè©•ä¼°
4. å»ºè­°æªæ–½ (æŒ‰å„ªå…ˆç´šæ’åº)
5. çµè«–

ä½¿ç”¨ Markdown æ ¼å¼ï¼Œèªæ°£å°ˆæ¥­ä½†æ˜“æ‡‚ã€‚
"""

        response = self.client.messages.create(
            model="claude-3-5-sonnet-20241022",
            max_tokens=3000,
            messages=[{"role": "user", "content": prompt}]
        )

        return response.content[0].text

    def _template_based_report(self, results: List[Dict]) -> str:
        """
        åŸºæ–¼æ¨¡æ¿çš„å ±å‘Šç”Ÿæˆ
        """
        critical_count = len([r for r in results if r.get('risk_level') == 'critical'])
        high_count = len([r for r in results if r.get('risk_level') == 'high'])

        report = f"""
# ç¶²è·¯ç•°å¸¸åˆ†æå ±å‘Š

## åŸ·è¡Œæ‘˜è¦
ç™¼ç¾ {len(results)} å€‹ç•°å¸¸äº‹ä»¶ï¼Œå…¶ä¸­ {critical_count} å€‹åš´é‡ã€{high_count} å€‹é«˜é¢¨éšªã€‚

## é—œéµç™¼ç¾
"""
        for i, result in enumerate(results[:5], 1):
            report += f"\n{i}. {result.get('root_cause', 'æœªçŸ¥ç•°å¸¸')}"

        return report
```

---

## ä¸‰ã€æ•´åˆåˆ°å®šæœŸåˆ†ææµç¨‹

### 3.1 å¢å¼·çš„åˆ†æå¼•æ“

```python
# nad/core/enhanced_engine.py

from nad.ml.isolation_forest_detector import IsolationForestDetector
from nad.ml.behavior_classifier import BehaviorClassifier
from nad.ai.llm_reasoner import LLMReasoner

class EnhancedAnalysisEngine:
    """
    AI å¢å¼·çš„åˆ†æå¼•æ“
    """

    def __init__(self, config):
        self.config = config

        # å‚³çµ±çµ„ä»¶
        self.es_client = ElasticSearchClient(config)
        self.mysql_client = MySQLClient(config)
        self.analyzer = TrafficAnalyzer()

        # ML/AI çµ„ä»¶
        self.anomaly_detector = IsolationForestDetector()
        self.behavior_classifier = BehaviorClassifier()
        self.llm_reasoner = LLMReasoner(api_key=config.get('anthropic_api_key'))

        # åŸºæº–ç·š
        self.baseline_manager = BaselineManager()

    def analyze(self, timeframe='1h'):
        """
        å®Œæ•´çš„ AI å¢å¼·åˆ†ææµç¨‹
        """
        print("ğŸ” Step 1: æ”¶é›†æ•¸æ“š...")
        traffic_data = self._fetch_aggregated_data(timeframe)

        print("ğŸ“Š Step 2: çµ±è¨ˆåˆ†æ...")
        statistics = self.analyzer.analyze(traffic_data)

        print("ğŸ¤– Step 3: ML ç•°å¸¸æª¢æ¸¬...")
        ml_anomalies = self.anomaly_detector.predict(traffic_data)

        print("ğŸ¯ Step 4: è¡Œç‚ºåˆ†é¡...")
        behaviors = self.behavior_classifier.predict(
            [a for a in ml_anomalies if a['is_anomaly']]
        )

        print("ğŸ§  Step 5: AI æ·±åº¦åˆ†æ...")
        ai_insights = self._ai_deep_analysis(behaviors)

        print("ğŸ“ Step 6: ç”Ÿæˆå ±å‘Š...")
        report = self._generate_enhanced_report(
            statistics, ml_anomalies, behaviors, ai_insights
        )

        return {
            'statistics': statistics,
            'ml_anomalies': ml_anomalies,
            'behaviors': behaviors,
            'ai_insights': ai_insights,
            'report': report
        }

    def _ai_deep_analysis(self, behaviors):
        """
        å°é«˜é¢¨éšªç•°å¸¸é€²è¡Œ AI æ·±åº¦åˆ†æ
        """
        insights = []

        # åªå°é«˜é¢¨éšªç•°å¸¸é€²è¡Œ LLM åˆ†æ (ç¯€çœæˆæœ¬)
        high_risk_behaviors = [
            b for b in behaviors
            if b['behavior'] in ['port_scanning', 'ddos', 'data_exfiltration']
            and b['confidence'] > 0.7
        ]

        for behavior in high_risk_behaviors:
            # ç²å–ä¸Šä¸‹æ–‡ä¿¡æ¯
            context = self._get_context(behavior['src_ip'])

            # LLM åˆ†æ
            insight = self.llm_reasoner.analyze_anomaly(behavior, context)

            insights.append({
                'src_ip': behavior['src_ip'],
                'behavior': behavior['behavior'],
                'ai_analysis': insight
            })

        return insights

    def _get_context(self, ip):
        """
        ç²å– IP çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        # å¾ MySQL ç²å–è¨­å‚™ä¿¡æ¯
        device_info = self.mysql_client.get_device_by_ip(ip)

        # å¾åŸºæº–ç·šç²å–æ­·å²æ•¸æ“š
        baseline = self.baseline_manager.get_baseline(ip)

        return {
            'device_name': device_info.get('Name'),
            'device_type': device_info.get('Type'),
            'baseline_connections': baseline.get('avg_connections'),
            'deviation_percentage': self._calculate_deviation(ip, baseline)
        }
```

### 3.2 å®šæœŸåŸ·è¡Œè…³æœ¬

```python
#!/usr/bin/env python3
# scripts/ai_periodic_analysis.py

import schedule
import time
from nad.core.enhanced_engine import EnhancedAnalysisEngine
from nad.utils.config import load_config

def run_analysis():
    """
    åŸ·è¡Œ AI å¢å¼·åˆ†æ
    """
    config = load_config()
    engine = EnhancedAnalysisEngine(config)

    print(f"\n{'='*60}")
    print(f"é–‹å§‹åˆ†æ: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    # åŸ·è¡Œåˆ†æ
    result = engine.analyze(timeframe='1h')

    # ä¿å­˜å ±å‘Š
    report_path = f"reports/ai_analysis_{time.strftime('%Y%m%d_%H%M%S')}.md"
    with open(report_path, 'w') as f:
        f.write(result['report'])

    print(f"\nâœ… åˆ†æå®Œæˆï¼Œå ±å‘Šå·²ä¿å­˜: {report_path}\n")

    # å¦‚æœæœ‰åš´é‡ç•°å¸¸ï¼Œç™¼é€é€šçŸ¥ (å¯é¸)
    critical_anomalies = [
        a for a in result['ai_insights']
        if a['ai_analysis'].get('risk_level') == 'critical'
    ]

    if critical_anomalies:
        send_alert(critical_anomalies)

def send_alert(anomalies):
    """
    ç™¼é€å‘Šè­¦ (å¯é¸)
    """
    # å¯¦ä½œå‘Šè­¦é‚è¼¯
    pass

if __name__ == "__main__":
    # æ¯å°æ™‚åŸ·è¡Œä¸€æ¬¡
    schedule.every(1).hours.do(run_analysis)

    # ç«‹å³åŸ·è¡Œä¸€æ¬¡
    run_analysis()

    # æŒçºŒé‹è¡Œ
    while True:
        schedule.run_pending()
        time.sleep(60)
```

---

## å››ã€è¨“ç·´æ•¸æ“šæº–å‚™

### 4.1 æ¨™è¨˜æ•¸æ“šç”Ÿæˆ

```python
# scripts/generate_training_data.py

def generate_labeled_dataset():
    """
    å¾æ­·å²æ•¸æ“šç”Ÿæˆæ¨™è¨˜è¨“ç·´é›†
    """

    # 1. å¾å·²çŸ¥è‰¯å¥½æ™‚æ®µæ¡æ¨£æ­£å¸¸æµé‡
    normal_samples = sample_normal_traffic(
        date_range=('2025-11-01', '2025-11-10'),
        hours=(9, 18),  # å·¥ä½œæ™‚é–“
        exclude_ips=['192.168.10.135']  # æ’é™¤å·²çŸ¥ç•°å¸¸
    )

    # 2. å¾å·²çŸ¥äº‹ä»¶æ¨™è¨˜ç•°å¸¸æµé‡
    labeled_anomalies = [
        {
            'ip': '192.168.10.135',
            'date': '2025-11-11',
            'label': 'port_scanning',
            'confidence': 1.0
        },
        {
            'ip': '192.168.20.56',
            'date': '2025-11-11',
            'label': 'dns_abuse',
            'confidence': 1.0
        }
    ]

    # 3. åˆæˆç•°å¸¸æ¨£æœ¬ (å¯é¸)
    synthetic_anomalies = generate_synthetic_anomalies()

    # 4. çµ„åˆä¸¦ä¿å­˜
    dataset = {
        'normal': normal_samples,
        'anomalies': labeled_anomalies + synthetic_anomalies
    }

    save_dataset(dataset, 'training_data/labeled_flows.json')
```

### 4.2 è‡ªå‹•åŒ–æ¨™è¨˜

```python
def auto_label_with_confidence():
    """
    ä½¿ç”¨è¦å‰‡å¼•æ“è‡ªå‹•æ¨™è¨˜ï¼Œä¸¦è¨­å®šç½®ä¿¡åº¦
    """
    unlabeled_data = load_historical_data()

    auto_labeled = []
    for record in unlabeled_data:
        label, confidence = apply_labeling_rules(record)

        if confidence > 0.8:  # åªä¿ç•™é«˜ç½®ä¿¡åº¦æ¨™è¨˜
            auto_labeled.append({
                'features': record,
                'label': label,
                'confidence': confidence,
                'method': 'auto_rule_based'
            })

    return auto_labeled

def apply_labeling_rules(record):
    """
    è¦å‰‡å¼•æ“æ¨™è¨˜
    """
    if (record['unique_dests'] > 100 and
        record['avg_bytes'] < 5000 and
        record['connections'] > 1000):
        return 'network_scanning', 0.95

    if (record['dst_port_53_ratio'] > 0.8 and
        record['connections'] > 10000):
        return 'dns_abuse', 0.90

    if record['connections'] < 100:
        return 'normal', 0.85

    return 'unknown', 0.5
```

---

## äº”ã€æŒçºŒå­¸ç¿’æ©Ÿåˆ¶

### 5.1 æ¨¡å‹æ›´æ–°ç­–ç•¥

```python
class ModelUpdateManager:
    """
    ç®¡ç† ML æ¨¡å‹çš„æŒçºŒå­¸ç¿’
    """

    def __init__(self):
        self.update_frequency = 'weekly'  # æ¯é€±æ›´æ–°
        self.min_new_samples = 1000  # æœ€å°‘æ–°æ¨£æœ¬æ•¸

    def should_update(self):
        """
        åˆ¤æ–·æ˜¯å¦éœ€è¦æ›´æ–°æ¨¡å‹
        """
        last_update = self.get_last_update_time()
        new_samples_count = self.count_new_labeled_samples()

        time_elapsed = datetime.now() - last_update

        return (
            time_elapsed > timedelta(days=7) and
            new_samples_count >= self.min_new_samples
        )

    def update_models(self):
        """
        é‡æ–°è¨“ç·´æ¨¡å‹
        """
        print("ğŸ“š æ”¶é›†è¨“ç·´æ•¸æ“š...")
        training_data = self.collect_training_data()

        print("ğŸ‹ï¸ è¨“ç·´ Isolation Forest...")
        self.train_isolation_forest(training_data['normal'])

        print("ğŸ¯ è¨“ç·´è¡Œç‚ºåˆ†é¡å™¨...")
        self.train_behavior_classifier(training_data['labeled'])

        print("âœ… æ¨¡å‹æ›´æ–°å®Œæˆ")
        self.save_update_timestamp()

    def collect_training_data(self):
        """
        æ”¶é›†è¨“ç·´æ•¸æ“š
        """
        return {
            'normal': load_normal_samples(),
            'labeled': load_labeled_samples()
        }
```

### 5.2 äººå·¥åé¥‹å¾ªç’°

```python
def incorporate_human_feedback():
    """
    æ•´åˆäººå·¥åé¥‹æ”¹é€²æ¨¡å‹
    """

    # 1. æ”¶é›†åˆ†æå¸«åé¥‹
    feedback = load_analyst_feedback()
    # Format: {'prediction_id': 'xxx', 'actual_label': 'yyy', 'notes': '...'}

    # 2. æ›´æ–°æ¨™è¨˜æ•¸æ“š
    for fb in feedback:
        update_label(fb['prediction_id'], fb['actual_label'])

    # 3. è­˜åˆ¥èª¤å ±æ¨¡å¼
    false_positives = identify_false_positive_patterns(feedback)

    # 4. èª¿æ•´è¦å‰‡æˆ–é‡è¨“ç·´
    if len(false_positives) > 10:
        adjust_detection_rules(false_positives)
        retrain_models()
```

---

## å…­ã€æˆæœ¬èˆ‡æ•ˆèƒ½è€ƒé‡

### 6.1 LLM API æˆæœ¬å„ªåŒ–

```python
class CostOptimizedLLMReasoner(LLMReasoner):
    """
    æˆæœ¬å„ªåŒ–çš„ LLM æ¨è«–å™¨
    """

    def __init__(self, api_key, budget_per_hour=10):
        super().__init__(api_key)
        self.budget_per_hour = budget_per_hour
        self.cost_tracker = CostTracker()

    def analyze_anomaly(self, anomaly_data, context):
        """
        å¸¶æˆæœ¬æ§åˆ¶çš„åˆ†æ
        """
        # 1. å…ˆç”¨æœ¬åœ°è¦å‰‡å¿«é€Ÿè©•ä¼°
        quick_assessment = self._rule_based_analysis(anomaly_data, context)

        # 2. åªå°é«˜é¢¨éšªä¸”æ¨¡ç³Šçš„æ¡ˆä¾‹ä½¿ç”¨ LLM
        if (quick_assessment['risk_level'] in ['high', 'critical'] and
            self._is_ambiguous(anomaly_data)):

            # 3. æª¢æŸ¥é ç®—
            if self.cost_tracker.can_afford_query():
                return super().analyze_anomaly(anomaly_data, context)
            else:
                return quick_assessment  # é ç®—ç”¨å®Œï¼Œé™ç´šåˆ°è¦å‰‡å¼•æ“

        return quick_assessment

    def _is_ambiguous(self, data):
        """
        åˆ¤æ–·æ˜¯å¦ç‚ºæ¨¡ç³Šæ¡ˆä¾‹ï¼Œéœ€è¦ LLM åˆ†æ
        """
        # ä¾‹å¦‚: è¡Œç‚ºåˆ†é¡ç½®ä¿¡åº¦ä½æ–¼ 0.7
        return data.get('classification_confidence', 1.0) < 0.7
```

### 6.2 æ‰¹æ¬¡è™•ç†ç­–ç•¥

```python
def batch_llm_analysis(anomalies):
    """
    æ‰¹æ¬¡è™•ç†å¤šå€‹ç•°å¸¸ï¼Œç¯€çœæˆæœ¬
    """
    # å°‡ç›¸ä¼¼ç•°å¸¸åˆ†çµ„
    grouped = group_similar_anomalies(anomalies)

    results = []
    for group in grouped:
        # å°æ¯çµ„åªåˆ†æä¸€å€‹ä»£è¡¨æ€§æ¨£æœ¬
        representative = group[0]
        analysis = llm_reasoner.analyze_anomaly(representative)

        # å°‡çµè«–å¥—ç”¨åˆ°æ•´çµ„
        for anomaly in group:
            results.append({
                'anomaly': anomaly,
                'analysis': analysis,
                'note': f'åŸºæ–¼ç›¸ä¼¼æ¡ˆä¾‹æ¨è«– (ç¾¤çµ„å¤§å°: {len(group)})'
            })

    return results
```

---

## ä¸ƒã€ç¸½çµèˆ‡å»ºè­°

### æ¨è–¦å¯¦ä½œè·¯å¾‘:

#### Phase 1 (Week 1-2): åŸºç¤ ML
âœ… å¯¦ä½œ Isolation Forest ç•°å¸¸æª¢æ¸¬
âœ… å»ºç«‹åˆå§‹è¨“ç·´æ•¸æ“šé›†
âœ… æ•´åˆåˆ°å®šæœŸåˆ†ææµç¨‹

#### Phase 2 (Week 3-4): è¡Œç‚ºåˆ†é¡
âœ… è¨“ç·´è¡Œç‚ºåˆ†é¡å™¨
âœ… æ¨™è¨˜æ­·å²æ•¸æ“š
âœ… èª¿å„ªæ¨¡å‹åƒæ•¸

#### Phase 3 (Week 5-6): LLM å¢å¼·
âœ… æ•´åˆ LLM æ·±åº¦åˆ†æ (å¯é¸)
âœ… æˆæœ¬å„ªåŒ–ç­–ç•¥
âœ… è‡ªç„¶èªè¨€å ±å‘Šç”Ÿæˆ

#### Phase 4 (æŒçºŒ): æ”¹é€²å¾ªç’°
âœ… æ”¶é›†åé¥‹
âœ… æŒçºŒè¨“ç·´
âœ… æ¨¡å‹ç‰ˆæœ¬ç®¡ç†

### é—œéµå„ªå‹¢:

ğŸ¯ **æº–ç¢ºç‡æå‡**: ML å¯è­˜åˆ¥è¦å‰‡é›£ä»¥æè¿°çš„æ¨¡å¼
ğŸš€ **æ•ˆç‡æå‡**: è‡ªå‹•åŒ–åˆ†æï¼Œæ¸›å°‘äººå·¥å·¥ä½œé‡
ğŸ§  **æ·±åº¦æ´å¯Ÿ**: LLM æä¾›æ ¹å› åˆ†æå’Œå»ºè­°
ğŸ“ˆ **æŒçºŒæ”¹é€²**: å¾åé¥‹ä¸­å­¸ç¿’ï¼Œè¶Šç”¨è¶Šæº–

---

**æ–‡æª”ç‰ˆæœ¬:** 1.0
**æ›´æ–°æ—¥æœŸ:** 2025-11-11
