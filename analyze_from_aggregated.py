#!/usr/bin/env python3
"""
åŸºæ–¼ netflow_stats_5m èšåˆæ•¸æ“šçš„ç•°å¸¸åˆ†æå·¥å…·

å„ªå‹¢:
- æŸ¥è©¢é€Ÿåº¦å¿« 100+ å€ (åªéœ€æƒæèšåˆæ•¸æ“š)
- æ•¸æ“šé‡å° 99%
- å¯é€²è¡Œå¿«é€Ÿç•°å¸¸åµæ¸¬
"""

import requests
import json
from datetime import datetime, timedelta
from collections import defaultdict
import sys

# ES é…ç½®
ES_HOST = "http://localhost:9200"
AGG_INDEX = "netflow_stats_5m"

class AggregatedDataAnalyzer:
    """åŸºæ–¼èšåˆæ•¸æ“šçš„åˆ†æå™¨"""

    def __init__(self):
        self.es_url = f"{ES_HOST}/{AGG_INDEX}/_search"

    def analyze_recent(self, hours=1):
        """åˆ†ææœ€è¿‘ N å°æ™‚çš„èšåˆæ•¸æ“š"""
        print(f"\n{'='*70}")
        print(f"åŸºæ–¼èšåˆæ•¸æ“šçš„ç•°å¸¸åˆ†æ - éå» {hours} å°æ™‚")
        print(f"{'='*70}\n")

        # 1. æƒæåµæ¸¬
        print("ğŸ” æª¢æ¸¬æƒæè¡Œç‚º...")
        scanners = self.detect_scanning(hours)

        # 2. é«˜é€£ç·šæ•¸åµæ¸¬
        print("\nğŸ“Š æª¢æ¸¬ç•°å¸¸é«˜é€£ç·šæ•¸...")
        high_conn = self.detect_high_connections(hours)

        # 3. å¤§æµé‡åµæ¸¬
        print("\nğŸ’¾ æª¢æ¸¬ç•°å¸¸å¤§æµé‡...")
        high_traffic = self.detect_high_traffic(hours)

        # 4. ç”Ÿæˆå ±å‘Š
        print(f"\n{'='*70}")
        print("ç•°å¸¸æ‘˜è¦")
        print(f"{'='*70}")
        print(f"æƒæè¡Œç‚º: {len(scanners)} å€‹IP")
        print(f"é«˜é€£ç·šæ•¸: {len(high_conn)} å€‹IP")
        print(f"é«˜æµé‡: {len(high_traffic)} å€‹IP")

        return {
            'scanners': scanners,
            'high_connections': high_conn,
            'high_traffic': high_traffic
        }

    def detect_scanning(self, hours=1):
        """
        åµæ¸¬æƒæè¡Œç‚º
        ç‰¹å¾µ: é€£ç·šåˆ°å¤šå€‹ç›®çš„åœ°ä¸”å¹³å‡æµé‡å°
        """
        query = {
            "size": 100,
            "query": {
                "bool": {
                    "must": [
                        {"range": {"time_bucket": {"gte": f"now-{hours}h"}}},
                        {"range": {"unique_dsts": {"gte": 30}}},  # é€£ç·šåˆ°30+å€‹ç›®çš„åœ°
                        {"range": {"avg_bytes": {"lt": 10000}}},   # å¹³å‡æµé‡å°æ–¼10KB
                        {"range": {"flow_count": {"gte": 50}}}     # è‡³å°‘50å€‹é€£ç·š
                    ]
                }
            },
            "sort": [{"unique_dsts": "desc"}]
        }

        response = requests.post(self.es_url, json=query, headers={'Content-Type': 'application/json'})
        data = response.json()

        scanners = []
        seen_ips = set()

        for hit in data['hits']['hits']:
            src = hit['_source']
            ip = src['src_ip']

            if ip not in seen_ips:
                seen_ips.add(ip)

                # è¨ˆç®—æƒæè©•åˆ†
                scan_score = self._calculate_scan_score(src)

                scanners.append({
                    'ip': ip,
                    'unique_dsts': src['unique_dsts'],
                    'flow_count': src['flow_count'],
                    'avg_bytes': src['avg_bytes'],
                    'scan_score': scan_score,
                    'time_bucket': src['time_bucket']
                })

        # è¼¸å‡ºçµæœ
        if scanners:
            print(f"\nâš ï¸  ç™¼ç¾ {len(scanners)} å€‹å¯ç–‘æƒæ IP:\n")
            for i, scanner in enumerate(scanners[:10], 1):
                print(f"{i:2}. {scanner['ip']:15} | "
                      f"{scanner['unique_dsts']:3} ç›®çš„åœ° | "
                      f"{scanner['flow_count']:5,} é€£ç·š | "
                      f"è©•åˆ†: {scanner['scan_score']}/100")
        else:
            print("âœ“ æœªç™¼ç¾æƒæè¡Œç‚º")

        return scanners

    def detect_high_connections(self, hours=1):
        """åµæ¸¬ç•°å¸¸é«˜é€£ç·šæ•¸"""
        query = {
            "size": 0,
            "query": {
                "range": {"time_bucket": {"gte": f"now-{hours}h"}}
            },
            "aggs": {
                "per_ip": {
                    "terms": {
                        "field": "src_ip",
                        "size": 100,
                        "order": {"total_connections": "desc"}
                    },
                    "aggs": {
                        "total_connections": {"sum": {"field": "flow_count"}},
                        "total_traffic": {"sum": {"field": "total_bytes"}},
                        "avg_unique_dsts": {"avg": {"field": "unique_dsts"}}
                    }
                }
            }
        }

        response = requests.post(self.es_url, json=query, headers={'Content-Type': 'application/json'})
        data = response.json()

        high_conn = []
        threshold = 1000 * hours  # æ¯å°æ™‚1000é€£ç·š

        for bucket in data['aggregations']['per_ip']['buckets']:
            conns = bucket['total_connections']['value']
            if conns > threshold:
                high_conn.append({
                    'ip': bucket['key'],
                    'connections': int(conns),
                    'traffic_mb': bucket['total_traffic']['value'] / 1024 / 1024,
                    'avg_unique_dsts': bucket['avg_unique_dsts']['value']
                })

        # è¼¸å‡ºçµæœ
        if high_conn:
            print(f"\nâš ï¸  ç™¼ç¾ {len(high_conn)} å€‹é«˜é€£ç·šæ•¸ IP:\n")
            for i, item in enumerate(high_conn[:10], 1):
                print(f"{i:2}. {item['ip']:15} | "
                      f"{item['connections']:7,} é€£ç·š | "
                      f"{item['traffic_mb']:8.2f} MB | "
                      f"å¹³å‡ {item['avg_unique_dsts']:.0f} ç›®çš„åœ°")
        else:
            print(f"âœ“ æœªç™¼ç¾ç•°å¸¸é«˜é€£ç·šæ•¸ (é–¾å€¼: {threshold:,})")

        return high_conn

    def detect_high_traffic(self, hours=1):
        """åµæ¸¬ç•°å¸¸å¤§æµé‡"""
        query = {
            "size": 0,
            "query": {
                "range": {"time_bucket": {"gte": f"now-{hours}h"}}
            },
            "aggs": {
                "per_ip": {
                    "terms": {
                        "field": "src_ip",
                        "size": 50,
                        "order": {"total_traffic": "desc"}
                    },
                    "aggs": {
                        "total_traffic": {"sum": {"field": "total_bytes"}},
                        "total_connections": {"sum": {"field": "flow_count"}},
                        "max_single_flow": {"max": {"field": "max_bytes"}}
                    }
                }
            }
        }

        response = requests.post(self.es_url, json=query, headers={'Content-Type': 'application/json'})
        data = response.json()

        high_traffic = []
        threshold_gb = 1  # 1GB

        for bucket in data['aggregations']['per_ip']['buckets']:
            traffic = bucket['total_traffic']['value']
            traffic_gb = traffic / 1024 / 1024 / 1024

            if traffic_gb > threshold_gb:
                high_traffic.append({
                    'ip': bucket['key'],
                    'traffic_gb': traffic_gb,
                    'connections': int(bucket['total_connections']['value']),
                    'max_single_flow_mb': bucket['max_single_flow']['value'] / 1024 / 1024
                })

        # è¼¸å‡ºçµæœ
        if high_traffic:
            print(f"\nâš ï¸  ç™¼ç¾ {len(high_traffic)} å€‹å¤§æµé‡ IP (>{threshold_gb}GB):\n")
            for i, item in enumerate(high_traffic[:10], 1):
                print(f"{i:2}. {item['ip']:15} | "
                      f"{item['traffic_gb']:6.2f} GB | "
                      f"{item['connections']:6,} é€£ç·š | "
                      f"æœ€å¤§å–®é€£ç·š: {item['max_single_flow_mb']:.2f} MB")
        else:
            print(f"âœ“ æœªç™¼ç¾ç•°å¸¸å¤§æµé‡ (é–¾å€¼: {threshold_gb}GB)")

        return high_traffic

    def _calculate_scan_score(self, data):
        """è¨ˆç®—æƒæè©•åˆ† (0-100)"""
        score = 0

        # ç›®çš„åœ°æ•¸é‡
        if data['unique_dsts'] > 100:
            score += 40
        elif data['unique_dsts'] > 50:
            score += 30
        elif data['unique_dsts'] > 30:
            score += 20

        # å¹³å‡æµé‡å°ï¼ˆæƒæç‰¹å¾µï¼‰
        if data['avg_bytes'] < 1000:
            score += 30
        elif data['avg_bytes'] < 5000:
            score += 20
        elif data['avg_bytes'] < 10000:
            score += 10

        # é€£ç·šæ•¸
        if data['flow_count'] > 1000:
            score += 20
        elif data['flow_count'] > 500:
            score += 10
        elif data['flow_count'] > 100:
            score += 5

        # ç«¯å£å¤šæ¨£æ€§
        if data['unique_ports'] > 50:
            score += 10

        return min(score, 100)

    def analyze_ip(self, ip, hours=24):
        """æ·±åº¦åˆ†æç‰¹å®š IP"""
        print(f"\n{'='*70}")
        print(f"IP æ·±åº¦åˆ†æ: {ip}")
        print(f"{'='*70}\n")

        query = {
            "size": 500,
            "query": {
                "bool": {
                    "must": [
                        {"term": {"src_ip": ip}},
                        {"range": {"time_bucket": {"gte": f"now-{hours}h"}}}
                    ]
                }
            },
            "sort": [{"time_bucket": "asc"}]
        }

        response = requests.post(self.es_url, json=query, headers={'Content-Type': 'application/json'})
        data = response.json()

        if not data['hits']['hits']:
            print(f"âŒ æœªæ‰¾åˆ° {ip} åœ¨éå» {hours} å°æ™‚çš„æ•¸æ“š")
            return

        # çµ±è¨ˆåˆ†æ
        total_connections = 0
        total_traffic = 0
        max_unique_dsts = 0
        time_series = []

        print(f"æ™‚é–“åºåˆ—æ•¸æ“š (æ¯5åˆ†é˜):")
        print(f"{'-'*70}")
        print(f"{'æ™‚é–“':<20} {'é€£ç·šæ•¸':>8} {'æµé‡(MB)':>10} {'ç›®çš„åœ°':>8} {'å¹³å‡æµé‡':>12}")
        print(f"{'-'*70}")

        for hit in data['hits']['hits'][:20]:  # åªé¡¯ç¤ºå‰20ç­†
            src = hit['_source']
            time = datetime.fromisoformat(src['time_bucket'].replace('Z', '+00:00'))
            conns = src['flow_count']
            traffic = src['total_bytes'] / 1024 / 1024
            dsts = src['unique_dsts']
            avg = src['avg_bytes']

            total_connections += conns
            total_traffic += traffic
            max_unique_dsts = max(max_unique_dsts, dsts)

            print(f"{time.strftime('%Y-%m-%d %H:%M'):<20} "
                  f"{conns:8,} {traffic:10.2f} {dsts:8} {avg:12,.0f}")

        print(f"{'-'*70}")
        print(f"\nçµ±è¨ˆæ‘˜è¦:")
        print(f"  ç¸½é€£ç·šæ•¸: {total_connections:,}")
        print(f"  ç¸½æµé‡: {total_traffic:.2f} MB")
        print(f"  æœ€å¤§ç›®çš„åœ°æ•¸: {max_unique_dsts}")
        print(f"  å¹³å‡é€£ç·š/5åˆ†é˜: {total_connections / len(data['hits']['hits']):.0f}")

        # ç•°å¸¸åˆ¤æ–·
        print(f"\nç•°å¸¸æŒ‡æ¨™:")
        is_scanning = max_unique_dsts > 50 and total_connections > 500
        is_high_conn = total_connections > 10000
        is_high_traffic = total_traffic > 1000  # 1GB

        if is_scanning:
            print(f"  ğŸ”´ æƒæè¡Œç‚º: æ˜¯ (æœ€å¤§ç›®çš„åœ°: {max_unique_dsts})")
        else:
            print(f"  âœ… æƒæè¡Œç‚º: å¦")

        if is_high_conn:
            print(f"  ğŸ”´ é«˜é€£ç·šæ•¸: æ˜¯ ({total_connections:,})")
        else:
            print(f"  âœ… é«˜é€£ç·šæ•¸: å¦")

        if is_high_traffic:
            print(f"  ğŸ”´ é«˜æµé‡: æ˜¯ ({total_traffic:.2f} MB)")
        else:
            print(f"  âœ… é«˜æµé‡: å¦")

    def compare_with_baseline(self, ip, baseline_days=7):
        """èˆ‡åŸºæº–ç·šæ¯”è¼ƒï¼ˆæœªä¾†å¯¦ä½œï¼‰"""
        # TODO: å¯¦ä½œåŸºæº–ç·šæ¯”è¼ƒåŠŸèƒ½
        pass


def main():
    analyzer = AggregatedDataAnalyzer()

    if len(sys.argv) > 1:
        # æŒ‡å®š IP åˆ†æ
        ip = sys.argv[1]
        hours = int(sys.argv[2]) if len(sys.argv) > 2 else 24
        analyzer.analyze_ip(ip, hours)
    else:
        # ä¸€èˆ¬ç•°å¸¸åˆ†æ
        hours = 1
        result = analyzer.analyze_recent(hours)

        # å¯é¸ï¼šä¿å­˜çµæœ
        # with open(f'anomaly_report_{datetime.now().strftime("%Y%m%d_%H%M")}.json', 'w') as f:
        #     json.dump(result, f, indent=2)


if __name__ == "__main__":
    main()
