#!/usr/bin/env python3
"""
ç•°å¸¸é©—è­‰åˆ†æå·¥å…·

æ·±å…¥åˆ†æ Isolation Forest æª¢æ¸¬å‡ºçš„ç•°å¸¸ IPï¼Œ
æŸ¥è©¢åŸå§‹ netflow æ•¸æ“šï¼Œåˆ¤æ–·æ˜¯å¦ç‚ºçœŸæ­£çš„ç•°å¸¸è¡Œç‚ºã€‚
"""

import sys
import json
import math
import argparse
import warnings
from datetime import datetime, timedelta
from collections import Counter, defaultdict
from elasticsearch import Elasticsearch
from nad.utils.config_loader import load_config

# é—œé–‰ Elasticsearch å®‰å…¨è­¦å‘Š
warnings.filterwarnings('ignore', message='.*Elasticsearch built-in security features are not enabled.*')

# MySQL æ”¯æ´ï¼ˆå¯é¸ï¼‰
try:
    import pymysql
    MYSQL_AVAILABLE = True
except ImportError:
    MYSQL_AVAILABLE = False


class AnomalyVerifier:
    """ç•°å¸¸é©—è­‰å™¨"""

    def __init__(self, es_client, config):
        self.es = es_client
        self.config = config
        self.netflow_index = config.get('elasticsearch', {}).get('indices', {}).get('raw', 'radar_flow_collector-*')

        # åˆå§‹åŒ– MySQL é€£ç·šï¼ˆå¯é¸ï¼‰
        self.mysql_conn = None
        self.mysql_connected = False  # MySQL é€£ç·šç‹€æ…‹
        self.ip_name_cache = {}  # IP åç¨±å¿«å–
        self._init_mysql_connection()

    def _init_mysql_connection(self):
        """åˆå§‹åŒ– MySQL é€£ç·šï¼ˆå¦‚æœå¯ç”¨ä¸”é…ç½®æ­£ç¢ºï¼‰"""
        if not MYSQL_AVAILABLE:
            self.mysql_connected = False
            return

        mysql_config = self.config.get('mysql', {})
        if not mysql_config:
            self.mysql_connected = False
            return

        try:
            self.mysql_conn = pymysql.connect(
                host=mysql_config.get('host', 'localhost'),
                port=mysql_config.get('port', 3306),
                user=mysql_config.get('user'),
                password=mysql_config.get('password'),
                database=mysql_config.get('database'),
                connect_timeout=3
            )
            # æ¸¬è©¦é€£ç·š
            with self.mysql_conn.cursor() as cursor:
                cursor.execute("SELECT 1")
            self.mysql_connected = True
        except Exception as e:
            # é€£ç·šå¤±æ•—æ™‚éœé»˜è™•ç†ï¼Œä¸å½±éŸ¿ä¸»è¦åŠŸèƒ½
            self.mysql_conn = None
            self.mysql_connected = False

    def _get_ip_name(self, ip):
        """
        å¾ MySQL æŸ¥è©¢ IP å°æ‡‰çš„è¨­å‚™åç¨±

        Args:
            ip: IP åœ°å€

        Returns:
            è¨­å‚™åç¨±ï¼Œå¦‚æœæ‰¾ä¸åˆ°å‰‡è¿”å› None
        """
        # æª¢æŸ¥å¿«å–
        if ip in self.ip_name_cache:
            return self.ip_name_cache[ip]

        # å¦‚æœ MySQL ä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›
        if not self.mysql_conn:
            self.ip_name_cache[ip] = None
            return None

        try:
            with self.mysql_conn.cursor() as cursor:
                # å…ˆæŸ¥è©¢ Device è¡¨
                cursor.execute(
                    "SELECT Name FROM Device WHERE IP = %s LIMIT 1",
                    (ip,)
                )
                result = cursor.fetchone()
                if result and result[0]:
                    name = result[0].strip()
                    self.ip_name_cache[ip] = name
                    return name

                # å†æŸ¥è©¢ ip_alias è¡¨
                cursor.execute(
                    "SELECT alias FROM ip_alias WHERE ip = %s LIMIT 1",
                    (ip,)
                )
                result = cursor.fetchone()
                if result and result[0]:
                    name = result[0].strip()
                    self.ip_name_cache[ip] = name
                    return name

            # æ²’æ‰¾åˆ°
            self.ip_name_cache[ip] = None
            return None

        except Exception:
            # æŸ¥è©¢å¤±æ•—æ™‚éœé»˜è™•ç†
            self.ip_name_cache[ip] = None
            return None

    def _format_ip_with_name(self, ip):
        """
        æ ¼å¼åŒ– IP é¡¯ç¤ºï¼ˆå¦‚æœæœ‰è¨­å‚™åç¨±å‰‡é™„åŠ ï¼‰

        Args:
            ip: IP åœ°å€

        Returns:
            æ ¼å¼åŒ–çš„å­—ä¸²ï¼Œä¾‹å¦‚ "192.168.1.1 (Web Server)"
        """
        name = self._get_ip_name(ip)
        if name:
            return f"{ip} ({name})"
        return ip

    def verify_ip(self, src_ip, time_range_minutes=30):
        """
        æ·±å…¥åˆ†æå–®å€‹ç•°å¸¸ IPï¼ˆé›™å‘å®Œæ•´åˆ†æï¼‰

        Args:
            src_ip: è¦åˆ†æçš„ IP åœ°å€
            time_range_minutes: åˆ†ææ™‚é–“ç¯„åœï¼ˆåˆ†é˜ï¼‰

        Returns:
            åˆ†æå ±å‘Šå­—å…¸ï¼ˆåŒ…å«é›™å‘åˆ†æçµæœï¼‰
        """
        print(f"\n{'='*100}")
        print(f"ğŸ” æ·±å…¥åˆ†æ: {src_ip}")
        print(f"{'='*100}\n")

        # æŸ¥è©¢ä½œç‚ºæº IP çš„æµé‡
        flows_as_src = self._fetch_netflow_data(src_ip, time_range_minutes, role='src')
        # æŸ¥è©¢ä½œç‚ºç›®çš„åœ°çš„æµé‡
        flows_as_dst = self._fetch_netflow_data(src_ip, time_range_minutes, role='dst')

        print(f"ğŸ“Š ä½œç‚ºæº IP: {len(flows_as_src):,} ç­†è¨˜éŒ„")
        print(f"ğŸ“Š ä½œç‚ºç›®çš„åœ°: {len(flows_as_dst):,} ç­†è¨˜éŒ„\n")

        if not flows_as_src and not flows_as_dst:
            print(f"âš ï¸  æ²’æœ‰æ‰¾åˆ° {src_ip} çš„ netflow æ•¸æ“š")
            return None

        # é›™å‘åˆ†æï¼šåˆ†æå…©å€‹æ–¹å‘
        analysis_result = {
            'target_ip': src_ip,
            'time_range_minutes': time_range_minutes,
            'as_source': None,
            'as_destination': None,
        }

        # åˆ†æä½œç‚ºæº IP çš„æƒ…æ³
        if flows_as_src:
            print(f"ğŸ“Š åˆ†æä½œç‚ºæº IP çš„ {len(flows_as_src):,} ç­†è¨˜éŒ„...\n")
            analysis_result['as_source'] = {
                'role': 'src',
                'total_flows': len(flows_as_src),
                'basic_stats': self._analyze_basic_stats(flows_as_src),
                'destination_analysis': self._analyze_destinations(flows_as_src, 'src'),
                'port_analysis': self._analyze_ports(flows_as_src, 'src'),
                'protocol_analysis': self._analyze_protocols(flows_as_src),
                'temporal_analysis': self._analyze_temporal_pattern(flows_as_src),
                'traffic_analysis': self._analyze_traffic_pattern(flows_as_src),
                'behavioral_analysis': self._analyze_behavior(flows_as_src, 'src'),
            }

        # åˆ†æä½œç‚ºç›®çš„åœ° IP çš„æƒ…æ³
        if flows_as_dst:
            print(f"ğŸ“Š åˆ†æä½œç‚ºç›®çš„åœ° IP çš„ {len(flows_as_dst):,} ç­†è¨˜éŒ„...\n")
            analysis_result['as_destination'] = {
                'role': 'dst',
                'total_flows': len(flows_as_dst),
                'basic_stats': self._analyze_basic_stats(flows_as_dst),
                'destination_analysis': self._analyze_destinations(flows_as_dst, 'dst'),
                'port_analysis': self._analyze_ports(flows_as_dst, 'dst'),
                'protocol_analysis': self._analyze_protocols(flows_as_dst),
                'temporal_analysis': self._analyze_temporal_pattern(flows_as_dst),
                'traffic_analysis': self._analyze_traffic_pattern(flows_as_dst),
                'behavioral_analysis': self._analyze_behavior(flows_as_dst, 'dst'),
            }

        # ç”Ÿæˆç¶œåˆåˆ¤æ–·
        analysis_result['verdict'] = self._generate_bidirectional_verdict(analysis_result)

        # é¡¯ç¤ºé›™å‘å ±å‘Š
        self._print_bidirectional_report(analysis_result)

        return analysis_result

    def _fetch_netflow_data(self, ip, minutes, role='src'):
        """
        æŸ¥è©¢åŸå§‹ netflow æ•¸æ“šï¼ˆä½¿ç”¨ scroll API ç²å–æ‰€æœ‰è¨˜éŒ„ï¼‰

        Args:
            ip: IP åœ°å€
            minutes: æ™‚é–“ç¯„åœï¼ˆåˆ†é˜ï¼‰
            role: 'src' æˆ– 'dst'ï¼ŒæŒ‡å®šæŸ¥è©¢æº IP é‚„æ˜¯ç›®çš„ IP
        """
        # æ”¯æŒ IPv4 å’Œ IPv6
        if role == 'src':
            query = {
                "size": 10000,  # æ¯æ‰¹æ¬¡å– 10000 ç­†
                "query": {
                    "bool": {
                        "should": [
                            {"term": {"IPV4_SRC_ADDR": ip}},
                            {"term": {"IPV6_SRC_ADDR": ip}},
                        ],
                        "minimum_should_match": 1,
                        "filter": [
                            {"range": {"FLOW_START_MILLISECONDS": {
                                "gte": f"now-{minutes}m",
                                "lte": "now+1m"  # é¿å…æœªä¾†æ™‚é–“æˆ³ï¼ˆ2106å¹´çš„éŒ¯èª¤æ•¸æ“šï¼‰
                            }}}
                        ]
                    }
                },
                "sort": [{"FLOW_START_MILLISECONDS": "desc"}]
            }
        else:  # dst
            query = {
                "size": 10000,
                "query": {
                    "bool": {
                        "should": [
                            {"term": {"IPV4_DST_ADDR": ip}},
                            {"term": {"IPV6_DST_ADDR": ip}},
                        ],
                        "minimum_should_match": 1,
                        "filter": [
                            {"range": {"FLOW_START_MILLISECONDS": {
                                "gte": f"now-{minutes}m",
                                "lte": "now+1m"  # é¿å…æœªä¾†æ™‚é–“æˆ³
                            }}}
                        ]
                    }
                },
                "sort": [{"FLOW_START_MILLISECONDS": "desc"}]
            }

        try:
            # ä½¿ç”¨ scroll API ç²å–æ‰€æœ‰æ•¸æ“š
            flows_raw = []

            # åˆå§‹æŸ¥è©¢
            response = self.es.search(index=self.netflow_index, scroll='5m', **query)
            scroll_id = response.get('_scroll_id')
            hits = response['hits']['hits']
            flows_raw.extend([hit['_source'] for hit in hits])

            # ç¹¼çºŒæ»¾å‹•ç²å–
            while len(hits) > 0:
                response = self.es.scroll(scroll_id=scroll_id, scroll='5m')
                scroll_id = response.get('_scroll_id')
                hits = response['hits']['hits']
                flows_raw.extend([hit['_source'] for hit in hits])

                # é€²åº¦æç¤ºï¼ˆæ¯10000ç­†é¡¯ç¤ºä¸€æ¬¡ï¼‰
                if len(flows_raw) % 10000 == 0 and len(hits) > 0:
                    print(f"   å·²è¼‰å…¥ {len(flows_raw):,} ç­†è¨˜éŒ„...")

            # æ¸…ç† scroll
            if scroll_id:
                try:
                    self.es.clear_scroll(scroll_id=scroll_id)
                except:
                    pass

            # æ¨™æº–åŒ–æ¬„ä½åç¨±ï¼Œæ–¹ä¾¿å¾ŒçºŒè™•ç†
            flows = []
            for f in flows_raw:
                normalized = {
                    'src_ip': f.get('IPV4_SRC_ADDR') or f.get('IPV6_SRC_ADDR'),
                    'dst_ip': f.get('IPV4_DST_ADDR') or f.get('IPV6_DST_ADDR'),
                    'src_port': f.get('L4_SRC_PORT', 0),
                    'dst_port': f.get('L4_DST_PORT', 0),
                    'protocol': f.get('PROTOCOL', 0),
                    'in_bytes': f.get('IN_BYTES', 0),
                    'out_bytes': 0,  # åŸå§‹æ•¸æ“šæ²’æœ‰ out_bytes
                    'in_pkts': f.get('IN_PKTS', 0),
                    'out_pkts': 0,  # åŸå§‹æ•¸æ“šæ²’æœ‰ out_pkts
                    '@timestamp': f.get('FLOW_START_MILLISECONDS'),
                    'timestamp': f.get('FLOW_START_MILLISECONDS'),
                }
                flows.append(normalized)

            return flows
        except Exception as e:
            print(f"âŒ æŸ¥è©¢å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            return []

    def _analyze_basic_stats(self, flows):
        """åŸºæœ¬çµ±è¨ˆ"""
        total_bytes = sum(f.get('in_bytes', 0) + f.get('out_bytes', 0) for f in flows)
        total_packets = sum(f.get('in_pkts', 0) + f.get('out_pkts', 0) for f in flows)

        return {
            'total_flows': len(flows),
            'total_bytes': total_bytes,
            'total_packets': total_packets,
            'avg_bytes_per_flow': total_bytes / len(flows) if flows else 0,
            'avg_packets_per_flow': total_packets / len(flows) if flows else 0,
        }

    def _analyze_destinations(self, flows, role='src'):
        """
        ç›®çš„åœ°åˆ†æï¼ˆæ ¹æ“šè§’è‰²å‹•æ…‹èª¿æ•´ï¼‰

        Args:
            flows: æµé‡è¨˜éŒ„åˆ—è¡¨
            role: 'src' è¡¨ç¤ºåˆ†æç›®çš„åœ°ï¼Œ'dst' è¡¨ç¤ºåˆ†æä¾†æº
        """
        # æ ¹æ“šè§’è‰²é¸æ“‡è¦åˆ†æçš„ IP æ¬„ä½
        if role == 'src':
            # IP ä½œç‚ºæºï¼šåˆ†æå®ƒé€£åˆ°å“ªäº›ç›®çš„åœ°
            target_ips = [f['dst_ip'] for f in flows if 'dst_ip' in f]
            label = 'destinations'
        else:  # role == 'dst'
            # IP ä½œç‚ºç›®çš„åœ°ï¼šåˆ†æèª°é€£åˆ°å®ƒï¼ˆä¾†æºåˆ†æï¼‰
            target_ips = [f['src_ip'] for f in flows if 'src_ip' in f]
            label = 'sources'

        target_counter = Counter(target_ips)
        unique_targets = len(target_counter)

        # æ‰¾å‡ºæœ€å¸¸è¦‹çš„ç›®æ¨™ï¼ˆé¡¯ç¤ºæ‰€æœ‰ï¼Œæœ€å¤š50å€‹ï¼‰
        top_targets = target_counter.most_common(50)

        # è¨ˆç®—åˆ†å¸ƒç†µï¼ˆè¡¡é‡å¤šæ¨£æ€§ï¼‰
        total = len(target_ips)
        entropy = 0
        if total > 0:
            for count in target_counter.values():
                p = count / total
                entropy -= p * (math.log(p) if p > 0 else 0)

        return {
            'role': role,
            'label': label,
            'unique_destinations': unique_targets,
            'total_connections': len(target_ips),
            'dst_diversity_ratio': unique_targets / len(target_ips) if target_ips else 0,
            'top_destinations': [{'ip': ip, 'count': count, 'percentage': count/total*100}
                                for ip, count in top_targets],
            'is_highly_distributed': unique_targets > 50,  # é«˜åº¦åˆ†æ•£
            'is_concentrated': unique_targets < 5 and len(target_ips) > 100,  # é«˜åº¦é›†ä¸­
        }

    def _analyze_ports(self, flows, role='src'):
        """
        é€šè¨ŠåŸ åˆ†æï¼ˆæ ¹æ“šè§’è‰²å‹•æ…‹èª¿æ•´ï¼‰

        Args:
            flows: æµé‡è¨˜éŒ„åˆ—è¡¨
            role: 'src' è¡¨ç¤ºåˆ†æç›®çš„åŸ ï¼Œ'dst' è¡¨ç¤ºåˆ†æä¾†æºåŸ 
        """
        # æ ¹æ“šè§’è‰²é¸æ“‡è¦åˆ†æçš„é€šè¨ŠåŸ æ¬„ä½
        if role == 'src':
            # IP ä½œç‚ºæºï¼šåˆ†æç›®çš„é€šè¨ŠåŸ 
            target_ports = [f['dst_port'] for f in flows if 'dst_port' in f and f['dst_port'] > 0]
            label = 'destination_ports'
        else:  # role == 'dst'
            # IP ä½œç‚ºç›®çš„åœ°ï¼šåˆ†æä¾†æºé€šè¨ŠåŸ 
            target_ports = [f['src_port'] for f in flows if 'src_port' in f and f['src_port'] > 0]
            label = 'source_ports'

        port_counter = Counter(target_ports)
        unique_ports = len(port_counter)

        # åˆ†é¡å¸¸è¦‹é€šè¨ŠåŸ 
        well_known_ports = {
            80: 'HTTP', 443: 'HTTPS', 53: 'DNS', 22: 'SSH',
            25: 'SMTP', 3389: 'RDP', 21: 'FTP', 23: 'Telnet',
            445: 'SMB', 3306: 'MySQL', 5432: 'PostgreSQL', 6379: 'Redis'
        }

        port_distribution = defaultdict(int)
        for port in target_ports:
            if port < 1024:
                port_distribution['well_known'] += 1
            elif port < 49152:
                port_distribution['registered'] += 1
            else:
                port_distribution['dynamic'] += 1

        top_ports = port_counter.most_common(10)

        # æª¢æ¸¬æƒææ¨¡å¼
        is_scanning = unique_ports > 20 and len(target_ports) > 100
        is_sequential_scan = self._check_sequential_ports(list(port_counter.keys()))

        return {
            'role': role,
            'label': label,
            'unique_ports': unique_ports,
            'total_connections': len(target_ports),
            'port_diversity_ratio': unique_ports / len(target_ports) if target_ports else 0,
            'top_ports': [{'port': port,
                          'service': well_known_ports.get(port, 'Unknown'),
                          'count': count,
                          'percentage': count/len(target_ports)*100}
                         for port, count in top_ports],
            'port_distribution': dict(port_distribution),
            'is_scanning': is_scanning,
            'is_sequential_scan': is_sequential_scan,
        }

    def _check_sequential_ports(self, ports):
        """æª¢æŸ¥æ˜¯å¦ç‚ºé€£çºŒé€šè¨ŠåŸ æƒæ"""
        if len(ports) < 10:
            return False

        sorted_ports = sorted(ports)
        sequential_count = 0

        for i in range(len(sorted_ports) - 1):
            if sorted_ports[i+1] - sorted_ports[i] == 1:
                sequential_count += 1

        return sequential_count / len(sorted_ports) > 0.3  # 30% ä»¥ä¸Šé€£çºŒ

    def _analyze_protocols(self, flows):
        """å”å®šåˆ†æ"""
        protocols = [f.get('protocol', 0) for f in flows]
        proto_counter = Counter(protocols)

        proto_names = {
            1: 'ICMP', 6: 'TCP', 17: 'UDP'
        }

        proto_distribution = [
            {
                'protocol': proto_names.get(proto, f'Unknown({proto})'),
                'count': count,
                'percentage': count/len(protocols)*100
            }
            for proto, count in proto_counter.most_common()
        ]

        return {
            'protocol_distribution': proto_distribution,
            'is_icmp_heavy': proto_counter.get(1, 0) / len(protocols) > 0.5 if protocols else False,
            'is_udp_heavy': proto_counter.get(17, 0) / len(protocols) > 0.5 if protocols else False,
        }

    def _analyze_temporal_pattern(self, flows):
        """æ™‚é–“æ¨¡å¼åˆ†æ"""
        timestamps = []
        for f in flows:
            ts = f.get('@timestamp', f.get('timestamp', ''))
            if ts:
                try:
                    # å¦‚æœæ˜¯æ¯«ç§’æ™‚é–“æˆ³ï¼ˆæ•¸å­—ï¼‰
                    if isinstance(ts, (int, float)):
                        ts_dt = datetime.fromtimestamp(ts / 1000.0)
                    else:
                        # å¦‚æœæ˜¯ ISO å­—ä¸²
                        ts_dt = datetime.fromisoformat(str(ts).replace('Z', '+00:00'))
                    timestamps.append(ts_dt)
                except:
                    continue

        if len(timestamps) < 2:
            return {'insufficient_data': True}

        timestamps.sort()

        # è¨ˆç®—æ™‚é–“é–“éš”
        intervals = [(timestamps[i+1] - timestamps[i]).total_seconds()
                    for i in range(len(timestamps) - 1)]

        avg_interval = sum(intervals) / len(intervals) if intervals else 0

        # æª¢æ¸¬çªç™¼æµé‡
        burst_threshold = 1  # 1 ç§’å…§
        burst_count = sum(1 for interval in intervals if interval < burst_threshold)
        burst_ratio = burst_count / len(intervals) if intervals else 0

        return {
            'time_span_seconds': (timestamps[-1] - timestamps[0]).total_seconds(),
            'average_interval_seconds': avg_interval,
            'is_burst_traffic': burst_ratio > 0.5,
            'is_periodic': self._check_periodicity(intervals),
        }

    def _check_periodicity(self, intervals):
        """æª¢æŸ¥æ˜¯å¦ç‚ºé€±æœŸæ€§æµé‡"""
        if len(intervals) < 10:
            return False

        # ç°¡å–®çš„é€±æœŸæ€§æª¢æ¸¬ï¼šçœ‹é–“éš”çš„æ¨™æº–å·®
        import numpy as np
        std = np.std(intervals)
        mean = np.mean(intervals)

        # å¦‚æœæ¨™æº–å·®å¾ˆå°ï¼Œè¡¨ç¤ºå¾ˆè¦å¾‹
        cv = std / mean if mean > 0 else float('inf')
        return cv < 0.3  # è®Šç•°ä¿‚æ•¸ < 0.3 è¡¨ç¤ºé€±æœŸæ€§

    def _analyze_traffic_pattern(self, flows):
        """æµé‡æ¨¡å¼åˆ†æ"""
        bytes_list = [f.get('in_bytes', 0) + f.get('out_bytes', 0) for f in flows]
        packets_list = [f.get('in_pkts', 0) + f.get('out_pkts', 0) for f in flows]

        if not bytes_list:
            return {'insufficient_data': True}

        import numpy as np

        return {
            'mean_bytes': np.mean(bytes_list),
            'std_bytes': np.std(bytes_list),
            'max_bytes': np.max(bytes_list),
            'min_bytes': np.min(bytes_list),
            'median_bytes': np.median(bytes_list),
            'bytes_cv': np.std(bytes_list) / np.mean(bytes_list) if np.mean(bytes_list) > 0 else 0,
            'is_uniform_size': np.std(bytes_list) / np.mean(bytes_list) < 0.3 if np.mean(bytes_list) > 0 else False,
        }

    def _analyze_behavior(self, flows, role='src'):
        """
        è¡Œç‚ºåˆ†æï¼ˆæ ¹æ“šè§’è‰²å‹•æ…‹èª¿æ•´ï¼‰

        Args:
            flows: æµé‡è¨˜éŒ„åˆ—è¡¨
            role: 'src' æˆ– 'dst'
        """
        dst_analysis = self._analyze_destinations(flows, role)
        port_analysis = self._analyze_ports(flows, role)
        proto_analysis = self._analyze_protocols(flows)
        traffic_analysis = self._analyze_traffic_pattern(flows)

        behaviors = []

        # æª¢æŸ¥æ˜¯å¦ç‚ºå¸¸è¦‹æœå‹™å›æ‡‰æµé‡ï¼ˆåªåœ¨ä½œç‚ºæºæ™‚æª¢æŸ¥ï¼‰
        if role == 'src':
            src_ports = [f.get('src_port', f.get('dst_port', 0)) for f in flows if 'src_port' in f or 'dst_port' in f]
            src_port_counter = Counter(src_ports)
            most_common_src_port = src_port_counter.most_common(1)[0] if src_port_counter else (0, 0)

            # DNS æœå‹™å™¨å›æ‡‰ï¼ˆæºé€šè¨ŠåŸ  53ï¼Œç›®çš„é€šè¨ŠåŸ å¤šæ¨£åŒ–æ˜¯æ­£å¸¸çš„ï¼‰
            if most_common_src_port[0] == 53 and most_common_src_port[1] > len(flows) * 0.8:
                behaviors.append({
                    'type': 'DNS_SERVER_RESPONSE',
                    'severity': 'LOW',
                    'description': f"DNS ä¼ºæœå™¨å›æ‡‰æµé‡ï¼ˆæºé€šè¨ŠåŸ  53ï¼‰",
                    'evidence': {
                        'dns_responses': most_common_src_port[1],
                        'percentage': most_common_src_port[1] / len(flows) * 100
                    }
                })
                # DNS æœå‹™å™¨ä¸æ‡‰è©²è¢«æ¨™è¨˜ç‚ºé€šè¨ŠåŸ æƒæ
                return behaviors

            # Web æœå‹™å™¨å›æ‡‰ï¼ˆæºé€šè¨ŠåŸ  80/443ï¼‰
            if most_common_src_port[0] in [80, 443] and most_common_src_port[1] > len(flows) * 0.5:
                behaviors.append({
                    'type': 'WEB_SERVER_RESPONSE',
                    'severity': 'LOW',
                    'description': f"Web ä¼ºæœå™¨å›æ‡‰æµé‡ï¼ˆæºé€šè¨ŠåŸ  {most_common_src_port[0]}ï¼‰",
                    'evidence': {
                        'responses': most_common_src_port[1],
                        'percentage': most_common_src_port[1] / len(flows) * 100
                    }
                })
                return behaviors

        # æƒæè¡Œç‚ºæª¢æ¸¬
        if port_analysis['is_scanning'] or port_analysis['is_sequential_scan']:
            if role == 'src':
                behaviors.append({
                    'type': 'PORT_SCANNING',
                    'severity': 'HIGH',
                    'description': f"æª¢æ¸¬åˆ°é€šè¨ŠåŸ æƒæï¼š{port_analysis['unique_ports']} å€‹ä¸åŒç›®çš„åŸ ",
                    'evidence': {
                        'unique_ports': port_analysis['unique_ports'],
                        'is_sequential': port_analysis['is_sequential_scan']
                    }
                })
            else:  # role == 'dst'
                behaviors.append({
                    'type': 'UNDER_PORT_SCAN',
                    'severity': 'HIGH',
                    'description': f"æª¢æ¸¬åˆ°è¢«æƒæï¼šä¾†è‡ª {port_analysis['unique_ports']} å€‹ä¸åŒä¾†æºåŸ ",
                    'evidence': {
                        'unique_ports': port_analysis['unique_ports'],
                        'is_sequential': port_analysis['is_sequential_scan']
                    }
                })

        if dst_analysis['is_highly_distributed']:
            if role == 'src':
                behaviors.append({
                    'type': 'NETWORK_SCANNING',
                    'severity': 'HIGH',
                    'description': f"æª¢æ¸¬åˆ°ç¶²è·¯æƒæï¼š{dst_analysis['unique_destinations']} å€‹ç›®çš„åœ°",
                    'evidence': {
                        'unique_destinations': dst_analysis['unique_destinations'],
                        'dst_diversity': dst_analysis['dst_diversity_ratio']
                    }
                })
            else:  # role == 'dst'
                behaviors.append({
                    'type': 'UNDER_ATTACK',
                    'severity': 'HIGH',
                    'description': f"æª¢æ¸¬åˆ°é­å—æ”»æ“Šï¼šä¾†è‡ª {dst_analysis['unique_destinations']} å€‹ä¸åŒä¾†æº",
                    'evidence': {
                        'unique_sources': dst_analysis['unique_destinations'],
                        'source_diversity': dst_analysis['dst_diversity_ratio']
                    }
                })

        # DNS æ¿«ç”¨ï¼ˆåªåœ¨ä½œç‚ºæºæ™‚æª¢æŸ¥ï¼‰
        if role == 'src':
            dns_flows = [f for f in flows if f.get('dst_port') == 53]
            if len(dns_flows) > 1000 and traffic_analysis.get('mean_bytes', 0) < 500:
                behaviors.append({
                    'type': 'DNS_ABUSE',
                    'severity': 'MEDIUM',
                    'description': f"ç–‘ä¼¼ DNS æ¿«ç”¨ï¼š{len(dns_flows)} å€‹ DNS æŸ¥è©¢",
                    'evidence': {
                        'dns_query_count': len(dns_flows),
                        'avg_bytes': traffic_analysis.get('mean_bytes', 0)
                    }
                })

        # æ•¸æ“šå¤–æ´©/å…§æµ
        total_bytes = sum(f.get('in_bytes', 0) + f.get('out_bytes', 0) for f in flows)
        if total_bytes > 1_000_000_000 and dst_analysis['unique_destinations'] < 5:  # 1GB+
            if role == 'src':
                behaviors.append({
                    'type': 'DATA_EXFILTRATION',
                    'severity': 'HIGH',
                    'description': f"ç–‘ä¼¼æ•¸æ“šå¤–æ´©ï¼š{total_bytes/1024/1024/1024:.2f} GB åˆ°å°‘æ•¸ç›®çš„åœ°",
                    'evidence': {
                        'total_bytes': total_bytes,
                        'unique_destinations': dst_analysis['unique_destinations']
                    }
                })
            else:  # role == 'dst'
                behaviors.append({
                    'type': 'LARGE_DATA_RECEIVE',
                    'severity': 'MEDIUM',
                    'description': f"æ¥æ”¶å¤§é‡æ•¸æ“šï¼š{total_bytes/1024/1024/1024:.2f} GB ä¾†è‡ªå°‘æ•¸ä¾†æº",
                    'evidence': {
                        'total_bytes': total_bytes,
                        'unique_sources': dst_analysis['unique_destinations']
                    }
                })

        # ICMP æ¿«ç”¨
        if proto_analysis['is_icmp_heavy']:
            behaviors.append({
                'type': 'ICMP_ABUSE',
                'severity': 'MEDIUM',
                'description': "å¤§é‡ ICMP æµé‡ï¼ˆå¯èƒ½ç‚º ping æƒææˆ– DDoSï¼‰",
                'evidence': proto_analysis['protocol_distribution']
            })

        # æ­£å¸¸è¡Œç‚ºæ¨¡å¼
        if not behaviors:
            # æª¢æŸ¥æ˜¯å¦ç‚ºæ­£å¸¸æœå‹™
            top_ports = [p['port'] for p in port_analysis['top_ports'][:3]]
            if set(top_ports).intersection({80, 443, 53, 22}):
                behaviors.append({
                    'type': 'NORMAL_SERVICE',
                    'severity': 'LOW',
                    'description': "çœ‹èµ·ä¾†åƒæ­£å¸¸æœå‹™æµé‡",
                    'evidence': {
                        'top_ports': port_analysis['top_ports'][:3]
                    }
                })

        return behaviors

    def _generate_verdict(self, analysis):
        """ç”Ÿæˆæœ€çµ‚åˆ¤æ–·"""
        behaviors = analysis['behavioral_analysis']

        high_severity = sum(1 for b in behaviors if b['severity'] == 'HIGH')
        medium_severity = sum(1 for b in behaviors if b['severity'] == 'MEDIUM')
        low_severity_types = [b['type'] for b in behaviors if b['severity'] == 'LOW']

        # æœå‹™å™¨å›æ‡‰æµé‡æ‡‰è©²è¢«è¦–ç‚ºèª¤å ±
        if any(t in ['DNS_SERVER_RESPONSE', 'WEB_SERVER_RESPONSE', 'NORMAL_SERVICE'] for t in low_severity_types):
            verdict = 'FALSE_POSITIVE'
            confidence = 'HIGH'
            if 'DNS_SERVER_RESPONSE' in low_severity_types:
                recommendation = 'é€™æ˜¯ DNS æœå‹™å™¨å›æ‡‰æµé‡ï¼Œå»ºè­°èª¿æ•´ Transform æˆ–ç‰¹å¾µå·¥ç¨‹ä¾†æ’é™¤æœå‹™å™¨å›æ‡‰'
            elif 'WEB_SERVER_RESPONSE' in low_severity_types:
                recommendation = 'é€™æ˜¯ Web æœå‹™å™¨å›æ‡‰æµé‡ï¼Œå»ºè­°èª¿æ•´ Transform æˆ–ç‰¹å¾µå·¥ç¨‹ä¾†æ’é™¤æœå‹™å™¨å›æ‡‰'
            else:
                recommendation = 'å»ºè­°èª¿æ•´ç‰¹å¾µé–¾å€¼ï¼Œé™ä½æ­¤é¡èª¤å ±'
        elif high_severity > 0:
            verdict = 'TRUE_ANOMALY'
            confidence = 'HIGH'
            recommendation = 'å»ºè­°ç«‹å³èª¿æŸ¥'
        elif medium_severity > 0:
            verdict = 'SUSPICIOUS'
            confidence = 'MEDIUM'
            recommendation = 'å»ºè­°æŒçºŒè§€å¯Ÿ'
        else:
            verdict = 'UNCLEAR'
            confidence = 'LOW'
            recommendation = 'éœ€è¦æ›´å¤šæ•¸æ“šæˆ–äººå·¥åˆ¤æ–·'

        return {
            'verdict': verdict,
            'confidence': confidence,
            'recommendation': recommendation,
            'detected_behaviors': len(behaviors),
        }

    def _generate_bidirectional_verdict(self, analysis_result):
        """ç”Ÿæˆé›™å‘åˆ†æçš„ç¶œåˆåˆ¤æ–·"""
        src_analysis = analysis_result.get('as_source')
        dst_analysis = analysis_result.get('as_destination')

        all_behaviors = []
        high_severity_count = 0
        medium_severity_count = 0
        low_severity_types = []

        # æ”¶é›†å…©å€‹æ–¹å‘çš„è¡Œç‚º
        if src_analysis:
            src_behaviors = src_analysis.get('behavioral_analysis', [])
            all_behaviors.extend([{**b, 'direction': 'as_source'} for b in src_behaviors])
            high_severity_count += sum(1 for b in src_behaviors if b['severity'] == 'HIGH')
            medium_severity_count += sum(1 for b in src_behaviors if b['severity'] == 'MEDIUM')
            low_severity_types.extend([b['type'] for b in src_behaviors if b['severity'] == 'LOW'])

        if dst_analysis:
            dst_behaviors = dst_analysis.get('behavioral_analysis', [])
            all_behaviors.extend([{**b, 'direction': 'as_destination'} for b in dst_behaviors])
            high_severity_count += sum(1 for b in dst_behaviors if b['severity'] == 'HIGH')
            medium_severity_count += sum(1 for b in dst_behaviors if b['severity'] == 'MEDIUM')
            low_severity_types.extend([b['type'] for b in dst_behaviors if b['severity'] == 'LOW'])

        # ç¶œåˆåˆ¤æ–·
        if high_severity_count > 0:
            verdict = 'TRUE_ANOMALY'
            confidence = 'HIGH'
            if high_severity_count > 1:
                recommendation = f'æª¢æ¸¬åˆ° {high_severity_count} å€‹é«˜å±ç•°å¸¸è¡Œç‚ºï¼Œå»ºè­°ç«‹å³èª¿æŸ¥'
            else:
                recommendation = 'æª¢æ¸¬åˆ°é«˜å±ç•°å¸¸è¡Œç‚ºï¼Œå»ºè­°ç«‹å³èª¿æŸ¥'
        elif any(t in ['DNS_SERVER_RESPONSE', 'WEB_SERVER_RESPONSE', 'NORMAL_SERVICE'] for t in low_severity_types):
            # å¦‚æœæœ‰é«˜å±è¡Œç‚ºï¼Œä¸æ‡‰è©²è¢«æœå‹™å™¨å›æ‡‰æ©è“‹
            if high_severity_count == 0:
                verdict = 'FALSE_POSITIVE'
                confidence = 'HIGH'
                recommendation = 'ä¸»è¦ç‚ºæ­£å¸¸æœå‹™æµé‡ï¼Œå»ºè­°èª¿æ•´ç‰¹å¾µé–¾å€¼'
            else:
                verdict = 'MIXED'
                confidence = 'MEDIUM'
                recommendation = 'åŒæ™‚æª¢æ¸¬åˆ°æ­£å¸¸æœå‹™å’Œç•°å¸¸è¡Œç‚ºï¼Œéœ€é€²ä¸€æ­¥åˆ†æ'
        elif medium_severity_count > 0:
            verdict = 'SUSPICIOUS'
            confidence = 'MEDIUM'
            recommendation = 'æª¢æ¸¬åˆ°å¯ç–‘è¡Œç‚ºï¼Œå»ºè­°æŒçºŒè§€å¯Ÿ'
        elif all_behaviors:
            verdict = 'UNCLEAR'
            confidence = 'LOW'
            recommendation = 'è¡Œç‚ºæ¨¡å¼ä¸æ˜ç¢ºï¼Œéœ€è¦æ›´å¤šæ•¸æ“šæˆ–äººå·¥åˆ¤æ–·'
        else:
            verdict = 'NORMAL'
            confidence = 'MEDIUM'
            recommendation = 'æœªæª¢æ¸¬åˆ°æ˜é¡¯ç•°å¸¸è¡Œç‚º'

        return {
            'verdict': verdict,
            'confidence': confidence,
            'recommendation': recommendation,
            'total_behaviors': len(all_behaviors),
            'high_severity': high_severity_count,
            'medium_severity': medium_severity_count,
            'behaviors': all_behaviors,
        }

    def _print_bidirectional_report(self, analysis_result):
        """åˆ—å°é›™å‘åˆ†æå ±å‘Š"""
        print(f"\n{'='*100}")
        print(f"ğŸ“‹ é›™å‘åˆ†æå ±å‘Š")
        print(f"{'='*100}\n")

        target_ip = analysis_result['target_ip']
        src_analysis = analysis_result.get('as_source')
        dst_analysis = analysis_result.get('as_destination')

        # å ±å‘Šä½œç‚ºæº IP çš„åˆ†æ
        if src_analysis:
            print(f"{'â”€'*100}")
            print(f"ğŸ“¤ ä½œç‚ºæº IP çš„è¡Œç‚ºåˆ†æ")
            print(f"{'â”€'*100}\n")
            self._print_single_direction_report(src_analysis)

        # å ±å‘Šä½œç‚ºç›®çš„åœ° IP çš„åˆ†æ
        if dst_analysis:
            print(f"\n{'â”€'*100}")
            print(f"ğŸ“¥ ä½œç‚ºç›®çš„åœ° IP çš„è¡Œç‚ºåˆ†æ")
            print(f"{'â”€'*100}\n")
            self._print_single_direction_report(dst_analysis)

        # ç¶œåˆåˆ¤æ–·
        verdict = analysis_result['verdict']
        print(f"\n{'='*100}")
        print(f"ğŸ¯ ç¶œåˆåˆ¤æ–·")
        print(f"{'='*100}\n")

        verdict_icons = {
            'TRUE_ANOMALY': 'ğŸš¨',
            'SUSPICIOUS': 'âš ï¸',
            'FALSE_POSITIVE': 'âœ…',
            'MIXED': 'ğŸ”€',
            'UNCLEAR': 'â“',
            'NORMAL': 'âœ”ï¸'
        }
        verdict_names = {
            'TRUE_ANOMALY': 'çœŸå¯¦ç•°å¸¸',
            'SUSPICIOUS': 'å¯ç–‘è¡Œç‚º',
            'FALSE_POSITIVE': 'èª¤å ±ï¼ˆæ­£å¸¸æµé‡ï¼‰',
            'MIXED': 'æ··åˆæƒ…æ³',
            'UNCLEAR': 'ç„¡æ³•ç¢ºå®š',
            'NORMAL': 'æ­£å¸¸æµé‡'
        }

        icon = verdict_icons.get(verdict['verdict'], 'â“')
        name = verdict_names.get(verdict['verdict'], verdict['verdict'])

        print(f"{icon} åˆ¤æ–·çµæœ: {name}")
        print(f"ğŸ“Š ç½®ä¿¡åº¦: {verdict['confidence']}")
        print(f"ğŸ“ æª¢æ¸¬åˆ°çš„è¡Œç‚ºæ•¸é‡: {verdict['total_behaviors']}")
        if verdict['high_severity'] > 0:
            print(f"   ğŸ”´ é«˜å±è¡Œç‚º: {verdict['high_severity']}")
        if verdict['medium_severity'] > 0:
            print(f"   ğŸŸ¡ å¯ç–‘è¡Œç‚º: {verdict['medium_severity']}")
        print(f"\nğŸ’¡ å»ºè­°: {verdict['recommendation']}\n")

    def _print_single_direction_report(self, analysis):
        """åˆ—å°å–®ä¸€æ–¹å‘çš„åˆ†æå ±å‘Šï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        role = analysis.get('role', 'src')

        # åŸºæœ¬çµ±è¨ˆ
        stats = analysis['basic_stats']
        print(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"   â€¢ ç¸½é€£ç·šæ•¸: {stats['total_flows']:,}")
        print(f"   â€¢ ç¸½æµé‡: {stats['total_bytes']/1024/1024:.2f} MB")
        print(f"   â€¢ å¹³å‡æ¯é€£ç·š: {stats['avg_bytes_per_flow']:,.0f} bytes\n")

        # ç›®çš„åœ°/ä¾†æºåˆ†æ
        dst = analysis['destination_analysis']
        if role == 'src':
            print(f"ğŸ¯ ç›®çš„åœ°åˆ†æ:")
            print(f"   â€¢ ä¸åŒç›®çš„åœ°æ•¸é‡: {dst['unique_destinations']}")
            print(f"   â€¢ åˆ†æ•£åº¦: {dst['dst_diversity_ratio']:.3f}")
            # é¡¯ç¤º TOP 5 ç›®çš„åœ°
            if dst['top_destinations']:
                num_to_display = min(5, len(dst['top_destinations']))
                if num_to_display == len(dst['top_destinations']) and num_to_display < 5:
                    print(f"\n   é€£ç·šæ¬¡æ•¸æœ€å¤šçš„å‰ {num_to_display} å€‹ç›®çš„åœ°:")
                else:
                    print(f"\n   TOP 5 é€£ç·šç›®çš„åœ°:")
                for i, dst_info in enumerate(dst['top_destinations'][:5], 1):
                    ip = dst_info['ip']
                    if ip in ['0.0.0.0', '::']:
                        ip_display = f"{ip} (æ•¸æ“šç¼ºå¤±)"
                    else:
                        ip_display = self._format_ip_with_name(ip)
                    print(f"      {i}. {ip_display:50s} â†’ {dst_info['count']:5,} æ¬¡ ({dst_info['percentage']:.1f}%)")
            print()
        else:
            print(f"ğŸ¯ ä¾†æºåˆ†æ:")
            print(f"   â€¢ ä¸åŒä¾†æºæ•¸é‡: {dst['unique_destinations']}")
            print(f"   â€¢ åˆ†æ•£åº¦: {dst['dst_diversity_ratio']:.3f}")
            # é¡¯ç¤º TOP 5 ä¾†æº
            if dst['top_destinations']:
                num_to_display = min(5, len(dst['top_destinations']))
                if num_to_display == len(dst['top_destinations']) and num_to_display < 5:
                    print(f"\n   é€£ç·šæ¬¡æ•¸æœ€å¤šçš„å‰ {num_to_display} å€‹ä¾†æº:")
                else:
                    print(f"\n   TOP 5 é€£ç·šä¾†æº:")
                for i, src_info in enumerate(dst['top_destinations'][:5], 1):
                    ip = src_info['ip']
                    if ip in ['0.0.0.0', '::']:
                        ip_display = f"{ip} (æ•¸æ“šç¼ºå¤±)"
                    else:
                        ip_display = self._format_ip_with_name(ip)
                    print(f"      {i}. {ip_display:50s} â†’ {src_info['count']:5,} æ¬¡ ({src_info['percentage']:.1f}%)")
            print()

        # é€šè¨ŠåŸ åˆ†æ
        port = analysis['port_analysis']
        if role == 'src':
            print(f"ğŸ”Œ ç›®çš„é€šè¨ŠåŸ åˆ†æ:")
        else:
            print(f"ğŸ”Œ ä¾†æºé€šè¨ŠåŸ åˆ†æ:")
        print(f"   â€¢ ä¸åŒé€šè¨ŠåŸ æ•¸é‡: {port['unique_ports']}")

        # é¡¯ç¤º TOP 5 é€šè¨ŠåŸ 
        if port['top_ports']:
            if role == 'src':
                print(f"\n   TOP 5 ç›®çš„é€šè¨ŠåŸ :")
            else:
                print(f"\n   TOP 5 ä¾†æºé€šè¨ŠåŸ :")
            for i, port_info in enumerate(port['top_ports'][:5], 1):
                print(f"      {i}. {port_info['port']:5d} ({port_info['service']:15s}) â†’ {port_info['count']:5,} æ¬¡ ({port_info['percentage']:.1f}%)")
            print()
        else:
            print()

        # è¡Œç‚ºåˆ†æ
        behaviors = analysis['behavioral_analysis']
        if behaviors:
            print(f"ğŸ” æª¢æ¸¬åˆ°çš„è¡Œç‚º:")
            for behavior in behaviors:
                severity_icon = {'HIGH': 'ğŸ”´', 'MEDIUM': 'ğŸŸ¡', 'LOW': 'ğŸŸ¢'}.get(behavior['severity'], 'âšª')
                print(f"   {severity_icon} [{behavior['severity']}] {behavior['type']}")
                print(f"      {behavior['description']}")
        else:
            print(f"ğŸ” æœªæª¢æ¸¬åˆ°æ˜é¡¯ç•°å¸¸è¡Œç‚º")

    def _print_report(self, analysis):
        """åˆ—å°åˆ†æå ±å‘Š"""
        print(f"ğŸ“‹ åˆ†æå ±å‘Š\n")

        role = analysis.get('role', 'src')

        # åŸºæœ¬çµ±è¨ˆ
        stats = analysis['basic_stats']
        print(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
        print(f"   â€¢ ç¸½é€£ç·šæ•¸: {stats['total_flows']:,}")
        print(f"   â€¢ ç¸½æµé‡: {stats['total_bytes']/1024/1024:.2f} MB")
        print(f"   â€¢ å¹³å‡æ¯é€£ç·š: {stats['avg_bytes_per_flow']:,.0f} bytes")
        print()

        # ç›®çš„åœ°/ä¾†æºåˆ†æï¼ˆæ ¹æ“šè§’è‰²å‹•æ…‹æ¨™ç±¤ï¼‰
        dst = analysis['destination_analysis']
        if role == 'src':
            title = "ğŸ¯ ç›®çš„åœ°åˆ†æ:"
            count_label = "ä¸åŒç›®çš„åœ°æ•¸é‡"
            diversity_label = "ç›®çš„åœ°åˆ†æ•£åº¦"
            top_label = "ç›®çš„åœ°"
        else:  # role == 'dst'
            title = "ğŸ¯ ä¾†æºåˆ†æ:"
            count_label = "ä¸åŒä¾†æºæ•¸é‡"
            diversity_label = "ä¾†æºåˆ†æ•£åº¦"
            top_label = "ä¾†æº"

        print(title)
        print(f"   â€¢ {count_label}: {dst['unique_destinations']}")
        print(f"   â€¢ {diversity_label}: {dst['dst_diversity_ratio']:.3f}")

        # æª¢æŸ¥æ˜¯å¦ç‚ºæ•¸æ“šè³ªé‡å•é¡Œ
        has_zero_ip = any(d['ip'] == '0.0.0.0' or d['ip'] == '::' for d in dst['top_destinations'])
        if has_zero_ip:
            print(f"   âš ï¸  æ³¨æ„ï¼š{top_label}åŒ…å« 0.0.0.0ï¼Œé€™æ˜¯æ•¸æ“šæ¡é›†å•é¡Œï¼ˆå–®å‘æµé‡è¨˜éŒ„ï¼‰")
        elif dst['is_highly_distributed']:
            if role == 'src':
                print(f"   âš ï¸  é€£ç·šé«˜åº¦åˆ†æ•£ï¼ˆç–‘ä¼¼æƒæè¡Œç‚ºï¼‰")
            else:
                print(f"   âš ï¸  ä¾†æºé«˜åº¦åˆ†æ•£ï¼ˆç–‘ä¼¼é­å—æ”»æ“Šï¼‰")
        elif dst['is_concentrated']:
            if role == 'src':
                print(f"   âš ï¸  é€£ç·šé«˜åº¦é›†ä¸­ï¼ˆç–‘ä¼¼å®šå‘æ”»æ“Šï¼‰")
            else:
                print(f"   âš ï¸  ä¾†æºé«˜åº¦é›†ä¸­ï¼ˆè¢«å°‘æ•¸ä¾†æºå¤§é‡é€£ç·šï¼‰")

        # é¡¯ç¤º TOP 5 ç›®çš„åœ°/ä¾†æºï¼ˆçªå‡ºé¡¯ç¤ºï¼‰
        if dst['top_destinations']:
            # å‹•æ…‹èª¿æ•´æ¨™é¡Œï¼šå¦‚æœå°‘æ–¼5å€‹ï¼Œé¡¯ç¤ºå¯¦éš›æ•¸é‡
            num_to_display = min(5, len(dst['top_destinations']))
            if num_to_display == len(dst['top_destinations']) and num_to_display < 5:
                # å¦‚æœç¸½æ•¸å°‘æ–¼5å€‹ï¼Œé¡¯ç¤ºå¯¦éš›æ•¸é‡
                print(f"\n   é€£ç·šæ¬¡æ•¸æœ€å¤šçš„å‰ {num_to_display} å€‹{top_label}:")
            else:
                # å¦‚æœæœ‰5å€‹ä»¥ä¸Šï¼Œé¡¯ç¤º TOP 5
                print(f"\n   ğŸ” TOP 5 é€£ç·š{top_label}:")

            for i, dst_info in enumerate(dst['top_destinations'][:5], 1):
                ip = dst_info['ip']
                if ip in ['0.0.0.0', '::']:
                    ip_display = f"{ip} (æ•¸æ“šç¼ºå¤±)"
                else:
                    ip_display = self._format_ip_with_name(ip)
                print(f"      {i}. {ip_display:50s} â†’ {dst_info['count']:5,} æ¬¡ ({dst_info['percentage']:.1f}%)")

        # é¡¯ç¤ºé¡å¤–çš„ç›®çš„åœ°/ä¾†æºï¼ˆ6-20åï¼‰
        if len(dst['top_destinations']) > 5:
            num_to_show = min(20, len(dst['top_destinations']))
            remaining_to_show = num_to_show - 5
            if remaining_to_show > 0:
                print(f"\n   å…¶ä»–é«˜é »{top_label}ï¼ˆ6-{num_to_show}åï¼‰:")
                for i, dst_info in enumerate(dst['top_destinations'][5:num_to_show], 6):
                    ip = dst_info['ip']
                    if ip in ['0.0.0.0', '::']:
                        ip_display = f"{ip} (æ•¸æ“šç¼ºå¤±)"
                    else:
                        ip_display = self._format_ip_with_name(ip)
                    print(f"      {i}. {ip_display:50s} â†’ {dst_info['count']:5,} æ¬¡ ({dst_info['percentage']:.1f}%)")

            # å¦‚æœé‚„æœ‰æ›´å¤šï¼Œæç¤ºç”¨æˆ¶
            if len(dst['top_destinations']) > num_to_show:
                remaining = len(dst['top_destinations']) - num_to_show
                print(f"      ... é‚„æœ‰ {remaining} å€‹{top_label}")
        print()

        # é€šè¨ŠåŸ åˆ†æï¼ˆæ ¹æ“šè§’è‰²å‹•æ…‹æ¨™ç±¤ï¼‰
        port = analysis['port_analysis']
        if role == 'src':
            port_title = "ğŸ”Œ ç›®çš„é€šè¨ŠåŸ åˆ†æ:"
            port_label = "ç›®çš„é€šè¨ŠåŸ "
        else:  # role == 'dst'
            port_title = "ğŸ”Œ ä¾†æºé€šè¨ŠåŸ åˆ†æ:"
            port_label = "ä¾†æºé€šè¨ŠåŸ "

        print(port_title)
        print(f"   â€¢ ä¸åŒ{port_label}æ•¸é‡: {port['unique_ports']}")
        print(f"   â€¢ {port_label}åˆ†æ•£åº¦: {port['port_diversity_ratio']:.3f}")
        if port['is_scanning']:
            if role == 'src':
                print(f"   âš ï¸  ç–‘ä¼¼é€šè¨ŠåŸ æƒæ")
            else:
                print(f"   âš ï¸  ç–‘ä¼¼è¢«é€šè¨ŠåŸ æƒæ")
        if port['is_sequential_scan']:
            print(f"   âš ï¸  æª¢æ¸¬åˆ°é€£çºŒé€šè¨ŠåŸ æ¨¡å¼")

        # é¡¯ç¤º TOP 5 é€šè¨ŠåŸ 
        if port['top_ports']:
            print(f"\n   ğŸ” TOP 5 {port_label}:")
            for i, port_info in enumerate(port['top_ports'][:5], 1):
                print(f"      {i}. {port_info['port']:5d} ({port_info['service']:15s}) â†’ {port_info['count']:5,} æ¬¡ ({port_info['percentage']:.1f}%)")
        print()

        # å”å®šåˆ†æ
        proto = analysis['protocol_analysis']
        print(f"ğŸ“¡ å”å®šåˆ†æ:")
        for proto_info in proto['protocol_distribution']:
            print(f"   â€¢ {proto_info['protocol']:10s}: {proto_info['count']:5,} ({proto_info['percentage']:.1f}%)")
        print()

        # è¡Œç‚ºåˆ†æ
        behaviors = analysis['behavioral_analysis']
        print(f"ğŸ” è¡Œç‚ºåˆ†æ:")
        if behaviors:
            for behavior in behaviors:
                severity_icon = {'HIGH': 'ğŸ”´', 'MEDIUM': 'ğŸŸ¡', 'LOW': 'ğŸŸ¢'}.get(behavior['severity'], 'âšª')
                print(f"   {severity_icon} [{behavior['severity']}] {behavior['type']}")
                print(f"      {behavior['description']}")
            print()
        else:
            print(f"   âœ“ æœªæª¢æ¸¬åˆ°æ˜é¡¯ç•°å¸¸è¡Œç‚º")
            print()

        # æœ€çµ‚åˆ¤æ–·
        verdict = analysis['verdict']
        verdict_icons = {
            'TRUE_ANOMALY': 'ğŸš¨',
            'SUSPICIOUS': 'âš ï¸',
            'FALSE_POSITIVE': 'âœ…',
            'UNCLEAR': 'â“'
        }
        verdict_names = {
            'TRUE_ANOMALY': 'çœŸå¯¦ç•°å¸¸',
            'SUSPICIOUS': 'å¯ç–‘è¡Œç‚º',
            'FALSE_POSITIVE': 'èª¤å ±ï¼ˆæ­£å¸¸æµé‡ï¼‰',
            'UNCLEAR': 'ç„¡æ³•ç¢ºå®š'
        }

        print(f"{'='*100}")
        icon = verdict_icons.get(verdict['verdict'], 'â“')
        name = verdict_names.get(verdict['verdict'], verdict['verdict'])
        print(f"{icon} æœ€çµ‚åˆ¤æ–·: {name} (ç½®ä¿¡åº¦: {verdict['confidence']})")
        print(f"{'='*100}")
        print(f"ğŸ’¡ å»ºè­°: {verdict['recommendation']}\n")


def main():
    parser = argparse.ArgumentParser(description='é©—è­‰ Isolation Forest æª¢æ¸¬å‡ºçš„ç•°å¸¸')
    parser.add_argument('--ip', type=str, help='è¦åˆ†æçš„ IP åœ°å€')
    parser.add_argument('--minutes', type=int, default=30, help='åˆ†ææ™‚é–“ç¯„åœï¼ˆåˆ†é˜ï¼Œé»˜èª 30ï¼‰')
    parser.add_argument('--auto', action='store_true', help='è‡ªå‹•åˆ†ææœ€è¿‘æª¢æ¸¬åˆ°çš„ç•°å¸¸')
    parser.add_argument('--top', type=int, default=5, help='è‡ªå‹•æ¨¡å¼ä¸‹åˆ†æå‰ N å€‹ç•°å¸¸ï¼ˆé»˜èª 5ï¼‰')

    args = parser.parse_args()

    # è¼‰å…¥é…ç½®
    config = load_config()

    # é€£æ¥ Elasticsearch
    es_host = config.get('elasticsearch', {}).get('host', 'http://localhost:9200')
    es = Elasticsearch([es_host], timeout=30)

    if not es.ping():
        print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearch: {es_host}")
        sys.exit(1)

    print(f"âœ“ å·²é€£æ¥åˆ° Elasticsearch: {es_host}")

    verifier = AnomalyVerifier(es, config)

    # é¡¯ç¤º MySQL é€£ç·šç‹€æ…‹
    if verifier.mysql_connected:
        mysql_cfg = config.get('mysql', {})
        print(f"âœ“ å·²é€£æ¥åˆ° MySQL: {mysql_cfg.get('host')}:{mysql_cfg.get('port')} (è¨­å‚™åç¨±æŸ¥è©¢å·²å•Ÿç”¨)")
    else:
        print(f"â—‹ MySQL æœªé€£æ¥ (è¨­å‚™åç¨±æŸ¥è©¢åŠŸèƒ½åœç”¨)")
    print()

    if args.auto:
        print(f"ğŸ¤– è‡ªå‹•æ¨¡å¼ï¼šåˆ†ææœ€è¿‘æª¢æ¸¬åˆ°çš„å‰ {args.top} å€‹ç•°å¸¸\n")
        print("ğŸ’¡ æç¤ºï¼šå…ˆé‹è¡Œ realtime_detection.py ä¾†æª¢æ¸¬ç•°å¸¸\n")

        # é€™è£¡å¯ä»¥å¾æª¢æ¸¬çµæœä¸­è‡ªå‹•è®€å–ç•°å¸¸ IP
        # æš«æ™‚æç¤ºç”¨æˆ¶æ‰‹å‹•æŒ‡å®š
        print("è«‹å…ˆé‹è¡Œï¼špython3 realtime_detection.py --minutes 30")
        print("ç„¶å¾Œä½¿ç”¨ --ip åƒæ•¸æŒ‡å®šè¦åˆ†æçš„ IP\n")

    elif args.ip:
        verifier.verify_ip(args.ip, args.minutes)
    else:
        print("ç”¨æ³•:")
        print("  python3 verify_anomaly.py --ip 192.168.1.100 --minutes 30")
        print("  python3 verify_anomaly.py --auto --top 5")
        print()


if __name__ == '__main__':
    main()
