#!/usr/bin/env python3
"""
è‡ªé©æ‡‰é–¾å€¼è¨ˆç®—å™¨

åŸºæ–¼æ­·å²æ•¸æ“šçµ±è¨ˆè‡ªå‹•è¨ˆç®—ç‰¹å¾µé–¾å€¼
ä½¿ç”¨ç™¾åˆ†ä½æ•¸æ–¹æ³•ç¢ºä¿é–¾å€¼é©æ‡‰ç¶²è·¯æµé‡çš„å¯¦éš›åˆ†å¸ƒ
"""

import sys
import argparse
import numpy as np
import yaml
from datetime import datetime, timedelta
from elasticsearch import Elasticsearch
from nad.utils.config_loader import load_config


class AdaptiveThresholdCalculator:
    """
    è‡ªé©æ‡‰é–¾å€¼è¨ˆç®—å™¨

    åŸºæ–¼æ­·å²æ•¸æ“šçš„çµ±è¨ˆåˆ†æä¾†è¨ˆç®—æœ€å„ªé–¾å€¼
    """

    def __init__(self, es_client, config):
        self.es = es_client
        self.config = config
        self.index = config.get('elasticsearch', {}).get('indices', {}).get('aggregated', 'netflow_stats_5m')

    def calculate_thresholds(self, days=7, percentiles=None):
        """
        åŸºæ–¼æ­·å²æ•¸æ“šè¨ˆç®—è‡ªé©æ‡‰é–¾å€¼

        Args:
            days: åˆ†æå¤©æ•¸
            percentiles: ç™¾åˆ†ä½æ•¸å­—å…¸ (ç‰¹å¾µå -> ç™¾åˆ†ä½æ•¸)
                       ä¾‹å¦‚: {'high_connection': 95, 'scanning_dsts': 90}

        Returns:
            é–¾å€¼å­—å…¸
        """
        if percentiles is None:
            # é»˜èªç™¾åˆ†ä½æ•¸è¨­ç½®
            # 95% è¡¨ç¤ºåªæœ‰ 5% çš„æ•¸æ“šæœƒè¢«æ¨™è¨˜ç‚ºç•°å¸¸
            percentiles = {
                'high_connection': 95,      # é«˜é€£ç·šæ•¸ï¼š95ç™¾åˆ†ä½
                'scanning_dsts': 90,        # æƒæç›®çš„åœ°ï¼š90ç™¾åˆ†ä½
                'scanning_avg_bytes': 50,   # æƒæå¹³å‡æµé‡ï¼š50ç™¾åˆ†ä½ï¼ˆä¸­ä½æ•¸ï¼‰
                'small_packet': 25,         # å°å°åŒ…ï¼š25ç™¾åˆ†ä½
                'large_flow': 99,           # å¤§æµé‡ï¼š99ç™¾åˆ†ä½
            }

        print(f"\n{'='*100}")
        print(f"ğŸ“Š åŸºæ–¼æ­·å²æ•¸æ“šè¨ˆç®—è‡ªé©æ‡‰é–¾å€¼")
        print(f"{'='*100}\n")
        print(f"åˆ†ææœŸé–“: éå» {days} å¤©")
        print(f"æ•¸æ“šæº: {self.index}")
        print()

        # Step 1: æ”¶é›†æ­·å²æ•¸æ“š
        print("ğŸ“š Step 1: æ”¶é›†æ­·å²èšåˆæ•¸æ“š...")
        agg_data = self._fetch_historical_data(days)

        if not agg_data:
            print("âŒ æ²’æœ‰æ‰¾åˆ°æ­·å²æ•¸æ“šï¼")
            return None

        print(f"âœ“ æ”¶é›†åˆ° {len(agg_data):,} ç­†èšåˆè¨˜éŒ„\n")

        # Step 2: æå–ç‰¹å¾µå€¼
        print("ğŸ” Step 2: æå–ç‰¹å¾µå€¼åˆ†å¸ƒ...")
        features = self._extract_features(agg_data)

        # Step 3: è¨ˆç®—çµ±è¨ˆé‡
        print("ğŸ“ˆ Step 3: è¨ˆç®—çµ±è¨ˆé‡å’Œç™¾åˆ†ä½æ•¸...\n")
        statistics = self._calculate_statistics(features)

        # Step 4: åŸºæ–¼ç™¾åˆ†ä½æ•¸è¨ˆç®—é–¾å€¼
        print("ğŸ¯ Step 4: è¨ˆç®—è‡ªé©æ‡‰é–¾å€¼...\n")
        thresholds = self._calculate_thresholds_from_percentiles(features, percentiles, statistics)

        # Step 5: é¡¯ç¤ºçµæœ
        self._display_results(thresholds, statistics, percentiles)

        return thresholds

    def _fetch_historical_data(self, days):
        """å¾ ES ç²å–æ­·å²èšåˆæ•¸æ“š"""
        start_time = datetime.utcnow() - timedelta(days=days)

        query = {
            "size": 10000,  # æ¯æ¬¡ç²å–ä¸Šé™
            "query": {
                "range": {
                    "time_bucket": {
                        "gte": start_time.isoformat()
                    }
                }
            },
            "_source": [
                "src_ip", "flow_count", "total_bytes", "total_packets",
                "unique_dsts", "unique_src_ports", "unique_dst_ports",
                "avg_bytes", "max_bytes"
            ]
        }

        all_data = []

        try:
            # ä½¿ç”¨ scroll API ç²å–æ‰€æœ‰æ•¸æ“š
            response = self.es.search(
                index=self.index,
                body=query,
                scroll='5m'
            )

            scroll_id = response['_scroll_id']
            hits = response['hits']['hits']
            all_data.extend([hit['_source'] for hit in hits])

            # ç¹¼çºŒæ»¾å‹•ç²å–
            while len(hits) > 0:
                response = self.es.scroll(scroll_id=scroll_id, scroll='5m')
                scroll_id = response['_scroll_id']
                hits = response['hits']['hits']
                all_data.extend([hit['_source'] for hit in hits])

                if len(all_data) % 50000 == 0:
                    print(f"   å·²æ”¶é›† {len(all_data):,} ç­†...")

            # æ¸…ç† scroll
            self.es.clear_scroll(scroll_id=scroll_id)

        except Exception as e:
            print(f"âŒ æŸ¥è©¢å¤±æ•—: {e}")
            return []

        return all_data

    def _extract_features(self, agg_data):
        """å¾èšåˆæ•¸æ“šä¸­æå–ç‰¹å¾µå€¼"""
        features = {
            'flow_count': [],
            'unique_dsts': [],
            'avg_bytes': [],
            'max_bytes': [],
            'total_bytes': [],
            'unique_src_ports': [],
            'unique_dst_ports': [],
            'dst_diversity': [],
            'src_port_diversity': [],
            'dst_port_diversity': [],
        }

        for record in agg_data:
            flow_count = record.get('flow_count', 0)
            total_bytes = record.get('total_bytes', 0)

            if flow_count == 0:
                continue

            # åŸºç¤ç‰¹å¾µ
            features['flow_count'].append(flow_count)
            features['unique_dsts'].append(record.get('unique_dsts', 0))
            features['avg_bytes'].append(record.get('avg_bytes', 0))
            features['max_bytes'].append(record.get('max_bytes', 0))
            features['total_bytes'].append(total_bytes)
            features['unique_src_ports'].append(record.get('unique_src_ports', 0))
            features['unique_dst_ports'].append(record.get('unique_dst_ports', 0))

            # è¡ç”Ÿç‰¹å¾µ
            features['dst_diversity'].append(record.get('unique_dsts', 0) / flow_count)
            features['src_port_diversity'].append(record.get('unique_src_ports', 0) / flow_count)
            features['dst_port_diversity'].append(record.get('unique_dst_ports', 0) / flow_count)

        # è½‰æ›ç‚º numpy arrays
        for key in features:
            features[key] = np.array(features[key])

        print(f"âœ“ æå–ç‰¹å¾µ: {', '.join(features.keys())}\n")

        return features

    def _calculate_statistics(self, features):
        """è¨ˆç®—ç‰¹å¾µçš„çµ±è¨ˆé‡"""
        statistics = {}

        print(f"{'ç‰¹å¾µåç¨±':<25} {'æœ€å°å€¼':>12} {'ä¸­ä½æ•¸':>12} {'å¹³å‡å€¼':>12} {'95%ä½':>12} {'99%ä½':>12} {'æœ€å¤§å€¼':>12}")
        print(f"{'-'*100}")

        for feature_name, values in features.items():
            if len(values) == 0:
                continue

            stats = {
                'min': np.min(values),
                'p25': np.percentile(values, 25),
                'median': np.median(values),
                'p75': np.percentile(values, 75),
                'p90': np.percentile(values, 90),
                'p95': np.percentile(values, 95),
                'p99': np.percentile(values, 99),
                'max': np.max(values),
                'mean': np.mean(values),
                'std': np.std(values),
            }

            statistics[feature_name] = stats

            # æ ¼å¼åŒ–è¼¸å‡º
            print(f"{feature_name:<25} "
                  f"{stats['min']:>12,.1f} "
                  f"{stats['median']:>12,.1f} "
                  f"{stats['mean']:>12,.1f} "
                  f"{stats['p95']:>12,.1f} "
                  f"{stats['p99']:>12,.1f} "
                  f"{stats['max']:>12,.1f}")

        print()
        return statistics

    def _calculate_thresholds_from_percentiles(self, features, percentiles, statistics):
        """åŸºæ–¼ç™¾åˆ†ä½æ•¸è¨ˆç®—é–¾å€¼"""
        thresholds = {}

        # 1. high_connection: åŸºæ–¼ flow_count
        p = percentiles['high_connection']
        thresholds['high_connection'] = int(np.percentile(features['flow_count'], p))

        # 2. scanning_dsts: åŸºæ–¼ unique_dsts
        p = percentiles['scanning_dsts']
        thresholds['scanning_dsts'] = int(np.percentile(features['unique_dsts'], p))

        # 3. scanning_avg_bytes: åŸºæ–¼ avg_bytes çš„è¼ƒä½ç™¾åˆ†ä½ï¼ˆæƒæé€šå¸¸æ˜¯å°æµé‡ï¼‰
        p = percentiles['scanning_avg_bytes']
        thresholds['scanning_avg_bytes'] = int(np.percentile(features['avg_bytes'], p))

        # 4. small_packet: åŸºæ–¼ avg_bytes çš„ä½ç™¾åˆ†ä½
        p = percentiles['small_packet']
        thresholds['small_packet'] = int(np.percentile(features['avg_bytes'], p))

        # 5. large_flow: åŸºæ–¼ max_bytes çš„é«˜ç™¾åˆ†ä½
        p = percentiles['large_flow']
        thresholds['large_flow'] = int(np.percentile(features['max_bytes'], p))

        return thresholds

    def _display_results(self, thresholds, statistics, percentiles):
        """é¡¯ç¤ºçµæœå°æ¯”"""
        print(f"{'='*100}")
        print(f"ğŸ¯ è‡ªé©æ‡‰é–¾å€¼è¨ˆç®—çµæœ")
        print(f"{'='*100}\n")

        # ç²å–ç•¶å‰é…ç½®çš„é–¾å€¼
        current_thresholds = self.config.get('thresholds', {})

        print(f"{'åƒæ•¸':<30} {'ç•¶å‰å€¼':>15} {'å»ºè­°å€¼':>15} {'ç™¾åˆ†ä½':>10} {'è®ŠåŒ–':>15}")
        print(f"{'-'*100}")

        for param, new_value in thresholds.items():
            current_value = current_thresholds.get(param, 'N/A')
            percentile = percentiles.get(param, 'N/A')

            # è¨ˆç®—è®ŠåŒ–
            if isinstance(current_value, (int, float)):
                change = ((new_value - current_value) / current_value * 100)
                change_str = f"{change:+.1f}%"

                # ç”¨é¡è‰²æ¨™è¨˜é¡¯è‘—è®ŠåŒ–
                if abs(change) > 50:
                    change_str = f"ğŸ”´ {change_str}"
                elif abs(change) > 20:
                    change_str = f"ğŸŸ¡ {change_str}"
                else:
                    change_str = f"ğŸŸ¢ {change_str}"
            else:
                change_str = "æ–°å¢"

            # æ ¼å¼åŒ–æ•¸å€¼
            if isinstance(current_value, (int, float)):
                current_str = f"{current_value:,}"
            else:
                current_str = str(current_value)

            print(f"{param:<30} "
                  f"{current_str:>15} "
                  f"{new_value:>15,} "
                  f"P{percentile:>9} "
                  f"{change_str:>15}")

        print(f"\n{'='*100}")
        print(f"ğŸ’¡ æ‡‰ç”¨å»ºè­°")
        print(f"{'='*100}\n")

        print("1ï¸âƒ£  è‡ªå‹•æ›´æ–°é…ç½®æ–‡ä»¶:")
        print(f"   python3 {sys.argv[0]} --days 7 --apply\n")

        print("2ï¸âƒ£  æ‰‹å‹•æ›´æ–° nad/config.yaml:")
        print("   ç·¨è¼¯ thresholds éƒ¨åˆ†:\n")
        print("   thresholds:")
        for param, value in thresholds.items():
            print(f"     {param}: {value}")
        print()

        print("3ï¸âƒ£  é‡æ–°è¨“ç·´æ¨¡å‹:")
        print("   python3 train_isolation_forest.py --days 7\n")

        print("4ï¸âƒ£  é©—è­‰æ•ˆæœ:")
        print("   python3 realtime_detection.py --minutes 30\n")

    def apply_thresholds(self, thresholds, config_path='nad/config.yaml'):
        """
        æ‡‰ç”¨é–¾å€¼åˆ°é…ç½®æ–‡ä»¶

        Args:
            thresholds: è¨ˆç®—å‡ºçš„é–¾å€¼å­—å…¸
            config_path: é…ç½®æ–‡ä»¶è·¯å¾‘
        """
        print(f"\n{'='*100}")
        print(f"ğŸ’¾ æ‡‰ç”¨é–¾å€¼åˆ°é…ç½®æ–‡ä»¶")
        print(f"{'='*100}\n")

        try:
            # è®€å–ç¾æœ‰é…ç½®
            with open(config_path, 'r', encoding='utf-8') as f:
                original_config = f.read()

            config = yaml.safe_load(original_config)

            # å‚™ä»½åŸé…ç½®ï¼ˆåœ¨ä¿®æ”¹ä¹‹å‰ï¼‰
            backup_path = f"{config_path}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            with open(backup_path, 'w', encoding='utf-8') as f:
                f.write(original_config)
            print(f"âœ“ å·²å‚™ä»½åŸé…ç½®: {backup_path}")
            print()

            # æ›´æ–°é–¾å€¼
            if 'thresholds' not in config:
                config['thresholds'] = {}

            print("ğŸ“ æ›´æ–°é–¾å€¼:")
            for param, value in thresholds.items():
                old_value = config['thresholds'].get(param, 'N/A')
                config['thresholds'][param] = value

                # æ ¼å¼åŒ–é¡¯ç¤º
                if isinstance(old_value, (int, float)):
                    change = ((value - old_value) / old_value * 100) if old_value != 0 else 0
                    print(f"   {param:<25} {old_value:>15,} â†’ {value:>15,}  ({change:+.1f}%)")
                else:
                    print(f"   {param:<25} {old_value:>15} â†’ {value:>15,}  (æ–°å¢)")

            # å¯«å…¥æ–°é…ç½®
            with open(config_path, 'w', encoding='utf-8') as f:
                yaml.dump(config, f, default_flow_style=False, allow_unicode=True, sort_keys=False)

            print(f"\nâœ“ å·²æ›´æ–°é…ç½®æ–‡ä»¶: {config_path}")
            print(f"\nğŸ’¡ å¦‚éœ€å›æ»¾ï¼ŒåŸ·è¡Œ:")
            print(f"   cp {backup_path} {config_path}")
            print(f"\n{'='*100}\n")

            return True

        except Exception as e:
            print(f"âŒ æ›´æ–°é…ç½®å¤±æ•—: {e}")
            return False


def main():
    parser = argparse.ArgumentParser(
        description='åŸºæ–¼æ­·å²æ•¸æ“šè¨ˆç®—è‡ªé©æ‡‰é–¾å€¼',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # åˆ†æéå» 7 å¤©çš„æ•¸æ“š
  python3 calculate_adaptive_thresholds.py --days 7

  # åˆ†æä¸¦è‡ªå‹•æ‡‰ç”¨åˆ°é…ç½®æ–‡ä»¶
  python3 calculate_adaptive_thresholds.py --days 7 --apply

  # ä½¿ç”¨è‡ªå®šç¾©ç™¾åˆ†ä½æ•¸
  python3 calculate_adaptive_thresholds.py --days 14 --percentile high_connection=98

  # åªè¨ˆç®—ç‰¹å®šåƒæ•¸
  python3 calculate_adaptive_thresholds.py --days 7 --params high_connection,scanning_dsts
        """
    )

    parser.add_argument('--days', type=int, default=7,
                       help='åˆ†æå¤©æ•¸ (é»˜èª: 7)')
    parser.add_argument('--apply', action='store_true',
                       help='è‡ªå‹•æ‡‰ç”¨åˆ°é…ç½®æ–‡ä»¶')
    parser.add_argument('--config', type=str, default='nad/config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾‘ (é»˜èª: nad/config.yaml)')
    parser.add_argument('--percentile', action='append',
                       help='è‡ªå®šç¾©ç™¾åˆ†ä½æ•¸ (æ ¼å¼: param=value, ä¾‹å¦‚: high_connection=98)')
    parser.add_argument('--params', type=str,
                       help='åªè¨ˆç®—æŒ‡å®šåƒæ•¸ (é€—è™Ÿåˆ†éš”)')

    args = parser.parse_args()

    # è¼‰å…¥é…ç½®
    config = load_config()

    # é€£æ¥ Elasticsearch
    es_host = config.get('elasticsearch', {}).get('host', 'http://localhost:9200')
    es = Elasticsearch([es_host], timeout=30)

    if not es.ping():
        print(f"âŒ ç„¡æ³•é€£æ¥åˆ° Elasticsearch: {es_host}")
        sys.exit(1)

    print(f"âœ“ å·²é€£æ¥åˆ° Elasticsearch: {es_host}")

    # å‰µå»ºè¨ˆç®—å™¨
    calculator = AdaptiveThresholdCalculator(es, config)

    # è§£æè‡ªå®šç¾©ç™¾åˆ†ä½æ•¸
    percentiles = None
    if args.percentile:
        percentiles = {}
        for item in args.percentile:
            param, value = item.split('=')
            percentiles[param.strip()] = float(value.strip())

    # è¨ˆç®—é–¾å€¼
    thresholds = calculator.calculate_thresholds(
        days=args.days,
        percentiles=percentiles
    )

    if not thresholds:
        print("âŒ é–¾å€¼è¨ˆç®—å¤±æ•—")
        sys.exit(1)

    # å¦‚æœæŒ‡å®šäº†ç‰¹å®šåƒæ•¸ï¼Œåªä¿ç•™é€™äº›
    if args.params:
        param_list = [p.strip() for p in args.params.split(',')]
        thresholds = {k: v for k, v in thresholds.items() if k in param_list}

    # æ‡‰ç”¨é–¾å€¼
    if args.apply:
        success = calculator.apply_thresholds(thresholds, args.config)
        if success:
            print("âœ… é–¾å€¼å·²æˆåŠŸæ‡‰ç”¨ï¼")
            print("âš ï¸  è«‹è¨˜å¾—é‡æ–°è¨“ç·´æ¨¡å‹: python3 train_isolation_forest.py --days 7")
        else:
            sys.exit(1)


if __name__ == '__main__':
    main()
