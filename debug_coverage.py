#!/usr/bin/env python3
"""
èª¿è©¦è¦†è“‹ç‡å•é¡Œ - ç²¾ç¢ºæ¯”è¼ƒåŸå§‹æ•¸æ“šå’Œèšåˆæ•¸æ“š
"""

import requests
from datetime import datetime, timedelta

ES_HOST = "http://localhost:9200"

def compare_time_aligned():
    """ä½¿ç”¨æ™‚é–“å°é½Šçš„æ–¹å¼æ¯”è¼ƒ"""

    # è¨ˆç®—å°é½Šåˆ°5åˆ†é˜é‚Šç•Œçš„æ™‚é–“ç¯„åœ
    now = datetime.now()

    # å¾€å›æ¨1å°æ™‚ï¼Œä¸¦å°é½Šåˆ°5åˆ†é˜é‚Šç•Œ
    end_time = now.replace(second=0, microsecond=0)
    end_time = end_time.replace(minute=(end_time.minute // 5) * 5)

    start_time = end_time - timedelta(hours=1)

    start_ms = int(start_time.timestamp() * 1000)
    end_ms = int(end_time.timestamp() * 1000)

    print("=" * 70)
    print("æ™‚é–“å°é½Šè¦†è“‹ç‡é©—è­‰")
    print("=" * 70)
    print(f"é–‹å§‹æ™‚é–“: {start_time}")
    print(f"çµæŸæ™‚é–“: {end_time}")
    print(f"æ™‚é–“ç¯„åœ: {(end_time - start_time).total_seconds() / 3600:.1f} å°æ™‚")
    print()

    # 1. æŸ¥è©¢åŸå§‹ç´¢å¼•
    print("ğŸ” æŸ¥è©¢åŸå§‹ç´¢å¼•...")
    raw_query = {
        "size": 0,
        "query": {
            "range": {
                "FLOW_START_MILLISECONDS": {
                    "gte": start_ms,
                    "lt": end_ms
                }
            }
        },
        "aggs": {
            "unique_ips": {
                "cardinality": {
                    "field": "IPV4_SRC_ADDR",
                    "precision_threshold": 10000
                }
            },
            "total_flows": {
                "value_count": {"field": "IPV4_SRC_ADDR"}
            },
            "time_range": {
                "stats": {"field": "FLOW_START_MILLISECONDS"}
            }
        }
    }

    resp = requests.post(
        f"{ES_HOST}/radar_flow_collector-*/_search",
        json=raw_query,
        headers={'Content-Type': 'application/json'}
    )

    raw_data = resp.json()
    raw_unique_ips = int(raw_data['aggregations']['unique_ips']['value'])
    raw_total = int(raw_data['aggregations']['total_flows']['value'])

    time_stats = raw_data['aggregations']['time_range']
    actual_min = datetime.fromtimestamp(time_stats['min'] / 1000)
    actual_max = datetime.fromtimestamp(time_stats['max'] / 1000)

    print(f"  å”¯ä¸€ IP: {raw_unique_ips:,}")
    print(f"  ç¸½æµè¨˜éŒ„: {raw_total:,}")
    print(f"  å¯¦éš›æ™‚é–“ç¯„åœ: {actual_min} åˆ° {actual_max}")
    print()

    # 2. æŸ¥è©¢èšåˆç´¢å¼• (ä½¿ç”¨ time_bucket)
    print("ğŸ” æŸ¥è©¢èšåˆç´¢å¼•...")

    # è½‰æ›ç‚º ISO æ ¼å¼çµ¦ time_bucket
    start_iso = start_time.isoformat() + "Z"
    end_iso = end_time.isoformat() + "Z"

    agg_query = {
        "size": 0,
        "query": {
            "range": {
                "time_bucket": {
                    "gte": start_iso,
                    "lt": end_iso
                }
            }
        },
        "aggs": {
            "unique_ips": {
                "cardinality": {
                    "field": "src_ip",
                    "precision_threshold": 10000
                }
            },
            "total_buckets": {
                "value_count": {"field": "src_ip"}
            },
            "time_buckets": {
                "cardinality": {
                    "field": "time_bucket"
                }
            }
        }
    }

    resp = requests.post(
        f"{ES_HOST}/netflow_stats_5m/_search",
        json=agg_query,
        headers={'Content-Type': 'application/json'}
    )

    agg_data = resp.json()
    agg_unique_ips = int(agg_data['aggregations']['unique_ips']['value'])
    agg_total = int(agg_data['aggregations']['total_buckets']['value'])
    agg_time_buckets = int(agg_data['aggregations']['time_buckets']['value'])

    print(f"  å”¯ä¸€ IP: {agg_unique_ips:,}")
    print(f"  èšåˆè¨˜éŒ„æ•¸: {agg_total:,}")
    expected_buckets = 60 // 5
    print(f"  æ™‚é–“æ¡¶æ•¸: {agg_time_buckets} (é æœŸ: {expected_buckets})")
    print()

    # 3. è¨ˆç®—è¦†è“‹ç‡
    print("=" * 70)
    print("è¦†è“‹ç‡åˆ†æ")
    print("=" * 70)

    if raw_unique_ips > 0:
        coverage = (agg_unique_ips / raw_unique_ips) * 100
        missing = raw_unique_ips - agg_unique_ips

        print(f"åŸå§‹ç´¢å¼•å”¯ä¸€ IP:    {raw_unique_ips:>8,}")
        print(f"èšåˆç´¢å¼•å”¯ä¸€ IP:    {agg_unique_ips:>8,}")
        print(f"éºæ¼ IP æ•¸:         {missing:>8,}")
        print(f"è¦†è“‹ç‡:            {coverage:>8.2f}%")
        print()

        # æ•¸æ“šå£“ç¸®æ¯”
        compression = (1 - agg_total / raw_total) * 100 if raw_total > 0 else 0
        reduction_ratio = raw_total / agg_total if agg_total > 0 else 0

        print(f"æ•¸æ“šå£“ç¸®:")
        print(f"  åŸå§‹: {raw_total:,} ç­†")
        print(f"  èšåˆ: {agg_total:,} ç­†")
        print(f"  å£“ç¸®ç‡: {compression:.2f}%")
        print(f"  ç¸®æ¸›æ¯”ä¾‹: {reduction_ratio:.1f}x")
        print()

        # æ¯å€‹æ™‚é–“æ¡¶çš„å¹³å‡ IP æ•¸
        if agg_time_buckets > 0:
            avg_ips_per_bucket = agg_total / agg_time_buckets
            print(f"å¹³å‡æ¯å€‹æ™‚é–“æ¡¶: {avg_ips_per_bucket:.0f} æ¢èšåˆè¨˜éŒ„")
            print()

        # è¨ºæ–·
        print("=" * 70)
        print("è¨ºæ–·")
        print("=" * 70)

        if coverage >= 95:
            print("âœ… è¦†è“‹ç‡å„ªç§€")
        elif coverage >= 80:
            print("âš ï¸  è¦†è“‹ç‡å¯æ¥å—ä½†æœ‰æ”¹é€²ç©ºé–“")
            print()
            print("å¯èƒ½åŸå› :")
            if agg_time_buckets < 12:
                print(f"  - Transform æ•¸æ“šä¸å®Œæ•´ (åªæœ‰ {agg_time_buckets}/12 å€‹æ™‚é–“æ¡¶)")
            if avg_ips_per_bucket < 100:
                print(f"  - æ¯å€‹æ™‚é–“æ¡¶å¹³å‡ IP æ•¸éå°‘ ({avg_ips_per_bucket:.0f})")
                print("    å¯èƒ½æ˜¯ Transform group_by terms çš„é»˜èª size é™åˆ¶")
        else:
            print("ğŸ”´ è¦†è“‹ç‡åš´é‡ä¸è¶³")
            print()
            print("ä¸»è¦å•é¡Œ:")

            if agg_time_buckets < 12:
                print(f"  1. æ™‚é–“æ¡¶ä¸å®Œæ•´: {agg_time_buckets}/12")
                print("     â†’ Transform å¯èƒ½å°šæœªè™•ç†æ‰€æœ‰æ•¸æ“š")
                print("     â†’ è§£æ±ºæ–¹æ¡ˆ: ç­‰å¾… Transform å®ŒæˆåŒæ­¥")

            if coverage < 50 and agg_time_buckets >= 10:
                print(f"  2. è¦†è“‹ç‡éä½ä½†æ™‚é–“æ¡¶å®Œæ•´")
                print("     â†’ Transform group_by çš„ terms aggregation é»˜èªåªè¿”å› Top 10")
                print("     â†’ å•é¡Œ: ES 7.17 Transform ä¸æ”¯æ´åœ¨ group_by ä¸­è¨­ç½® size")
                print("     â†’ é€™æ˜¯ ES Transform çš„å·²çŸ¥é™åˆ¶")
                print()
                print("     å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
                print("     a) ä½¿ç”¨ Python è…³æœ¬ç›´æ¥èšåˆ (æœ€éˆæ´»)")
                print("     b) ä½¿ç”¨ Logstashèšåˆ (å¯æ§åˆ¶ terms size)")
                print("     c) å‡ç´šåˆ° ES 8.x (æ”¯æ´ group_by terms size)")

    return {
        'raw_ips': raw_unique_ips,
        'agg_ips': agg_unique_ips,
        'coverage': coverage if raw_unique_ips > 0 else 0,
        'time_buckets': agg_time_buckets
    }


def check_terms_limit():
    """æª¢æŸ¥ terms aggregation æ˜¯å¦å—åˆ° size é™åˆ¶"""

    print("\n" + "=" * 70)
    print("æª¢æŸ¥ Terms Aggregation é™åˆ¶")
    print("=" * 70)

    # é¸æ“‡æœ€è¿‘çš„ä¸€å€‹å®Œæ•´æ™‚é–“æ¡¶
    query = {
        "size": 0,
        "query": {
            "range": {
                "time_bucket": {
                    "gte": "now-30m"
                }
            }
        },
        "aggs": {
            "by_time_bucket": {
                "terms": {
                    "field": "time_bucket",
                    "size": 1,
                    "order": {"_key": "desc"}
                },
                "aggs": {
                    "ip_count": {
                        "cardinality": {
                            "field": "src_ip",
                            "precision_threshold": 5000
                        }
                    },
                    "doc_count_stat": {
                        "value_count": {"field": "src_ip"}
                    }
                }
            }
        }
    }

    resp = requests.post(
        f"{ES_HOST}/netflow_stats_5m/_search",
        json=query,
        headers={'Content-Type': 'application/json'}
    )

    data = resp.json()

    if 'aggregations' in data and data['aggregations']['by_time_bucket']['buckets']:
        bucket = data['aggregations']['by_time_bucket']['buckets'][0]
        time_bucket = bucket['key_as_string']
        ip_count = int(bucket['ip_count']['value'])
        doc_count = int(bucket['doc_count'])

        print(f"æœ€è¿‘æ™‚é–“æ¡¶: {time_bucket}")
        print(f"  è©²æ¡¶ä¸­çš„å”¯ä¸€ IP æ•¸: {ip_count:,}")
        print(f"  è©²æ¡¶ä¸­çš„æ–‡æª”æ•¸: {doc_count:,}")
        print()

        # ç¾åœ¨æŸ¥è©¢åŸå§‹æ•¸æ“šä¸­é€™å€‹æ™‚é–“æ¡¶æ‡‰è©²æœ‰å¤šå°‘ IP
        bucket_time = datetime.fromisoformat(time_bucket.replace('Z', '+00:00'))
        bucket_start_ms = int(bucket_time.timestamp() * 1000)
        bucket_end_ms = bucket_start_ms + (5 * 60 * 1000)

        raw_query = {
            "size": 0,
            "query": {
                "range": {
                    "FLOW_START_MILLISECONDS": {
                        "gte": bucket_start_ms,
                        "lt": bucket_end_ms
                    }
                }
            },
            "aggs": {
                "unique_ips": {
                    "cardinality": {
                        "field": "IPV4_SRC_ADDR",
                        "precision_threshold": 5000
                    }
                }
            }
        }

        resp = requests.post(
            f"{ES_HOST}/radar_flow_collector-*/_search",
            json=raw_query,
            headers={'Content-Type': 'application/json'}
        )

        raw_data = resp.json()
        raw_ip_count = int(raw_data['aggregations']['unique_ips']['value'])

        print(f"åŸå§‹æ•¸æ“šä¸­è©²æ™‚é–“æ¡¶çš„å”¯ä¸€ IP: {raw_ip_count:,}")
        print()

        if raw_ip_count > 0:
            bucket_coverage = (ip_count / raw_ip_count) * 100
            print(f"è©²æ™‚é–“æ¡¶çš„è¦†è“‹ç‡: {bucket_coverage:.2f}%")
            print()

            if bucket_coverage < 50:
                print("ğŸ”´ å•é¡Œç¢ºèª: Transform çš„ group_by terms é™åˆ¶å°è‡´å¤§é‡ IP éºæ¼")
                print(f"   é æœŸ: {raw_ip_count:,} å€‹ IP")
                print(f"   å¯¦éš›: {ip_count:,} å€‹ IP")
                print(f"   éºæ¼: {raw_ip_count - ip_count:,} å€‹ IP")
            elif bucket_coverage < 95:
                print("âš ï¸  æœ‰éƒ¨åˆ† IP æœªè¢«è¨˜éŒ„")
            else:
                print("âœ… è©²æ™‚é–“æ¡¶è¦†è“‹ç‡è‰¯å¥½")


if __name__ == "__main__":
    result = compare_time_aligned()
    check_terms_limit()

    print("\n" + "=" * 70)
    print("çµè«–")
    print("=" * 70)

    if result['coverage'] < 80:
        print("Transform é…ç½®å­˜åœ¨æ ¹æœ¬æ€§å•é¡Œï¼Œç„¡æ³•ç”¨æ–¼å…¨é¢çš„ç•°å¸¸åµæ¸¬")
        print()
        print("å»ºè­°æ›¿ä»£æ–¹æ¡ˆ:")
        print("1. ä½¿ç”¨ Python è…³æœ¬é€²è¡Œèšåˆ (å®Œå…¨å¯æ§)")
        print("2. æ”¹ç”¨ Logstash pipeline é€²è¡Œèšåˆ")
        print("3. ç›´æ¥å°åŸå§‹æ•¸æ“šé€²è¡Œç•°å¸¸åˆ†æ (analyze_from_aggregated.py å·²é©—è­‰å¯è¡Œ)")
    elif result['coverage'] >= 95:
        print("Transform é…ç½®è‰¯å¥½ï¼Œå¯ç”¨æ–¼ç•°å¸¸åµæ¸¬")
    else:
        print("Transform å¯ç”¨ï¼Œä½†å»ºè­°å„ªåŒ–ä»¥æé«˜è¦†è“‹ç‡")

    print("=" * 70)
