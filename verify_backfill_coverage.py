#!/usr/bin/env python3
"""
å›å¡«è³‡æ–™è¦†è“‹ç‡é©—è­‰å·¥å…·

é©—è­‰å›å¡«çš„æ­·å²è³‡æ–™æ˜¯å¦å®Œæ•´ï¼Œæ¯”å°åŸå§‹ç´¢å¼•å’Œèšåˆç´¢å¼•çš„ IP æ•¸é‡
"""

import requests
import json
from datetime import datetime, timedelta

ES_HOST = "http://localhost:9200"

def verify_backfill_coverage(start_date=None, end_date=None):
    """
    é©—è­‰ç‰¹å®šæ™‚é–“ç¯„åœçš„å›å¡«è¦†è“‹ç‡

    Args:
        start_date: é–‹å§‹æ™‚é–“ (ISO format æˆ– None è¡¨ç¤ºè‡ªå‹•è¨ˆç®—)
        end_date: çµæŸæ™‚é–“ (ISO format æˆ– None è¡¨ç¤ºè‡ªå‹•è¨ˆç®—)
    """
    print("=" * 80)
    print("å›å¡«è³‡æ–™è¦†è“‹ç‡é©—è­‰")
    print("=" * 80)

    # å¦‚æœæ²’æŒ‡å®šæ™‚é–“ï¼Œå…ˆæŸ¥è©¢èšåˆç´¢å¼•çš„æ™‚é–“ç¯„åœ
    if not start_date or not end_date:
        print("\nğŸ” æŸ¥è©¢èšåˆç´¢å¼•çš„æ™‚é–“ç¯„åœ...")
        query = {
            "size": 0,
            "aggs": {
                "time_range": {
                    "stats": {"field": "time_bucket"}
                }
            }
        }

        try:
            resp = requests.post(
                f"{ES_HOST}/netflow_stats_5m/_search",
                json=query,
                headers={'Content-Type': 'application/json'}
            )
            resp.raise_for_status()
            data = resp.json()

            stats = data['aggregations']['time_range']
            min_ts = int(stats['min'])
            max_ts = int(stats['max'])

            start_date = datetime.fromtimestamp(min_ts / 1000).isoformat()
            end_date = datetime.fromtimestamp(max_ts / 1000).isoformat()

            print(f"âœ“ èšåˆç´¢å¼•æ™‚é–“ç¯„åœ:")
            print(f"  é–‹å§‹: {stats['min_as_string']}")
            print(f"  çµæŸ: {stats['max_as_string']}")

        except Exception as e:
            print(f"âŒ æŸ¥è©¢å¤±æ•—: {e}")
            return

    print(f"\nåˆ†ææ™‚é–“ç¯„åœ:")
    print(f"  é–‹å§‹: {start_date}")
    print(f"  çµæŸ: {end_date}")
    print()

    # 1. æŸ¥è©¢åŸå§‹ç´¢å¼•çš„çµ±è¨ˆ
    print("ğŸ” æŸ¥è©¢åŸå§‹ç´¢å¼• (radar_flow_collector-*)...")
    raw_query = {
        "size": 0,
        "query": {
            "range": {
                "FLOW_START_MILLISECONDS": {
                    "gte": start_date,
                    "lte": end_date,
                    "format": "strict_date_optional_time"
                }
            }
        },
        "aggs": {
            "unique_ips": {
                "cardinality": {
                    "field": "IPV4_SRC_ADDR",
                    "precision_threshold": 40000
                }
            },
            "total_docs": {
                "value_count": {
                    "field": "IPV4_SRC_ADDR"
                }
            },
            "time_range": {
                "stats": {
                    "field": "FLOW_START_MILLISECONDS"
                }
            }
        }
    }

    try:
        resp1 = requests.post(
            f"{ES_HOST}/radar_flow_collector-*/_search",
            json=raw_query,
            headers={'Content-Type': 'application/json'},
            timeout=120
        )
        resp1.raise_for_status()
        raw_data = resp1.json()

        raw_unique_ips = raw_data['aggregations']['unique_ips']['value']
        raw_total_docs = raw_data['aggregations']['total_docs']['value']
        raw_time_range = raw_data['aggregations']['time_range']

        print(f"âœ“ åŸå§‹ç´¢å¼•çµ±è¨ˆ:")
        print(f"  å”¯ä¸€ IP æ•¸: {raw_unique_ips:,}")
        print(f"  ç¸½æ–‡æª”æ•¸: {raw_total_docs:,}")
        print(f"  å¯¦éš›æ™‚é–“ç¯„åœ:")
        print(f"    {raw_time_range['min_as_string']} â†’ {raw_time_range['max_as_string']}")
        print()

    except Exception as e:
        print(f"âŒ æŸ¥è©¢åŸå§‹ç´¢å¼•å¤±æ•—: {e}")
        return

    # 2. æŸ¥è©¢èšåˆç´¢å¼•çš„çµ±è¨ˆ
    print("ğŸ” æŸ¥è©¢èšåˆç´¢å¼• (netflow_stats_5m)...")
    agg_query = {
        "size": 0,
        "query": {
            "range": {
                "time_bucket": {
                    "gte": start_date,
                    "lte": end_date,
                    "format": "strict_date_optional_time"
                }
            }
        },
        "aggs": {
            "unique_ips": {
                "cardinality": {
                    "field": "src_ip",
                    "precision_threshold": 40000
                }
            },
            "total_docs": {
                "value_count": {
                    "field": "src_ip"
                }
            },
            "time_buckets": {
                "cardinality": {
                    "field": "time_bucket",
                    "precision_threshold": 10000
                }
            }
        }
    }

    try:
        resp2 = requests.post(
            f"{ES_HOST}/netflow_stats_5m/_search",
            json=agg_query,
            headers={'Content-Type': 'application/json'},
            timeout=120
        )
        resp2.raise_for_status()
        agg_data = resp2.json()

        agg_unique_ips = agg_data['aggregations']['unique_ips']['value']
        agg_total_docs = agg_data['aggregations']['total_docs']['value']
        agg_time_buckets = agg_data['aggregations']['time_buckets']['value']

        print(f"âœ“ èšåˆç´¢å¼•çµ±è¨ˆ:")
        print(f"  å”¯ä¸€ IP æ•¸: {agg_unique_ips:,}")
        print(f"  ç¸½æ–‡æª”æ•¸: {agg_total_docs:,}")
        print(f"  æ™‚é–“æ¡¶æ•¸: {agg_time_buckets:,} å€‹")
        print(f"  å¹³å‡æ¯æ¡¶ IP æ•¸: {agg_total_docs / agg_time_buckets:.0f}")
        print()

    except Exception as e:
        print(f"âŒ æŸ¥è©¢èšåˆç´¢å¼•å¤±æ•—: {e}")
        return

    # 3. è¨ˆç®—è¦†è“‹ç‡
    print("=" * 80)
    print("è¦†è“‹ç‡åˆ†æ")
    print("=" * 80)

    if raw_unique_ips > 0:
        coverage_rate = (agg_unique_ips / raw_unique_ips) * 100

        print(f"åŸå§‹æ•¸æ“šå”¯ä¸€ IP:   {raw_unique_ips:>10,}")
        print(f"èšåˆæ•¸æ“šå”¯ä¸€ IP:   {agg_unique_ips:>10,}")
        print(f"è¦†è“‹ç‡:            {coverage_rate:>9.2f}%")
        print()

        # æ•¸æ“šå£“ç¸®ç‡
        if raw_total_docs > 0 and agg_total_docs > 0:
            compression_rate = (1 - agg_total_docs / raw_total_docs) * 100
            reduction_ratio = raw_total_docs / agg_total_docs

            print(f"åŸå§‹æ–‡æª”æ•¸:        {raw_total_docs:>10,}")
            print(f"èšåˆæ–‡æª”æ•¸:        {agg_total_docs:>10,}")
            print(f"æ•¸æ“šå£“ç¸®ç‡:        {compression_rate:>9.1f}%")
            print(f"å£“ç¸®æ¯”ä¾‹:          {reduction_ratio:>9.0f}x")
            print()

        # è©•ä¼°çµæœ
        print("=" * 80)
        print("è©•ä¼°çµæœ")
        print("=" * 80)

        if coverage_rate >= 99:
            print("âœ… è¦†è“‹ç‡å„ªç§€ (â‰¥99%)")
            print("   å›å¡«è³‡æ–™å®Œæ•´ï¼Œå¹¾ä¹æ•ç²æ‰€æœ‰ IP")
        elif coverage_rate >= 95:
            print("âœ… è¦†è“‹ç‡è‰¯å¥½ (95-99%)")
            print("   å›å¡«è³‡æ–™å“è³ªå„ªè‰¯ï¼Œç•¥æœ‰éºæ¼å±¬æ­£å¸¸")
        elif coverage_rate >= 90:
            print("âš ï¸  è¦†è“‹ç‡å¯æ¥å— (90-95%)")
            print("   æœ‰å°‘é‡ IP æœªè¢«è¨˜éŒ„")
        else:
            print("ğŸ”´ è¦†è“‹ç‡ä¸è¶³ (<90%)")
            print("   å¤§é‡ IP æœªè¢«è¨˜éŒ„ï¼Œå»ºè­°æª¢æŸ¥:")
            print("   - å›å¡«è…³æœ¬çš„ size åƒæ•¸æ˜¯å¦è¶³å¤ ")
            print("   - æ™‚é–“ç¯„åœæ˜¯å¦å°é½Š")
            print("   - ES æ˜¯å¦æœ‰è™•ç†é™åˆ¶")

        # é¡¯ç¤ºéºæ¼æƒ…æ³
        if coverage_rate < 100:
            missing_ips = int(raw_unique_ips - agg_unique_ips)
            missing_rate = ((raw_unique_ips - agg_unique_ips) / raw_unique_ips) * 100

            print(f"\nğŸ“Š éºæ¼çµ±è¨ˆ:")
            print(f"  éºæ¼ IP æ•¸é‡: {missing_ips:,} å€‹")
            print(f"  éºæ¼æ¯”ä¾‹: {missing_rate:.2f}%")

            if missing_ips < 10:
                print(f"  å¯èƒ½åŸå› : cardinality èšåˆçš„è¿‘ä¼¼èª¤å·®ï¼ˆæ­£å¸¸ï¼‰")
            elif missing_ips < 100:
                print(f"  å¯èƒ½åŸå› : æ¥µä½æµé‡ IP æˆ–é‚Šç·£æ™‚é–“é»çš„è³‡æ–™")
            else:
                print(f"  å¯èƒ½åŸå› : å›å¡«è…³æœ¬çš„è™•ç†é™åˆ¶ï¼ˆæª¢æŸ¥ size åƒæ•¸ï¼‰")

        # æ•ˆèƒ½è©•ä¼°
        print(f"\nğŸ“ˆ æ•ˆèƒ½è©•ä¼°:")
        print(f"  æ™‚é–“æ¡¶æ•¸: {agg_time_buckets:,} å€‹ (æ¯5åˆ†é˜ä¸€å€‹)")
        print(f"  è³‡æ–™ç¸®æ¸›: åŸå§‹ {raw_total_docs:,} â†’ èšåˆ {agg_total_docs:,}")
        print(f"  æŸ¥è©¢åŠ é€Ÿ: ç´„ {reduction_ratio:.0f}x å€")

    else:
        print("âŒ åŸå§‹æ•¸æ“šä¸­æ²’æœ‰ IPï¼Œç„¡æ³•è¨ˆç®—è¦†è“‹ç‡")

    print("=" * 80)


def verify_specific_time_bucket(time_bucket):
    """é©—è­‰ç‰¹å®šæ™‚é–“æ¡¶çš„è¦†è“‹ç‡ï¼ˆç²¾ç¢ºé©—è­‰ï¼‰"""
    print("=" * 80)
    print(f"å–®ä¸€æ™‚é–“æ¡¶ç²¾ç¢ºé©—è­‰")
    print("=" * 80)
    print(f"æ™‚é–“æ¡¶: {time_bucket}")
    print()

    # è§£ææ™‚é–“æ¡¶ï¼ˆå‡è¨­æ˜¯ ISO æ ¼å¼ï¼‰
    try:
        bucket_dt = datetime.fromisoformat(time_bucket.replace('Z', '+00:00'))
        start_time = bucket_dt.isoformat()
        end_time = (bucket_dt + timedelta(minutes=5)).isoformat()
    except:
        print("âŒ æ™‚é–“æ ¼å¼éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨ ISO æ ¼å¼ï¼ˆä¾‹å¦‚ï¼š2025-11-11T12:00:00.000Zï¼‰")
        return

    print(f"æŸ¥è©¢ç¯„åœ: {start_time} â†’ {end_time}")
    print()

    # æŸ¥è©¢åŸå§‹ç´¢å¼•
    print("ğŸ” æŸ¥è©¢åŸå§‹ç´¢å¼•...")
    raw_query = {
        "size": 0,
        "query": {
            "range": {
                "FLOW_START_MILLISECONDS": {
                    "gte": start_time,
                    "lt": end_time,
                    "format": "strict_date_optional_time"
                }
            }
        },
        "aggs": {
            "unique_ips": {
                "cardinality": {
                    "field": "IPV4_SRC_ADDR",
                    "precision_threshold": 10000
                }
            },
            "all_ips": {
                "terms": {
                    "field": "IPV4_SRC_ADDR",
                    "size": 10000
                }
            }
        }
    }

    try:
        resp = requests.post(
            f"{ES_HOST}/radar_flow_collector-*/_search",
            json=raw_query,
            headers={'Content-Type': 'application/json'}
        )
        resp.raise_for_status()
        raw_data = resp.json()

        raw_unique_ips = raw_data['aggregations']['unique_ips']['value']
        raw_ip_set = set([b['key'] for b in raw_data['aggregations']['all_ips']['buckets']])

        print(f"âœ“ åŸå§‹ç´¢å¼•å”¯ä¸€ IP: {raw_unique_ips:,}")
        print(f"âœ“ å¯¦éš›å–å¾— IP æ•¸: {len(raw_ip_set):,}")

    except Exception as e:
        print(f"âŒ æŸ¥è©¢å¤±æ•—: {e}")
        return

    # æŸ¥è©¢èšåˆç´¢å¼•
    print("\nğŸ” æŸ¥è©¢èšåˆç´¢å¼•...")
    agg_query = {
        "size": 10000,
        "query": {
            "term": {
                "time_bucket": time_bucket
            }
        },
        "_source": ["src_ip", "time_bucket"]
    }

    try:
        resp = requests.post(
            f"{ES_HOST}/netflow_stats_5m/_search",
            json=agg_query,
            headers={'Content-Type': 'application/json'}
        )
        resp.raise_for_status()
        agg_data = resp.json()

        agg_ip_set = set([hit['_source']['src_ip'] for hit in agg_data['hits']['hits']])

        print(f"âœ“ èšåˆç´¢å¼•å”¯ä¸€ IP: {len(agg_ip_set):,}")

    except Exception as e:
        print(f"âŒ æŸ¥è©¢å¤±æ•—: {e}")
        return

    # æ¯”å°
    print("\n" + "=" * 80)
    print("ç²¾ç¢ºæ¯”å°çµæœ")
    print("=" * 80)

    coverage = len(agg_ip_set) / len(raw_ip_set) * 100 if raw_ip_set else 0

    print(f"åŸå§‹ IP æ•¸: {len(raw_ip_set):,}")
    print(f"èšåˆ IP æ•¸: {len(agg_ip_set):,}")
    print(f"è¦†è“‹ç‡: {coverage:.2f}%")

    # æ‰¾å‡ºéºæ¼å’Œå¤šé¤˜çš„ IP
    missing_ips = raw_ip_set - agg_ip_set
    extra_ips = agg_ip_set - raw_ip_set

    if missing_ips:
        print(f"\nâš ï¸  éºæ¼ IP ({len(missing_ips)} å€‹):")
        for ip in list(missing_ips)[:10]:
            print(f"  - {ip}")
        if len(missing_ips) > 10:
            print(f"  ... é‚„æœ‰ {len(missing_ips) - 10} å€‹")

    if extra_ips:
        print(f"\nâš ï¸  å¤šé¤˜ IP ({len(extra_ips)} å€‹):")
        for ip in list(extra_ips)[:10]:
            print(f"  - {ip}")
        if len(extra_ips) > 10:
            print(f"  ... é‚„æœ‰ {len(extra_ips) - 10} å€‹")

    if not missing_ips and not extra_ips:
        print("\nâœ… å®Œç¾åŒ¹é…ï¼æ‰€æœ‰ IP éƒ½å·²æ­£ç¢ºå›å¡«")

    print("=" * 80)


if __name__ == "__main__":
    import sys

    if '--help' in sys.argv or '-h' in sys.argv:
        print("""
å›å¡«è³‡æ–™è¦†è“‹ç‡é©—è­‰å·¥å…·

ç”¨æ³•:
    # é©—è­‰æ•´å€‹å›å¡«ç¯„åœçš„è¦†è“‹ç‡
    python3 verify_backfill_coverage.py

    # é©—è­‰ç‰¹å®šæ™‚é–“ç¯„åœ
    python3 verify_backfill_coverage.py --start 2025-11-09T00:00:00 --end 2025-11-12T00:00:00

    # ç²¾ç¢ºé©—è­‰å–®ä¸€æ™‚é–“æ¡¶
    python3 verify_backfill_coverage.py --bucket 2025-11-11T12:00:00.000Z

åƒæ•¸èªªæ˜:
    --start DATETIME    é–‹å§‹æ™‚é–“ (ISO format)
    --end DATETIME      çµæŸæ™‚é–“ (ISO format)
    --bucket DATETIME   ç²¾ç¢ºé©—è­‰å–®ä¸€æ™‚é–“æ¡¶
    --help, -h          é¡¯ç¤ºæ­¤èªªæ˜

ç¯„ä¾‹:
    # è‡ªå‹•åµæ¸¬ä¸¦é©—è­‰æ•´å€‹å›å¡«ç¯„åœ
    python3 verify_backfill_coverage.py

    # é©—è­‰ç‰¹å®š3å¤©
    python3 verify_backfill_coverage.py --start 2025-11-09T00:00:00 --end 2025-11-12T00:00:00

    # ç²¾ç¢ºé©—è­‰ç‰¹å®šæ™‚é–“æ¡¶
    python3 verify_backfill_coverage.py --bucket 2025-11-11T12:00:00.000Z
        """)
        sys.exit(0)

    # å–®ä¸€æ™‚é–“æ¡¶é©—è­‰
    if '--bucket' in sys.argv:
        bucket_idx = sys.argv.index('--bucket')
        if bucket_idx + 1 < len(sys.argv):
            time_bucket = sys.argv[bucket_idx + 1]
            verify_specific_time_bucket(time_bucket)
        else:
            print("âŒ è«‹æŒ‡å®šæ™‚é–“æ¡¶")
        sys.exit(0)

    # æ™‚é–“ç¯„åœé©—è­‰
    start_date = None
    end_date = None

    if '--start' in sys.argv:
        start_idx = sys.argv.index('--start')
        if start_idx + 1 < len(sys.argv):
            start_date = sys.argv[start_idx + 1]

    if '--end' in sys.argv:
        end_idx = sys.argv.index('--end')
        if end_idx + 1 < len(sys.argv):
            end_date = sys.argv[end_idx + 1]

    verify_backfill_coverage(start_date, end_date)
