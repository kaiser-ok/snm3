# AI è¼”åŠ©ç¶²è·¯ç•°å¸¸åµæ¸¬ç³»çµ±è¨­è¨ˆï¼ˆå„ªåŒ–ç‰ˆï¼‰

## åŸºæ–¼é©—è­‰çµæœçš„èª¿æ•´

**é—œéµç™¼ç¾ï¼š**
- âœ… Transform èšåˆæ•¸æ“šè¦†è“‹ç‡ï¼š99.57%
- âœ… æ•¸æ“šç¸®æ¸›æ¯”ä¾‹ï¼š100-200x
- âœ… æŸ¥è©¢é€Ÿåº¦æå‡ï¼š100x
- âœ… æ¯5åˆ†é˜ï¼š400-600 å€‹å”¯ä¸€ IP

**å½±éŸ¿ï¼š**
- å¯ç›´æ¥ä½¿ç”¨ `netflow_stats_5m` ä½œç‚º ML è¨“ç·´å’Œæ¨è«–çš„æ•¸æ“šæº
- ç„¡éœ€æ“”å¿ƒæ•¸æ“šéºæ¼å•é¡Œ
- å¯é€²è¡Œå¯¦æ™‚ ML æ¨è«–ï¼ˆå»¶é²ä½ï¼‰

---

## ä¸€ã€å„ªåŒ–çš„æ•¸æ“šæµæ¶æ§‹

### 1.1 æ•¸æ“šå±¤ç´š

```
åŸå§‹ NetFlow æ•¸æ“š (radar_flow_collector-*)
    â†“
ES Transform (æ¯5åˆ†é˜)
    â†“
èšåˆæ•¸æ“š (netflow_stats_5m) â† âœ… 99.57% è¦†è“‹ç‡
    â†“                              âœ… 100x æŸ¥è©¢åŠ é€Ÿ
    â†“                              âœ… å®Œæ•´ç‰¹å¾µå·¥ç¨‹
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ML/AI åˆ†æå±¤                            â”‚
â”‚                                          â”‚
â”‚  Layer 1: Rule-based (å¯¦æ™‚ï¼Œ< 1s)        â”‚
â”‚  Layer 2: ML Classification (å¯¦æ™‚ï¼Œ< 5s) â”‚
â”‚  Layer 3: Time Series (æ‰¹æ¬¡ï¼Œ< 30s)      â”‚
â”‚  Layer 4: LLM Reasoning (æŒ‰éœ€ï¼Œ< 10s)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
ç•°å¸¸å ±å‘Š + å»ºè­°æªæ–½
```

**é—œéµå„ªå‹¢ï¼š**
- æ‰€æœ‰ ML æ¨¡å‹ç›´æ¥è®€å– `netflow_stats_5m`ï¼ˆå·²åŒ…å«å®Œæ•´ç‰¹å¾µï¼‰
- ä¸éœ€è¦å†æ¬¡èšåˆæˆ–ç‰¹å¾µå·¥ç¨‹
- è¨“ç·´å’Œæ¨è«–ä½¿ç”¨ç›¸åŒæ•¸æ“šæºï¼ˆé¿å… train-serve skewï¼‰

---

## äºŒã€åŸºæ–¼èšåˆæ•¸æ“šçš„ç‰¹å¾µå·¥ç¨‹

### 2.1 å·²æœ‰ç‰¹å¾µï¼ˆä¾†è‡ª Transformï¼‰

Transform å·²ç¶“æä¾›çš„ç‰¹å¾µï¼š

```python
# netflow_stats_5m çš„åŸç”Ÿæ¬„ä½
NATIVE_FEATURES = [
    'time_bucket',          # æ™‚é–“æ¡¶ï¼ˆ5åˆ†é˜ï¼‰
    'src_ip',               # ä¾†æº IP
    'flow_count',           # é€£ç·šæ•¸ âœ…
    'total_bytes',          # ç¸½æµé‡ âœ…
    'total_packets',        # ç¸½å°åŒ…æ•¸ âœ…
    'unique_dsts',          # å”¯ä¸€ç›®çš„åœ°æ•¸ âœ…
    'unique_ports',         # å”¯ä¸€ç«¯å£æ•¸ âœ…
    'avg_bytes',            # å¹³å‡æµé‡ âœ…
    'max_bytes'             # æœ€å¤§å–®ä¸€é€£ç·šæµé‡ âœ…
]
```

### 2.2 è¡ç”Ÿç‰¹å¾µï¼ˆå¿«é€Ÿè¨ˆç®—ï¼‰

åœ¨ Python ä¸­å³æ™‚è¨ˆç®—çš„è¡ç”Ÿç‰¹å¾µï¼š

```python
class FeatureEngineer:
    """
    å¾èšåˆæ•¸æ“šæå– ML ç‰¹å¾µ
    """

    def extract_features(self, agg_record):
        """
        å¾å–®ç­† netflow_stats_5m è¨˜éŒ„æå–ç‰¹å¾µ

        å„ªå‹¢ï¼š
        - ä¸éœ€è¦é‡æ–°æŸ¥è©¢åŸå§‹æ•¸æ“š
        - è¨ˆç®—é€Ÿåº¦å¿«ï¼ˆæ¯«ç§’ç´šï¼‰
        - ç‰¹å¾µå®Œæ•´ä¸”ä¸€è‡´
        """
        features = {}

        # 1. åŸç”Ÿç‰¹å¾µï¼ˆç›´æ¥ä½¿ç”¨ï¼‰
        features['flow_count'] = agg_record['flow_count']
        features['total_bytes'] = agg_record['total_bytes']
        features['unique_dsts'] = agg_record['unique_dsts']
        features['unique_ports'] = agg_record['unique_ports']
        features['avg_bytes'] = agg_record['avg_bytes']
        features['max_bytes'] = agg_record['max_bytes']

        # 2. æ¯”ä¾‹ç‰¹å¾µ
        features['dst_diversity'] = (
            agg_record['unique_dsts'] / max(agg_record['flow_count'], 1)
        )
        features['port_diversity'] = (
            agg_record['unique_ports'] / max(agg_record['flow_count'], 1)
        )

        # 3. æµé‡åˆ†å¸ƒç‰¹å¾µ
        features['traffic_concentration'] = (
            agg_record['max_bytes'] / max(agg_record['total_bytes'], 1)
        )
        features['bytes_per_packet'] = (
            agg_record['total_bytes'] / max(agg_record['total_packets'], 1)
        )

        # 4. è¡Œç‚ºæ¨™è¨˜ï¼ˆäºŒå€¼ç‰¹å¾µï¼‰
        features['is_high_connection'] = 1 if agg_record['flow_count'] > 1000 else 0
        features['is_scanning_pattern'] = (
            1 if (agg_record['unique_dsts'] > 30 and
                  agg_record['avg_bytes'] < 10000) else 0
        )
        features['is_small_packet'] = 1 if agg_record['avg_bytes'] < 1000 else 0
        features['is_large_flow'] = 1 if agg_record['max_bytes'] > 100*1024*1024 else 0

        # 5. å°æ•¸è®Šæ›ï¼ˆè™•ç†åæ…‹åˆ†å¸ƒï¼‰
        import numpy as np
        features['log_flow_count'] = np.log1p(agg_record['flow_count'])
        features['log_total_bytes'] = np.log1p(agg_record['total_bytes'])

        return features

    def extract_time_series_features(self, ip, hours=24):
        """
        å¾å¤šå€‹æ™‚é–“æ¡¶æå–æ™‚é–“åºåˆ—ç‰¹å¾µ

        å„ªå‹¢ï¼š
        - æŸ¥è©¢é€Ÿåº¦å¿«ï¼ˆå·²èšåˆï¼‰
        - å¯æª¢æ¸¬è¶¨å‹¢è®ŠåŒ–
        - é©åˆç•°å¸¸åå·®æª¢æ¸¬
        """
        # æŸ¥è©¢è©² IP éå»24å°æ™‚çš„èšåˆæ•¸æ“š
        query = {
            "size": 288,  # 24å°æ™‚ Ã— 12å€‹5åˆ†é˜
            "query": {
                "bool": {
                    "must": [
                        {"term": {"src_ip": ip}},
                        {"range": {"time_bucket": {"gte": f"now-{hours}h"}}}
                    ]
                }
            },
            "sort": [{"time_bucket": "asc"}]
        }

        records = self.es.search(index="netflow_stats_5m", body=query)

        # æå–æ™‚é–“åºåˆ—
        flow_counts = [r['_source']['flow_count'] for r in records['hits']['hits']]
        unique_dsts = [r['_source']['unique_dsts'] for r in records['hits']['hits']]

        # çµ±è¨ˆç‰¹å¾µ
        import numpy as np

        ts_features = {
            # åŸºæœ¬çµ±è¨ˆ
            'mean_flow_count': np.mean(flow_counts),
            'std_flow_count': np.std(flow_counts),
            'max_flow_count': np.max(flow_counts),

            # è®Šç•°æ€§
            'cv_flow_count': np.std(flow_counts) / (np.mean(flow_counts) + 1),

            # è¶¨å‹¢
            'flow_count_trend': self._calculate_trend(flow_counts),

            # çªè®Šæª¢æ¸¬
            'recent_spike': 1 if flow_counts[-1] > np.mean(flow_counts) + 2*np.std(flow_counts) else 0,

            # é€±æœŸæ€§ï¼ˆç°¡åŒ–ç‰ˆï¼‰
            'hour_of_day': datetime.fromisoformat(
                records['hits']['hits'][-1]['_source']['time_bucket']
            ).hour
        }

        return ts_features

    def _calculate_trend(self, values):
        """è¨ˆç®—ç°¡å–®ç·šæ€§è¶¨å‹¢"""
        if len(values) < 2:
            return 0

        import numpy as np
        x = np.arange(len(values))
        y = np.array(values)

        # ç°¡å–®ç·šæ€§å›æ­¸
        slope = np.corrcoef(x, y)[0, 1] if len(values) > 2 else 0
        return slope
```

### 2.3 æœ€çµ‚ç‰¹å¾µé›†

```python
# ML æ¨¡å‹ä½¿ç”¨çš„ç‰¹å¾µï¼ˆå…± 20+ å€‹ï¼‰
FEATURE_SET = {
    # åŸç”Ÿç‰¹å¾µ (8å€‹)
    'basic': [
        'flow_count', 'total_bytes', 'total_packets',
        'unique_dsts', 'unique_ports', 'avg_bytes', 'max_bytes'
    ],

    # æ¯”ä¾‹ç‰¹å¾µ (4å€‹)
    'ratios': [
        'dst_diversity', 'port_diversity',
        'traffic_concentration', 'bytes_per_packet'
    ],

    # è¡Œç‚ºæ¨™è¨˜ (4å€‹)
    'binary': [
        'is_high_connection', 'is_scanning_pattern',
        'is_small_packet', 'is_large_flow'
    ],

    # å°æ•¸ç‰¹å¾µ (2å€‹)
    'log_transformed': [
        'log_flow_count', 'log_total_bytes'
    ],

    # æ™‚é–“åºåˆ—ç‰¹å¾µ (7å€‹) - å¯é¸
    'time_series': [
        'mean_flow_count', 'std_flow_count', 'cv_flow_count',
        'flow_count_trend', 'recent_spike', 'hour_of_day'
    ]
}

# ç¸½è¨ˆï¼š17å€‹åŸºç¤ç‰¹å¾µ + 7å€‹æ™‚åºç‰¹å¾µ = 24å€‹ç‰¹å¾µ
```

---

## ä¸‰ã€å„ªåŒ–çš„ ML æ¨¡å‹é¸æ“‡

### 3.1 Isolation Forestï¼ˆç„¡ç›£ç£ç•°å¸¸æª¢æ¸¬ï¼‰

**é©ç”¨å ´æ™¯ï¼š**
- ä¸éœ€è¦æ¨™è¨˜æ•¸æ“š
- åˆæœŸå¿«é€Ÿéƒ¨ç½²
- æª¢æ¸¬æœªçŸ¥ç•°å¸¸æ¨¡å¼

**å„ªåŒ–å»ºè­°ï¼š**

```python
from sklearn.ensemble import IsolationForest
import numpy as np

class OptimizedIsolationForest:
    """
    åŸºæ–¼èšåˆæ•¸æ“šå„ªåŒ–çš„ Isolation Forest
    """

    def __init__(self):
        self.model = IsolationForest(
            contamination=0.05,      # é æœŸ5%ç•°å¸¸ç‡ï¼ˆåŸºæ–¼å¯¦æ¸¬æ•¸æ“šèª¿æ•´ï¼‰
            n_estimators=150,        # å¢åŠ æ¨¹çš„æ•¸é‡ä»¥æé«˜ç©©å®šæ€§
            max_samples=512,         # æ¯æ£µæ¨¹æ¡æ¨£512å€‹æ¨£æœ¬
            max_features=0.8,        # ä½¿ç”¨80%çš„ç‰¹å¾µ
            random_state=42,
            n_jobs=-1                # ä½¿ç”¨æ‰€æœ‰ CPU æ ¸å¿ƒ
        )
        self.feature_engineer = FeatureEngineer()
        self.scaler = StandardScaler()

    def train_on_aggregated_data(self, days=7):
        """
        ä½¿ç”¨éå» N å¤©çš„èšåˆæ•¸æ“šè¨“ç·´

        å„ªå‹¢ï¼š
        - æ•¸æ“šé‡é©ä¸­ï¼ˆ7å¤© Ã— 288å€‹æ™‚é–“æ¡¶ Ã— 500IP â‰ˆ 100è¬ç­†ï¼‰
        - è¨“ç·´é€Ÿåº¦å¿«ï¼ˆå¹¾åˆ†é˜å…§å®Œæˆï¼‰
        - åŒ…å«æ­£å¸¸æµé‡çš„å…¨éƒ¨æ¨¡å¼
        """
        print(f"ğŸ“š æ”¶é›†éå» {days} å¤©çš„èšåˆæ•¸æ“š...")

        # æŸ¥è©¢èšåˆæ•¸æ“š
        query = {
            "size": 10000,
            "query": {
                "range": {
                    "time_bucket": {
                        "gte": f"now-{days}d"
                    }
                }
            }
        }

        records = self.es.search(
            index="netflow_stats_5m",
            body=query,
            scroll='5m'
        )

        # æå–ç‰¹å¾µ
        X = []
        for record in self._scroll_all(records):
            features = self.feature_engineer.extract_features(record['_source'])
            X.append(list(features.values()))

        X = np.array(X)

        # æ¨™æº–åŒ–
        X_scaled = self.scaler.fit_transform(X)

        print(f"ğŸ‹ï¸  è¨“ç·´ Isolation Forest ({len(X):,} æ¨£æœ¬)...")
        self.model.fit(X_scaled)

        print("âœ… è¨“ç·´å®Œæˆ")
        self._save_model()

    def predict_realtime(self, recent_minutes=10):
        """
        å°æœ€è¿‘çš„æ•¸æ“šé€²è¡Œå¯¦æ™‚ç•°å¸¸æª¢æ¸¬

        å„ªå‹¢ï¼š
        - å»¶é²ä½ï¼ˆ< 5ç§’ï¼‰
        - åªéœ€æƒææœ€è¿‘å¹¾å€‹æ™‚é–“æ¡¶
        - å¯æ¯5åˆ†é˜åŸ·è¡Œä¸€æ¬¡
        """
        query = {
            "size": 1000,
            "query": {
                "range": {
                    "time_bucket": {
                        "gte": f"now-{recent_minutes}m"
                    }
                }
            }
        }

        records = self.es.search(index="netflow_stats_5m", body=query)

        results = []
        for record in records['hits']['hits']:
            src = record['_source']
            features = self.feature_engineer.extract_features(src)

            X = np.array([list(features.values())])
            X_scaled = self.scaler.transform(X)

            prediction = self.model.predict(X_scaled)[0]
            score = self.model.score_samples(X_scaled)[0]

            if prediction == -1:  # ç•°å¸¸
                results.append({
                    'src_ip': src['src_ip'],
                    'time_bucket': src['time_bucket'],
                    'anomaly_score': -score,  # è½‰ç‚ºæ­£å€¼ï¼Œè¶Šå¤§è¶Šç•°å¸¸
                    'features': features,
                    'flow_count': src['flow_count'],
                    'unique_dsts': src['unique_dsts']
                })

        # æŒ‰ç•°å¸¸åˆ†æ•¸æ’åº
        results.sort(key=lambda x: x['anomaly_score'], reverse=True)

        return results
```

**è¨“ç·´å»ºè­°ï¼š**
```python
# åˆå§‹è¨“ç·´ï¼šä½¿ç”¨éå»7å¤©çš„æ­£å¸¸æµé‡
detector = OptimizedIsolationForest()
detector.train_on_aggregated_data(days=7)

# å®šæœŸé‡è¨“ç·´ï¼šæ¯é€±ä¸€æ¬¡
# ä½¿ç”¨æœ€æ–°7å¤©æ•¸æ“šï¼Œæ’é™¤å·²çŸ¥ç•°å¸¸
```

### 3.2 Random Forest Classifierï¼ˆç›£ç£å¼è¡Œç‚ºåˆ†é¡ï¼‰

**é©ç”¨å ´æ™¯ï¼š**
- å·²æœ‰éƒ¨åˆ†æ¨™è¨˜æ•¸æ“š
- éœ€è¦å¯è§£é‡‹çš„åˆ†é¡çµæœ
- å¤šé¡åˆ¥ç•°å¸¸æª¢æ¸¬

**å„ªåŒ–å»ºè­°ï¼š**

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

class BehaviorClassifier:
    """
    æµé‡è¡Œç‚ºåˆ†é¡å™¨ï¼ˆåŸºæ–¼èšåˆæ•¸æ“šå„ªåŒ–ï¼‰
    """

    BEHAVIOR_LABELS = {
        0: 'normal',
        1: 'port_scanning',       # ç«¯å£æƒæ
        2: 'network_scanning',    # ç¶²è·¯æƒæ
        3: 'dns_abuse',           # DNS æ¿«ç”¨
        4: 'data_exfiltration',   # æ•¸æ“šå¤–æ´©
        5: 'high_traffic',        # é«˜æµé‡ï¼ˆå¯èƒ½æ­£å¸¸ï¼‰
    }

    def __init__(self):
        self.model = RandomForestClassifier(
            n_estimators=200,
            max_depth=15,
            min_samples_split=10,
            min_samples_leaf=5,
            class_weight='balanced',  # è™•ç†é¡åˆ¥ä¸å¹³è¡¡
            random_state=42,
            n_jobs=-1
        )
        self.feature_engineer = FeatureEngineer()

    def create_training_data_from_rules(self):
        """
        ä½¿ç”¨è¦å‰‡å¼•æ“è‡ªå‹•ç”Ÿæˆè¨“ç·´æ•¸æ“š

        å„ªå‹¢ï¼š
        - ä¸éœ€è¦äººå·¥æ¨™è¨˜
        - å¯å¿«é€Ÿç”Ÿæˆå¤§é‡æ¨£æœ¬
        - åŸºæ–¼å·²çŸ¥ç•°å¸¸æ¨¡å¼
        """
        print("ğŸ“š å¾èšåˆæ•¸æ“šç”Ÿæˆè¨“ç·´é›†...")

        # æŸ¥è©¢éå»30å¤©çš„æ•¸æ“š
        query = {
            "size": 10000,
            "query": {
                "range": {"time_bucket": {"gte": "now-30d"}}
            }
        }

        records = self.es.search(index="netflow_stats_5m", body=query, scroll='5m')

        X = []
        y = []

        for record in self._scroll_all(records):
            src = record['_source']
            features = self.feature_engineer.extract_features(src)

            # ä½¿ç”¨è¦å‰‡å¼•æ“è‡ªå‹•æ¨™è¨˜
            label = self._auto_label(src)

            if label is not None:  # åªä¿ç•™é«˜ç½®ä¿¡åº¦æ¨™è¨˜
                X.append(list(features.values()))
                y.append(label)

        print(f"âœ… ç”Ÿæˆ {len(X):,} å€‹è¨“ç·´æ¨£æœ¬")
        print(f"   é¡åˆ¥åˆ†å¸ƒ: {np.bincount(y)}")

        return np.array(X), np.array(y)

    def _auto_label(self, record):
        """
        è‡ªå‹•æ¨™è¨˜è¦å‰‡ï¼ˆåŸºæ–¼å¯¦æ¸¬æ•¸æ“šå„ªåŒ–ï¼‰
        """
        # æ­£å¸¸æµé‡ï¼ˆä¿å®ˆæ¨™è¨˜ï¼‰
        if (record['flow_count'] < 100 and
            record['unique_dsts'] < 10):
            return 0  # normal

        # ç«¯å£æƒæ
        if (record['unique_ports'] > 50 and
            record['avg_bytes'] < 5000 and
            record['flow_count'] > 100):
            return 1  # port_scanning

        # ç¶²è·¯æƒæ
        if (record['unique_dsts'] > 100 and
            record['avg_bytes'] < 10000 and
            record['flow_count'] > 500):
            return 2  # network_scanning

        # DNS æ¿«ç”¨
        # æ³¨æ„ï¼šéœ€è¦åœ¨ Transform ä¸­æ·»åŠ  dst_port çµ±è¨ˆ
        # æš«æ™‚ä½¿ç”¨é«˜é€£ç·šæ•¸ + å°å°åŒ…ä½œç‚ºè¿‘ä¼¼
        if (record['flow_count'] > 10000 and
            record['avg_bytes'] < 500 and
            record['unique_dsts'] < 10):
            return 3  # dns_abuse

        # æ•¸æ“šå¤–æ´©
        if (record['total_bytes'] > 1024*1024*1024 and  # > 1GB
            record['unique_dsts'] < 5 and
            record['avg_bytes'] > 1024*1024):  # > 1MB å¹³å‡
            return 4  # data_exfiltration

        # é«˜æµé‡ï¼ˆä½†å¯èƒ½æ­£å¸¸ï¼‰
        if record['flow_count'] > 5000:
            return 5  # high_traffic

        # ä¸ç¢ºå®šçš„æ¨¡å¼ä¸æ¨™è¨˜
        return None

    def train(self):
        """è¨“ç·´æ¨¡å‹"""
        X, y = self.create_training_data_from_rules()

        # äº¤å‰é©—è­‰
        scores = cross_val_score(self.model, X, y, cv=5)
        print(f"ğŸ“Š äº¤å‰é©—è­‰æº–ç¢ºç‡: {scores.mean():.3f} (+/- {scores.std():.3f})")

        # è¨“ç·´
        self.model.fit(X, y)

        # ç‰¹å¾µé‡è¦æ€§
        self._print_feature_importance()

    def _print_feature_importance(self):
        """è¼¸å‡ºç‰¹å¾µé‡è¦æ€§"""
        importances = self.model.feature_importances_
        feature_names = list(FEATURE_SET['basic']) + list(FEATURE_SET['ratios'])

        print("\nğŸ“ˆ ç‰¹å¾µé‡è¦æ€§ Top 5:")
        indices = np.argsort(importances)[-5:][::-1]
        for i in indices:
            print(f"   {feature_names[i]}: {importances[i]:.3f}")
```

### 3.3 æ™‚é–“åºåˆ—ç•°å¸¸æª¢æ¸¬ï¼ˆLSTM / Prophetï¼‰

**é©ç”¨å ´æ™¯ï¼š**
- æª¢æ¸¬æµé‡è¶¨å‹¢ç•°å¸¸
- è­˜åˆ¥çªç™¼æµé‡
- é æ¸¬æœªä¾†ç•°å¸¸

**ç°¡åŒ–æ–¹æ¡ˆï¼ˆåŸºæ–¼çµ±è¨ˆï¼‰ï¼š**

```python
class TimeSeriesAnomalyDetector:
    """
    åŸºæ–¼çµ±è¨ˆçš„æ™‚é–“åºåˆ—ç•°å¸¸æª¢æ¸¬
    ï¼ˆæ¯” LSTM ç°¡å–®ï¼Œä½†å°èšåˆæ•¸æ“šå¾ˆæœ‰æ•ˆï¼‰
    """

    def detect_spike(self, ip, window_hours=24):
        """
        æª¢æ¸¬æµé‡çªå¢

        å„ªå‹¢ï¼š
        - ç›´æ¥ä½¿ç”¨èšåˆæ•¸æ“š
        - è¨ˆç®—é€Ÿåº¦å¿«
        - ç„¡éœ€è¤‡é›œæ¨¡å‹
        """
        # æŸ¥è©¢éå»24å°æ™‚çš„æ™‚é–“åºåˆ—
        query = {
            "size": 288,  # 24h Ã— 12
            "query": {
                "bool": {
                    "must": [
                        {"term": {"src_ip": ip}},
                        {"range": {"time_bucket": {"gte": f"now-{window_hours}h"}}}
                    ]
                }
            },
            "sort": [{"time_bucket": "asc"}]
        }

        records = self.es.search(index="netflow_stats_5m", body=query)

        # æå–æµé‡æ™‚é–“åºåˆ—
        flow_counts = [
            r['_source']['flow_count']
            for r in records['hits']['hits']
        ]

        if len(flow_counts) < 12:  # è‡³å°‘1å°æ™‚æ•¸æ“š
            return None

        # çµ±è¨ˆæ–¹æ³•æª¢æ¸¬ç•°å¸¸
        import numpy as np

        # ä½¿ç”¨ç§»å‹•å¹³å‡å’Œæ¨™æº–å·®
        recent = flow_counts[-1]
        baseline = np.mean(flow_counts[:-1])
        std = np.std(flow_counts[:-1])

        # Z-score
        z_score = (recent - baseline) / (std + 1)

        # åˆ¤æ–·ç•°å¸¸
        is_spike = z_score > 3  # 3å€‹æ¨™æº–å·®

        return {
            'is_anomaly': is_spike,
            'z_score': z_score,
            'current': recent,
            'baseline': baseline,
            'deviation_pct': ((recent - baseline) / baseline * 100) if baseline > 0 else 0
        }
```

---

## å››ã€å¯¦æ™‚æ¨è«–æµç¨‹ï¼ˆå„ªåŒ–ç‰ˆï¼‰

```python
class RealtimeAnomalyEngine:
    """
    å¯¦æ™‚ç•°å¸¸æª¢æ¸¬å¼•æ“ï¼ˆåŸºæ–¼èšåˆæ•¸æ“šï¼‰
    """

    def __init__(self):
        self.isolation_forest = OptimizedIsolationForest()
        self.behavior_classifier = BehaviorClassifier()
        self.ts_detector = TimeSeriesAnomalyDetector()

        # åŠ è¼‰é è¨“ç·´æ¨¡å‹
        self.isolation_forest.load_model()
        self.behavior_classifier.load_model()

    def analyze_recent(self, minutes=5):
        """
        åˆ†ææœ€è¿‘ N åˆ†é˜çš„æ•¸æ“š

        åŸ·è¡Œæµç¨‹ï¼š
        1. å¾ netflow_stats_5m è®€å–æœ€æ–°æ•¸æ“šï¼ˆ< 0.5sï¼‰
        2. Isolation Forest ç•°å¸¸æª¢æ¸¬ï¼ˆ< 1sï¼‰
        3. Behavior Classificationï¼ˆ< 2sï¼‰
        4. æ™‚é–“åºåˆ—åˆ†æï¼ˆå¯é¸ï¼Œ< 2sï¼‰

        ç¸½å»¶é²ï¼š< 5sï¼ˆå¯æ¥å—çš„å¯¦æ™‚æ€§ï¼‰
        """
        import time
        start_time = time.time()

        print(f"\n{'='*60}")
        print(f"å¯¦æ™‚ç•°å¸¸åˆ†æ - éå» {minutes} åˆ†é˜")
        print(f"{'='*60}\n")

        # Step 1: Isolation Forestï¼ˆå¿«é€Ÿç¯©é¸ï¼‰
        print("ğŸ¤– Step 1: ç„¡ç›£ç£ç•°å¸¸æª¢æ¸¬...")
        anomalies = self.isolation_forest.predict_realtime(minutes)
        print(f"   ç™¼ç¾ {len(anomalies)} å€‹æ½›åœ¨ç•°å¸¸")

        # Step 2: Behavior Classificationï¼ˆç²¾ç¢ºåˆ†é¡ï¼‰
        print("\nğŸ¯ Step 2: è¡Œç‚ºåˆ†é¡...")
        classified = []
        for anomaly in anomalies:
            behavior = self.behavior_classifier.predict_single(anomaly)

            classified.append({
                **anomaly,
                'behavior': behavior['label'],
                'confidence': behavior['confidence']
            })

        # Step 3: æ™‚é–“åºåˆ—åˆ†æï¼ˆæª¢æ¸¬çªè®Šï¼‰
        print("\nğŸ“ˆ Step 3: æ™‚é–“åºåˆ—ç•°å¸¸æª¢æ¸¬...")
        for item in classified:
            ts_result = self.ts_detector.detect_spike(item['src_ip'])
            if ts_result:
                item['is_spike'] = ts_result['is_anomaly']
                item['z_score'] = ts_result['z_score']

        # æ’åºï¼šå„ªå…ˆé¡¯ç¤ºé«˜ç½®ä¿¡åº¦ç•°å¸¸
        classified.sort(
            key=lambda x: (x['confidence'], x['anomaly_score']),
            reverse=True
        )

        elapsed = time.time() - start_time
        print(f"\nâœ… åˆ†æå®Œæˆ (è€—æ™‚: {elapsed:.2f}s)")

        return classified

    def continuous_monitoring(self, interval_minutes=5):
        """
        æŒçºŒç›£æ§æ¨¡å¼

        æ¯ N åˆ†é˜åŸ·è¡Œä¸€æ¬¡å¯¦æ™‚åˆ†æ
        """
        import schedule
        import time

        def job():
            results = self.analyze_recent(minutes=interval_minutes)

            # è¼¸å‡ºé«˜é¢¨éšªç•°å¸¸
            high_risk = [
                r for r in results
                if r['behavior'] in ['port_scanning', 'network_scanning', 'dns_abuse']
                and r['confidence'] > 0.7
            ]

            if high_risk:
                print(f"\nâš ï¸  ç™¼ç¾ {len(high_risk)} å€‹é«˜é¢¨éšªç•°å¸¸:\n")
                for i, anomaly in enumerate(high_risk, 1):
                    print(f"{i}. {anomaly['src_ip']:15} | "
                          f"{anomaly['behavior']:20} | "
                          f"ç½®ä¿¡åº¦: {anomaly['confidence']:.2f} | "
                          f"é€£ç·šæ•¸: {anomaly['flow_count']:,}")
            else:
                print("\nâœ… æœªç™¼ç¾é«˜é¢¨éšªç•°å¸¸")

        # ç«‹å³åŸ·è¡Œä¸€æ¬¡
        job()

        # å®šæœŸåŸ·è¡Œ
        schedule.every(interval_minutes).minutes.do(job)

        print(f"\nğŸ”„ æŒçºŒç›£æ§æ¨¡å¼å•Ÿå‹• (æ¯ {interval_minutes} åˆ†é˜åˆ†æä¸€æ¬¡)")
        print("   æŒ‰ Ctrl+C åœæ­¢\n")

        while True:
            schedule.run_pending()
            time.sleep(10)
```

---

## äº”ã€è¨“ç·´æ•¸æ“šç­–ç•¥ï¼ˆåŸºæ–¼èšåˆæ•¸æ“šï¼‰

### 5.1 è‡ªå‹•æ¨™è¨˜ç­–ç•¥

```python
class AutoLabelingEngine:
    """
    å¾èšåˆæ•¸æ“šè‡ªå‹•ç”Ÿæˆè¨“ç·´é›†
    """

    def generate_labeled_dataset(self, days=30):
        """
        ç”Ÿæˆéå» N å¤©çš„æ¨™è¨˜æ•¸æ“šé›†

        å„ªå‹¢ï¼š
        - æ•¸æ“šé‡é©ä¸­ï¼ˆ30å¤© Ã— 288 Ã— 500IP â‰ˆ 430è¬ç­†ï¼‰
        - æ¨™è¨˜é€Ÿåº¦å¿«ï¼ˆåŸºæ–¼è¦å‰‡ï¼‰
        - å¯æŒçºŒæ›´æ–°
        """
        labeled_data = {
            'normal': [],
            'port_scanning': [],
            'network_scanning': [],
            'dns_abuse': [],
            'data_exfiltration': [],
            'high_traffic': []
        }

        # æŸ¥è©¢èšåˆæ•¸æ“š
        query = {
            "size": 10000,
            "query": {
                "range": {"time_bucket": {"gte": f"now-{days}d"}}
            }
        }

        records = self.es.search(
            index="netflow_stats_5m",
            body=query,
            scroll='10m'
        )

        for record in self._scroll_all(records):
            src = record['_source']

            # æå–ç‰¹å¾µ
            features = self.feature_engineer.extract_features(src)

            # è‡ªå‹•æ¨™è¨˜
            label, confidence = self._auto_label_with_confidence(src)

            # åªä¿ç•™é«˜ç½®ä¿¡åº¦æ¨£æœ¬
            if confidence > 0.8:
                labeled_data[label].append({
                    'features': features,
                    'label': label,
                    'confidence': confidence,
                    'src_ip': src['src_ip'],
                    'time_bucket': src['time_bucket']
                })

        # å¹³è¡¡æ•¸æ“šé›†
        balanced = self._balance_dataset(labeled_data)

        return balanced

    def _auto_label_with_confidence(self, record):
        """
        è‡ªå‹•æ¨™è¨˜ä¸¦è¿”å›ç½®ä¿¡åº¦
        """
        # ç«¯å£æƒæï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰
        if (record['unique_ports'] > 100 and
            record['avg_bytes'] < 5000 and
            record['flow_count'] > 1000):
            return 'port_scanning', 0.95

        # ç¶²è·¯æƒæï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰
        if (record['unique_dsts'] > 200 and
            record['avg_bytes'] < 10000 and
            record['flow_count'] > 2000):
            return 'network_scanning', 0.90

        # DNS æ¿«ç”¨ï¼ˆä¸­ç­‰ç½®ä¿¡åº¦ï¼‰
        if (record['flow_count'] > 20000 and
            record['avg_bytes'] < 500):
            return 'dns_abuse', 0.75

        # æ­£å¸¸æµé‡ï¼ˆä¿å®ˆæ¨™è¨˜ï¼‰
        if (record['flow_count'] < 100 and
            record['unique_dsts'] < 20 and
            record['avg_bytes'] > 1000):
            return 'normal', 0.85

        # å…¶ä»–
        return 'normal', 0.5  # ä½ç½®ä¿¡åº¦

    def _balance_dataset(self, labeled_data):
        """
        å¹³è¡¡é¡åˆ¥æ•¸é‡ï¼ˆè™•ç†é¡åˆ¥ä¸å¹³è¡¡ï¼‰
        """
        # æ‰¾åˆ°æœ€å°é¡åˆ¥æ•¸é‡
        min_count = min(len(samples) for samples in labeled_data.values())

        # æ¯å€‹é¡åˆ¥éš¨æ©Ÿæ¡æ¨£ç›¸åŒæ•¸é‡
        balanced = {}
        for label, samples in labeled_data.items():
            if len(samples) > min_count:
                balanced[label] = np.random.choice(
                    samples, size=min_count, replace=False
                ).tolist()
            else:
                balanced[label] = samples

        return balanced
```

### 5.2 å¢é‡å­¸ç¿’

```python
class IncrementalLearner:
    """
    æŒçºŒå­¸ç¿’ç®¡ç†å™¨
    """

    def should_retrain(self):
        """
        åˆ¤æ–·æ˜¯å¦éœ€è¦é‡æ–°è¨“ç·´
        """
        # 1. æª¢æŸ¥è·é›¢ä¸Šæ¬¡è¨“ç·´çš„æ™‚é–“
        last_train_time = self.get_last_train_time()
        days_since_train = (datetime.now() - last_train_time).days

        # 2. æª¢æŸ¥æ–°æ•¸æ“šé‡
        new_data_count = self.count_new_labeled_data()

        # 3. æª¢æŸ¥æ¨¡å‹æ€§èƒ½
        recent_accuracy = self.evaluate_recent_performance()

        # é‡è¨“ç·´æ¢ä»¶
        return (
            days_since_train >= 7 or          # æ¯é€±é‡è¨“ç·´
            new_data_count >= 10000 or        # æœ‰è¶³å¤ æ–°æ•¸æ“š
            recent_accuracy < 0.85            # æ€§èƒ½ä¸‹é™
        )

    def incremental_update(self):
        """
        å¢é‡æ›´æ–°æ¨¡å‹
        """
        print("ğŸ”„ é–‹å§‹å¢é‡æ›´æ–°...")

        # 1. æ”¶é›†æ–°æ•¸æ“š
        new_data = self.collect_recent_data(days=7)

        # 2. è‡ªå‹•æ¨™è¨˜
        labeled = self.auto_label(new_data)

        # 3. èˆ‡èˆŠæ•¸æ“šåˆä½µ
        full_dataset = self.merge_with_historical(labeled)

        # 4. é‡æ–°è¨“ç·´
        self.retrain_models(full_dataset)

        # 5. é©—è­‰æ€§èƒ½
        self.validate_and_save()

        print("âœ… å¢é‡æ›´æ–°å®Œæˆ")
```

---

## å…­ã€æˆæœ¬èˆ‡æ€§èƒ½å„ªåŒ–

### 6.1 LLM ä½¿ç”¨ç­–ç•¥ï¼ˆå„ªåŒ–ç‰ˆï¼‰

```python
class SmartLLMReasoner:
    """
    æ™ºèƒ½ LLM æ¨è«–å™¨ï¼ˆæˆæœ¬å„ªåŒ–ï¼‰
    """

    def __init__(self, api_key, budget_per_day=50):
        self.llm = anthropic.Anthropic(api_key=api_key)
        self.budget = budget_per_day
        self.cost_tracker = DailyCostTracker()

    def analyze_if_worth(self, anomaly):
        """
        åªå°å€¼å¾—åˆ†æçš„æ¡ˆä¾‹ä½¿ç”¨ LLM
        """
        # æ¢ä»¶ 1: é«˜é¢¨éšª
        is_high_risk = anomaly['behavior'] in [
            'port_scanning', 'network_scanning', 'data_exfiltration'
        ]

        # æ¢ä»¶ 2: ML åˆ†é¡ä¸ç¢ºå®š
        is_uncertain = anomaly['confidence'] < 0.75

        # æ¢ä»¶ 3: é¦–æ¬¡ç™¼ç¾
        is_first_time = self.is_first_occurrence(anomaly['src_ip'], anomaly['behavior'])

        # æ¢ä»¶ 4: é ç®—å……è¶³
        has_budget = self.cost_tracker.remaining_budget() > 0

        # åªæœ‰æ»¿è¶³æ‰€æœ‰æ¢ä»¶æ‰ä½¿ç”¨ LLM
        if is_high_risk and (is_uncertain or is_first_time) and has_budget:
            return self._llm_deep_analysis(anomaly)
        else:
            return self._rule_based_analysis(anomaly)

    def batch_analyze(self, anomalies):
        """
        æ‰¹æ¬¡åˆ†æï¼ˆç¯€çœ API èª¿ç”¨ï¼‰
        """
        # å°‡ç›¸ä¼¼ç•°å¸¸åˆ†çµ„
        groups = self._group_similar_anomalies(anomalies)

        results = []
        for group in groups:
            # æ¯çµ„åªåˆ†æä¸€å€‹ä»£è¡¨
            representative = group[0]
            analysis = self._llm_deep_analysis(representative)

            # çµè«–å¥—ç”¨åˆ°æ•´çµ„
            for anomaly in group:
                results.append({
                    **anomaly,
                    'ai_analysis': analysis,
                    'note': f'åŸºæ–¼ç›¸ä¼¼æ¡ˆä¾‹æ¨è«– (çµ„å¤§å°: {len(group)})'
                })

        return results

    def _group_similar_anomalies(self, anomalies):
        """
        å°‡ç›¸ä¼¼ç•°å¸¸åˆ†çµ„
        """
        from sklearn.cluster import DBSCAN
        import numpy as np

        # æå–ç‰¹å¾µå‘é‡
        X = np.array([
            [
                a['flow_count'],
                a['unique_dsts'],
                a['avg_bytes']
            ]
            for a in anomalies
        ])

        # èšé¡
        clustering = DBSCAN(eps=0.3, min_samples=2).fit(X)

        # åˆ†çµ„
        groups = {}
        for i, label in enumerate(clustering.labels_):
            if label not in groups:
                groups[label] = []
            groups[label].append(anomalies[i])

        return list(groups.values())
```

### 6.2 æ€§èƒ½åŸºæº–æ¸¬è©¦

```python
# åŸºæ–¼èšåˆæ•¸æ“šçš„æ€§èƒ½åŸºæº–

PERFORMANCE_BENCHMARKS = {
    'data_loading': {
        'raw_index_query': '15-30s',          # æŸ¥è©¢åŸå§‹æ•¸æ“š
        'aggregated_index_query': '0.1-0.5s', # æŸ¥è©¢èšåˆæ•¸æ“š âœ…
        'speedup': '100x'
    },

    'feature_extraction': {
        'from_raw': '10-20s',                 # å¾åŸå§‹æ•¸æ“šèšåˆ
        'from_aggregated': '0.01-0.05s',      # å¾èšåˆæ•¸æ“šæå– âœ…
        'speedup': '200-400x'
    },

    'ml_inference': {
        'isolation_forest': '< 1s',           # 1000å€‹æ¨£æœ¬
        'random_forest': '< 2s',              # 1000å€‹æ¨£æœ¬
        'time_series_stats': '< 2s',          # å–®å€‹ IP
    },

    'total_realtime_analysis': {
        'target': '< 5s',                      # ç›®æ¨™å»¶é²
        'typical': '3-4s',                     # å…¸å‹æƒ…æ³
        'worst_case': '< 10s'                  # æœ€å£æƒ…æ³
    }
}
```

---

## ä¸ƒã€å®Œæ•´å¯¦ä½œç¯„ä¾‹

### 7.1 å¿«é€Ÿå•Ÿå‹•è…³æœ¬

```python
#!/usr/bin/env python3
# quick_start_ml_detection.py

"""
åŸºæ–¼èšåˆæ•¸æ“šçš„ ML ç•°å¸¸æª¢æ¸¬ - å¿«é€Ÿå•Ÿå‹•

æ­¥é©Ÿï¼š
1. è¨“ç·´ Isolation Forestï¼ˆä½¿ç”¨éå»7å¤©æ•¸æ“šï¼‰
2. è¨“ç·´ Behavior Classifierï¼ˆä½¿ç”¨è‡ªå‹•æ¨™è¨˜æ•¸æ“šï¼‰
3. åŸ·è¡Œå¯¦æ™‚æª¢æ¸¬
"""

from nad.ml.optimized_isolation_forest import OptimizedIsolationForest
from nad.ml.behavior_classifier import BehaviorClassifier
from nad.core.realtime_engine import RealtimeAnomalyEngine

def main():
    print("="*60)
    print("ML ç•°å¸¸æª¢æ¸¬ç³»çµ± - åˆå§‹åŒ–")
    print("="*60)

    # Step 1: è¨“ç·´ Isolation Forest
    print("\nğŸ“š Step 1: è¨“ç·´ç„¡ç›£ç£ç•°å¸¸æª¢æ¸¬æ¨¡å‹...")
    iso_forest = OptimizedIsolationForest()
    iso_forest.train_on_aggregated_data(days=7)

    # Step 2: è¨“ç·´ Behavior Classifier
    print("\nğŸ¯ Step 2: è¨“ç·´è¡Œç‚ºåˆ†é¡å™¨...")
    classifier = BehaviorClassifier()
    classifier.train()

    # Step 3: å•Ÿå‹•å¯¦æ™‚ç›£æ§
    print("\nğŸš€ Step 3: å•Ÿå‹•å¯¦æ™‚ç›£æ§...")
    engine = RealtimeAnomalyEngine()

    # é¸æ“‡æ¨¡å¼
    mode = input("\né¸æ“‡é‹è¡Œæ¨¡å¼:\n1. å–®æ¬¡åˆ†æ\n2. æŒçºŒç›£æ§\nè«‹é¸æ“‡ (1/2): ")

    if mode == '1':
        results = engine.analyze_recent(minutes=10)
        print(f"\nç™¼ç¾ {len(results)} å€‹ç•°å¸¸")
    else:
        engine.continuous_monitoring(interval_minutes=5)

if __name__ == "__main__":
    main()
```

---

## å…«ã€ç¸½çµèˆ‡å»ºè­°

### åŸºæ–¼ 99.57% è¦†è“‹ç‡çš„é—œéµå„ªå‹¢

1. **æ•¸æ“šè³ªé‡ä¿è­‰**
   - âœ… å¹¾ä¹ä¸éºæ¼ IP
   - âœ… ç‰¹å¾µå®Œæ•´ä¸”ä¸€è‡´
   - âœ… ç„¡éœ€æ“”å¿ƒæ•¸æ“šåå·®

2. **è¨“ç·´æ•ˆç‡æå‡**
   - âœ… æ•¸æ“šé‡é©ä¸­ï¼ˆ100è¬ç­† vs 40å„„ç­†ï¼‰
   - âœ… è¨“ç·´æ™‚é–“çŸ­ï¼ˆåˆ†é˜ç´š vs å°æ™‚ç´šï¼‰
   - âœ… å¯é »ç¹é‡è¨“ç·´

3. **æ¨è«–å»¶é²é™ä½**
   - âœ… å¯¦æ™‚æª¢æ¸¬ < 5ç§’
   - âœ… å¯æ¯5åˆ†é˜åŸ·è¡Œ
   - âœ… æ”¯æŒæŒçºŒç›£æ§

4. **æˆæœ¬å¤§å¹…é™ä½**
   - âœ… ES æŸ¥è©¢æˆæœ¬é™ä½ 100x
   - âœ… CPU/å…§å­˜ä½¿ç”¨é™ä½
   - âœ… LLM èª¿ç”¨æ¬¡æ•¸å¯æ§

### æ¨è–¦å¯¦ä½œè·¯å¾‘

**Week 1-2: åŸºç¤ ML**
- âœ… å¯¦ä½œ Isolation Forestï¼ˆåŸºæ–¼èšåˆæ•¸æ“šï¼‰
- âœ… é©—è­‰æ€§èƒ½å’Œæº–ç¢ºç‡
- âœ… æ•´åˆåˆ°å®šæœŸåˆ†æ

**Week 3-4: è¡Œç‚ºåˆ†é¡**
- âœ… è‡ªå‹•ç”Ÿæˆè¨“ç·´æ•¸æ“š
- âœ… è¨“ç·´ Random Forest Classifier
- âœ… å„ªåŒ–ç‰¹å¾µå·¥ç¨‹

**Week 5-6: é«˜ç´šåŠŸèƒ½**
- âœ… æ™‚é–“åºåˆ—ç•°å¸¸æª¢æ¸¬
- âœ… LLM æ·±åº¦åˆ†æï¼ˆå¯é¸ï¼‰
- âœ… æŒçºŒå­¸ç¿’æ©Ÿåˆ¶

**æŒçºŒæ”¹é€²**
- âœ… æ”¶é›†åé¥‹
- âœ… æ¯é€±é‡è¨“ç·´
- âœ… æ€§èƒ½ç›£æ§

### é—œéµæˆåŠŸå› ç´ 

1. **å……åˆ†åˆ©ç”¨èšåˆæ•¸æ“š**
   - æ‰€æœ‰ ML æ¨¡å‹ç›´æ¥è®€å– `netflow_stats_5m`
   - ç‰¹å¾µå·¥ç¨‹ç°¡åŒ–ä¸”é«˜æ•ˆ
   - è¨“ç·´å’Œæ¨è«–æ•¸æ“šä¸€è‡´

2. **åˆ†å±¤æª¢æ¸¬ç­–ç•¥**
   - Layer 1: è¦å‰‡ï¼ˆå¿«é€Ÿç¯©é¸ï¼‰
   - Layer 2: MLï¼ˆç²¾ç¢ºåˆ†é¡ï¼‰
   - Layer 3: LLMï¼ˆæ·±åº¦åˆ†æï¼‰

3. **æˆæœ¬æ§åˆ¶**
   - å„ªå…ˆä½¿ç”¨æœ¬åœ° ML
   - LLM åƒ…ç”¨æ–¼é«˜åƒ¹å€¼æ¡ˆä¾‹
   - æ‰¹æ¬¡è™•ç†ç›¸ä¼¼ç•°å¸¸

4. **æŒçºŒå­¸ç¿’**
   - è‡ªå‹•æ¨™è¨˜æ–°æ•¸æ“š
   - å®šæœŸé‡è¨“ç·´æ¨¡å‹
   - æ•´åˆäººå·¥åé¥‹

---

**æ–‡æª”ç‰ˆæœ¬:** 2.0ï¼ˆå„ªåŒ–ç‰ˆï¼‰
**æ›´æ–°æ—¥æœŸ:** 2025-11-11
**åŸºæ–¼:** Transform è¦†è“‹ç‡é©—è­‰çµæœï¼ˆ99.57%ï¼‰
