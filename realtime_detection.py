#!/usr/bin/env python3
"""
å¯¦æ™‚ç•°å¸¸æª¢æ¸¬è…³æœ¬

ä½¿ç”¨è¨“ç·´å¥½çš„ Isolation Forest é€²è¡Œå¯¦æ™‚ç•°å¸¸æª¢æ¸¬
ä¸¦ä½¿ç”¨åˆ†é¡å™¨åˆ¤æ–·ç•°å¸¸é¡å‹
"""

import sys
import argparse
import time
import warnings
from datetime import datetime, timezone

# å¿½ç•¥ Elasticsearch å®‰å…¨è­¦å‘Š
warnings.filterwarnings('ignore', message='.*Elasticsearch built-in security features.*')

from nad.utils import load_config
from nad.ml import OptimizedIsolationForest
from nad.ml.anomaly_classifier import AnomalyClassifier
from nad.device_classifier import DeviceClassifier
from nad.anomaly_logger import AnomalyLogger


def print_anomalies(anomalies, top_n=20):
    """æ‰“å°ç•°å¸¸çµæœ"""
    if not anomalies:
        print("âœ… æœªç™¼ç¾ç•°å¸¸\n")
        return

    # åˆå§‹åŒ–è®¾å¤‡åˆ†ç±»å™¨
    device_classifier = DeviceClassifier()

    print(f"\nâš ï¸  ç™¼ç¾ {len(anomalies)} å€‹ç•°å¸¸\n")
    print(f"{'='*100}")
    print(f"{'æ’å':<6} {'è¨­å‚™':<6} {'IPåœ°å€':<17} {'ç•°å¸¸åˆ†æ•¸':<12} {'ç½®ä¿¡åº¦':<10} {'é€£ç·šæ•¸':<10} {'ç›®çš„åœ°':<8} {'å¹³å‡æµé‡':<15}")
    print(f"{'='*100}")

    for i, anomaly in enumerate(anomalies[:top_n], 1):
        # è·å–è®¾å¤‡ç±»å‹
        device_type = device_classifier.classify(anomaly['src_ip'])
        device_emoji = device_classifier.get_type_emoji(device_type)

        print(f"{i:<6} "
              f"{device_emoji:<6} "
              f"{anomaly['src_ip']:<17} "
              f"{anomaly['anomaly_score']:< 12.4f} "
              f"{anomaly['confidence']:< 10.2f} "
              f"{anomaly['flow_count']:< 10,} "
              f"{anomaly['unique_dsts']:< 8} "
              f"{anomaly['avg_bytes']:< 15,.0f}")

    print(f"{'='*100}\n")


def analyze_anomalies(anomalies):
    """åˆ†æç•°å¸¸æ¨¡å¼"""
    if not anomalies:
        return

    print("ğŸ“Š ç•°å¸¸æ¨¡å¼åˆ†æ\n")

    # çµ±è¨ˆè¡Œç‚ºæ¨¡å¼
    high_conn = sum(1 for a in anomalies if a['features']['is_high_connection'])
    scanning = sum(1 for a in anomalies if a['features']['is_scanning_pattern'])
    small_packet = sum(1 for a in anomalies if a['features']['is_small_packet'])
    large_flow = sum(1 for a in anomalies if a['features']['is_large_flow'])

    print(f"è¡Œç‚ºçµ±è¨ˆ:")
    print(f"  é«˜é€£ç·šæ•¸: {high_conn} å€‹IP")
    print(f"  æƒææ¨¡å¼: {scanning} å€‹IP")
    print(f"  å°å°åŒ…: {small_packet} å€‹IP")
    print(f"  å¤§æµé‡: {large_flow} å€‹IP\n")

    # Top ç•°å¸¸è©³ç´°åˆ†æï¼ˆåŠ å…¥åˆ†é¡ï¼‰
    print("ğŸ” Top 5 ç•°å¸¸è©³ç´°åˆ†æ:\n")

    # å‰µå»ºåˆ†é¡å™¨
    classifier = AnomalyClassifier()

    for i, anomaly in enumerate(anomalies[:5], 1):
        # è¨ˆç®—æ™‚é–“å·®ä¸¦è½‰æ›ç‚ºæœ¬åœ°æ™‚é–“
        from datetime import timedelta
        anomaly_dt_utc = datetime.fromisoformat(anomaly['time_bucket'].replace('Z', '+00:00'))
        local_tz = timezone(timedelta(hours=8))
        anomaly_dt_local = anomaly_dt_utc.astimezone(local_tz)

        now = datetime.now(timezone.utc)
        time_diff_minutes = (now - anomaly_dt_utc).total_seconds() / 60

        print(f"{i}. {anomaly['src_ip']}")
        print(f"   æ™‚é–“: {anomaly_dt_local.strftime('%Y-%m-%d %H:%M:%S')} (æœ¬åœ°æ™‚é–“, {time_diff_minutes:.1f} åˆ†é˜å‰)")
        print(f"   ç•°å¸¸åˆ†æ•¸: {anomaly['anomaly_score']:.4f} (ç½®ä¿¡åº¦: {anomaly['confidence']:.2f})")
        print(f"   é€£ç·šæ•¸: {anomaly['flow_count']:,}")
        print(f"   ä¸åŒç›®çš„åœ°æ•¸é‡: {anomaly['unique_dsts']}")
        print(f"   ä¸åŒæºé€šè¨ŠåŸ æ•¸é‡: {anomaly.get('unique_src_ports', 'N/A')}")
        print(f"   ä¸åŒç›®çš„é€šè¨ŠåŸ æ•¸é‡: {anomaly.get('unique_dst_ports', 'N/A')}")
        print(f"   ç¸½æµé‡: {anomaly['total_bytes'] / 1024 / 1024:.2f} MB")
        print(f"   å¹³å‡æµé‡: {anomaly['avg_bytes']:,.0f} bytes")

        # è¡Œç‚ºæ¨™è¨˜ï¼ˆä¿ç•™ï¼Œä½œç‚ºè¼”åŠ©ä¿¡æ¯ï¼‰
        behaviors = []
        if anomaly['features']['is_high_connection']:
            behaviors.append("é«˜é€£ç·šæ•¸")
        if anomaly['features']['is_scanning_pattern']:
            behaviors.append("æƒææ¨¡å¼")
        if anomaly['features']['is_small_packet']:
            behaviors.append("å°å°åŒ…")
        if anomaly['features']['is_large_flow']:
            behaviors.append("å¤§æµé‡")

        if behaviors:
            print(f"   è¡Œç‚ºç‰¹å¾µ: {', '.join(behaviors)}")

        # ========== æ–°å¢ï¼šå¨è„…åˆ†é¡ ==========
        try:
            # æº–å‚™ä¸Šä¸‹æ–‡
            context = {
                'timestamp': datetime.fromisoformat(anomaly['time_bucket'].replace('Z', '+00:00')),
                'src_ip': anomaly['src_ip'],
                'anomaly_score': anomaly['anomaly_score']
            }

            # é€²è¡Œåˆ†é¡
            classification = classifier.classify(anomaly['features'], context)

            # é¡¯ç¤ºåˆ†é¡çµæœ
            severity_emoji = classifier.get_severity_emoji(classification['severity'])
            print(f"\n   {severity_emoji} å¨è„…åˆ†é¡: {classification['class_name']} ({classification['class_name_en']})")
            print(f"      ç½®ä¿¡åº¦: {classification['confidence']:.0%}")
            print(f"      åš´é‡æ€§: {classification['severity']} | å„ªå…ˆç´š: {classification['priority']}")
            print(f"      æè¿°: {classification['description']}")

            # é—œéµæŒ‡æ¨™
            if classification['indicators']:
                print(f"      é—œéµæŒ‡æ¨™:")
                for indicator in classification['indicators'][:3]:  # æœ€å¤šé¡¯ç¤º3å€‹
                    print(f"         â€¢ {indicator}")

            # éŸ¿æ‡‰å»ºè­°ï¼ˆç°¡åŒ–é¡¯ç¤ºï¼‰
            if classification['response']:
                print(f"      å»ºè­°è¡Œå‹•: {classification['response'][0]}")  # åªé¡¯ç¤ºç¬¬ä¸€å€‹

        except Exception as e:
            print(f"   âš ï¸  åˆ†é¡å¤±æ•—: {e}")

        print()


def single_detection(detector, minutes, exclude_servers=False):
    """å–®æ¬¡æª¢æ¸¬"""
    # è¨ˆç®—ä¸¦é¡¯ç¤ºæŸ¥è©¢æ™‚é–“ç¯„åœ
    from datetime import datetime, timedelta, timezone

    end_time_utc = datetime.now(timezone.utc)
    start_time_utc = end_time_utc - timedelta(minutes=minutes)

    # è½‰æ›ç‚ºæœ¬åœ°æ™‚é–“ (UTC+8)
    local_tz = timezone(timedelta(hours=8))
    end_time_local = end_time_utc.astimezone(local_tz)
    start_time_local = start_time_utc.astimezone(local_tz)

    print(f"\n{'='*100}")
    print(f"å¯¦æ™‚ç•°å¸¸æª¢æ¸¬ - åˆ†ææœ€è¿‘ {minutes} åˆ†é˜")
    if exclude_servers:
        print("(éæ¿¾æœå‹™å™¨å›æ‡‰æµé‡)")
    print(f"{'='*100}")
    print(f"\næŸ¥è©¢æ™‚é–“ç¯„åœ:")
    print(f"  é–‹å§‹: {start_time_local.strftime('%Y-%m-%d %H:%M:%S')} (æœ¬åœ°æ™‚é–“ UTC+8)")
    print(f"  çµæŸ: {end_time_local.strftime('%Y-%m-%d %H:%M:%S')} (æœ¬åœ°æ™‚é–“ UTC+8)")
    print(f"  ç•¶å‰: {end_time_local.strftime('%Y-%m-%d %H:%M:%S')} (æœ¬åœ°æ™‚é–“ UTC+8)\n")

    start_time = time.time()

    try:
        # æª¢æŸ¥æ•¸æ“šå¯ç”¨æ€§
        from elasticsearch import Elasticsearch
        es = Elasticsearch([detector.config.es_host], request_timeout=30)
        count_query = {
            'query': {
                'range': {
                    'time_bucket': {
                        'gte': start_time_utc.strftime('%Y-%m-%dT%H:%M:%S.000Z'),
                        'lte': end_time_utc.strftime('%Y-%m-%dT%H:%M:%S.000Z')
                    }
                }
            }
        }
        count_response = es.count(index=detector.config.es_aggregated_index, body=count_query)
        available_records = count_response['count']

        if available_records == 0:
            print(f"âš ï¸  è­¦å‘Š: åœ¨æŒ‡å®šæ™‚é–“ç¯„åœå…§æ²’æœ‰å¯ç”¨æ•¸æ“š")
            print(f"   åŸå› : èšåˆç´¢å¼•æ˜¯ 5 åˆ†é˜ä¸€å€‹æ™‚é–“æ¡¶ï¼Œæœ€æ–°æ•¸æ“šå¯èƒ½é‚„åœ¨è™•ç†ä¸­")
            print(f"   å»ºè­°: ä½¿ç”¨ --minutes 10 æˆ–æ›´é•·çš„æ™‚é–“çª—å£\n")
        else:
            print(f"ğŸ“Š å¯ç”¨æ•¸æ“š: {available_records:,} ç­†è¨˜éŒ„\n")

        anomalies = detector.predict_realtime(recent_minutes=minutes)

        # éæ¿¾æœå‹™å™¨å›æ‡‰ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
        if exclude_servers:
            original_count = len(anomalies)
            anomalies = [a for a in anomalies if not a.get('is_likely_server_response', 0)]
            filtered_count = original_count - len(anomalies)
            if filtered_count > 0:
                print(f"ğŸš« éæ¿¾æ‰ {filtered_count} å€‹æœå‹™å™¨å›æ‡‰æµé‡\n")

        elapsed = time.time() - start_time
        print(f"â±ï¸  æª¢æ¸¬è€—æ™‚: {elapsed:.2f} ç§’\n")

        # é¡¯ç¤ºçµæœ
        print_anomalies(anomalies)

        # åˆ†æç•°å¸¸
        analyze_anomalies(anomalies)

        return anomalies

    except Exception as e:
        print(f"âŒ æª¢æ¸¬å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return []


def continuous_monitoring(detector, interval_minutes, window_minutes, exclude_servers=False):
    """æŒçºŒç›£æ§æ¨¡å¼"""
    print(f"\n{'='*100}")
    print(f"æŒçºŒç›£æ§æ¨¡å¼")
    if exclude_servers:
        print("(éæ¿¾æœå‹™å™¨å›æ‡‰æµé‡)")
    print(f"{'='*100}\n")
    print(f"æª¢æ¸¬é–“éš”: æ¯ {interval_minutes} åˆ†é˜")
    print(f"åˆ†æçª—å£: æœ€è¿‘ {window_minutes} åˆ†é˜")
    print(f"ç•°å¸¸è¨˜éŒ„: è‡ªå‹•å¯«å…¥ Elasticsearch")
    print(f"æŒ‰ Ctrl+C åœæ­¢\n")
    print(f"{'='*100}\n")

    iteration = 0

    # å‰µå»ºåˆ†é¡å™¨ï¼ˆç”¨æ–¼å¨è„…åˆ†é¡ï¼‰
    classifier = AnomalyClassifier()
    device_classifier = DeviceClassifier()

    # å‰µå»ºç•°å¸¸è¨˜éŒ„æ—¥èªŒå™¨
    anomaly_logger = AnomalyLogger(
        es_host=detector.config.es_host if detector.config else 'localhost:9200'
    )

    try:
        while True:
            iteration += 1
            current_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')

            print(f"\nğŸ” æª¢æ¸¬ #{iteration} - {current_time}")
            print(f"{'-'*100}")

            anomalies = detector.predict_realtime(recent_minutes=window_minutes)

            # éæ¿¾æœå‹™å™¨å›æ‡‰ï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if exclude_servers and anomalies:
                anomalies = [a for a in anomalies if not a.get('is_likely_server_response', 0)]

            if anomalies:
                # åªé¡¯ç¤ºé«˜ç½®ä¿¡åº¦ç•°å¸¸
                high_conf = [a for a in anomalies if a['confidence'] > 0.6]
                low_conf_count = len(anomalies) - len(high_conf)

                if high_conf:
                    print(f"\nâš ï¸  ç™¼ç¾ {len(high_conf)} å€‹é«˜ç½®ä¿¡åº¦ç•°å¸¸:\n")

                    # é¡¯ç¤ºå‰ 10 å€‹ç•°å¸¸ï¼ŒåŒ…å«åˆ†é¡ä¿¡æ¯
                    for i, anomaly in enumerate(high_conf[:10], 1):
                        # è·å–è®¾å¤‡ç±»å‹
                        device_type = device_classifier.classify(anomaly['src_ip'])
                        device_emoji = device_classifier.get_type_emoji(device_type)

                        # åŸºæœ¬ä¿¡æ¯
                        print(f"  {i:2}. {device_emoji} {anomaly['src_ip']:15} | "
                              f"åˆ†æ•¸: {anomaly['anomaly_score']:.4f} | "
                              f"ç½®ä¿¡åº¦: {anomaly['confidence']:.2f} | "
                              f"{anomaly['flow_count']:,} é€£ç·š | "
                              f"{anomaly['unique_dsts']:3} ç›®çš„åœ°", end='')

                        # å¨è„…åˆ†é¡
                        try:
                            context = {
                                'timestamp': datetime.fromisoformat(anomaly['time_bucket'].replace('Z', '+00:00')),
                                'src_ip': anomaly['src_ip'],
                                'anomaly_score': anomaly['anomaly_score']
                            }
                            classification = classifier.classify(anomaly['features'], context)

                            # ç°¡çŸ­çš„åˆ†é¡ä¿¡æ¯ï¼ˆå–®è¡Œé¡¯ç¤ºï¼‰
                            severity_emoji = classifier.get_severity_emoji(classification['severity'])
                            print(f" | {severity_emoji} {classification['class_name']:10} ({classification['confidence']:.0%})")
                        except:
                            print()  # å¦‚æœåˆ†é¡å¤±æ•—ï¼Œåªæ›è¡Œ

                    # å¦‚æœæœ‰è¶…é 10 å€‹ç•°å¸¸ï¼Œé¡¯ç¤ºé‚„æœ‰å¤šå°‘å€‹
                    if len(high_conf) > 10:
                        print(f"\n  ... é‚„æœ‰ {len(high_conf) - 10} å€‹ç•°å¸¸æœªé¡¯ç¤º")

                    # é¡¯ç¤ºè¢«éæ¿¾çš„ä½ç½®ä¿¡åº¦ç•°å¸¸æ•¸é‡
                    if low_conf_count > 0:
                        print(f"\n  ğŸ” [Debug] éæ¿¾æ‰ {low_conf_count} å€‹ä½ç½®ä¿¡åº¦ç•°å¸¸ (â‰¤ 0.6)")

                    # ğŸ“ è‡ªå‹•è¨˜éŒ„é«˜ç½®ä¿¡åº¦ç•°å¸¸åˆ° ES
                    try:
                        anomaly_logger.log_anomalies_batch(
                            high_conf,
                            device_classifier=device_classifier,
                            classifier=classifier
                        )
                        print(f"\n  ğŸ’¾ å·²è¨˜éŒ„ {len(high_conf)} ç­†ç•°å¸¸åˆ° Elasticsearch")
                    except Exception as e:
                        print(f"\n  âš ï¸  è¨˜éŒ„ç•°å¸¸å¤±æ•—: {e}")

                else:
                    print(f"  âœ“ æª¢æ¸¬åˆ° {len(anomalies)} å€‹ç•°å¸¸ï¼Œä½†ç½®ä¿¡åº¦è¼ƒä½ (â‰¤ 0.6)")
            else:
                print("  âœ… æœªç™¼ç¾ç•°å¸¸")

            # ç­‰å¾…ä¸‹ä¸€æ¬¡æª¢æ¸¬
            print(f"\nâ° ä¸‹æ¬¡æª¢æ¸¬: {interval_minutes} åˆ†é˜å¾Œ...")
            time.sleep(interval_minutes * 60)

    except KeyboardInterrupt:
        print(f"\n\n{'='*100}")
        print("ç›£æ§å·²åœæ­¢")
        print(f"{'='*100}\n")


def main():
    parser = argparse.ArgumentParser(
        description='å¯¦æ™‚ç•°å¸¸æª¢æ¸¬'
    )
    parser.add_argument(
        '--minutes',
        type=int,
        default=10,
        help='åˆ†ææœ€è¿‘ N åˆ†é˜çš„æ•¸æ“šï¼ˆé»˜èª: 10ï¼‰'
    )
    parser.add_argument(
        '--config',
        type=str,
        default='nad/config.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾‘'
    )
    parser.add_argument(
        '--continuous',
        action='store_true',
        help='æŒçºŒç›£æ§æ¨¡å¼'
    )
    parser.add_argument(
        '--interval',
        type=int,
        default=5,
        help='æŒçºŒç›£æ§æ™‚çš„æª¢æ¸¬é–“éš”ï¼ˆåˆ†é˜ï¼Œé»˜èª: 5ï¼‰'
    )
    parser.add_argument(
        '--exclude-servers',
        action='store_true',
        help='éæ¿¾æ‰å¯èƒ½çš„æœå‹™å™¨å›æ‡‰æµé‡ï¼ˆæ¨è–¦ä½¿ç”¨ï¼‰'
    )

    args = parser.parse_args()

    # åŠ è¼‰é…ç½®
    print(f"\nğŸ“‹ åŠ è¼‰é…ç½®...")
    try:
        config = load_config(args.config)
        print(f"âœ“ é…ç½®åŠ è¼‰æˆåŠŸ")
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è¼‰å¤±æ•—: {e}")
        sys.exit(1)

    # å‰µå»ºæª¢æ¸¬å™¨ä¸¦åŠ è¼‰æ¨¡å‹
    print(f"ğŸ¤– åŠ è¼‰æ¨¡å‹...")
    try:
        detector = OptimizedIsolationForest(config)
        detector._load_model()
        print(f"âœ“ æ¨¡å‹åŠ è¼‰æˆåŠŸ")

        model_info = detector.get_model_info()
        print(f"\næ¨¡å‹ä¿¡æ¯:")
        print(f"  æ¨¹çš„æ•¸é‡: {model_info['n_estimators']}")
        print(f"  ç‰¹å¾µæ•¸é‡: {model_info['n_features']}")
        print(f"  æ±¡æŸ“ç‡: {model_info['contamination']}")

    except FileNotFoundError as e:
        print(f"\nâŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
        print(f"   è«‹å…ˆé‹è¡Œè¨“ç·´: python3 train_isolation_forest.py --days 7\n")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
        sys.exit(1)

    # åŸ·è¡Œæª¢æ¸¬
    if args.continuous:
        continuous_monitoring(detector, args.interval, args.minutes, args.exclude_servers)
    else:
        single_detection(detector, args.minutes, args.exclude_servers)


if __name__ == "__main__":
    main()
